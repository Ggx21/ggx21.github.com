<!DOCTYPE html>
<html lang=zh>
<head>
  <!-- so meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=5"
  />
  <meta name="description" content="1.准备数据 在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。  读写数据:(save&amp;load)   torch.save(x, &#39;x.pt&#39;):将x存在文件名同为x.pt的文件里。   将数据从存储的文件读回内存x2 &#x3D; torch.load(&#39;x.pt&#39;">
<meta property="og:type" content="article">
<meta property="og:title" content="Dive-into-DL-PyTorch">
<meta property="og:url" content="https://ggxloveslife.top/notes/pytorch/learn_torch/index.html">
<meta property="og:site_name" content="Hello From YUX">
<meta property="og:description" content="1.准备数据 在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。  读写数据:(save&amp;load)   torch.save(x, &#39;x.pt&#39;):将x存在文件名同为x.pt的文件里。   将数据从存储的文件读回内存x2 &#x3D; torch.load(&#39;x.pt&#39;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/3.8_sigmoid.png">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/3.8_sigmoid_grad.png">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/3.13_dropout.svg">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/5.5_lenet.png">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/5.6_alexnet.png">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/5.9_inception.svg">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/6.2_rnn.svg">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/6.5.png">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/6.7_gru_3.svg">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/6.8_lstm_2.svg">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/6.9_deep-rnn.svg">
<meta property="og:image" content="https://ggxloveslife.top/img/learn_torch.assets/6.10_birnn.svg">
<meta property="article:published_time" content="2023-05-03T08:25:01.000Z">
<meta property="article:modified_time" content="2023-05-03T18:20:55.211Z">
<meta property="article:author" content="Andy Gao">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ggxloveslife.top/img/learn_torch.assets/3.8_sigmoid.png">
     
  <link rel="shortcut icon" href="/images/favicon.ico" />
     
  <link
    rel="icon"
    type="image/png"
    href="/images/favicon-192x192.png"
    sizes="192x192"
  />
     
  <link
    rel="apple-touch-icon"
    sizes="180x180"
    href="/images/apple-touch-icon.png"
  />
    
  <!-- title -->
  <title>Dive-into-DL-PyTorch</title>
  <!-- async scripts -->
  <!-- Google Analytics -->

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-86660611-1"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-86660611-1');
  </script>

 <!-- Umami Analytics -->


  <!-- styles -->
  
<link rel="stylesheet" href="/css/style.css">

  <!-- persian styles -->
  
  <!-- rss -->
   
  <!-- mathjax -->
  
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
     });
  </script>
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
    async
  ></script>
  
   
  <meta name="generator" content="Hexo 6.3.0"><link rel="stylesheet" href="\css\APlayer.min.css" class="aplayer-style-marker">
  <script src="\js\APlayer.min.js" class="aplayer-script-marker"></script>
  
</head>


<script src="https://cdn.jsdelivr.net/npm/echarts@5.4.0/dist/echarts.min.js"></script>


<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"
    ><i class="fa-solid fa-bars fa-lg"></i
  ></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"
    ><i class="fa-solid fa-bars fa-lg"></i
  ></a>
  <a
    id="top-icon-tablet"
    href="#"
    aria-label="顶部"
    onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"
    style="display: none"
    ><i class="fa-solid fa-chevron-up fa-lg"></i
  ></a>
  <span id="menu">
    <br />
    <span id="actions">
      <ul>
        <li>
          <a class="icon" aria-label="homepage" href="/"
            ><i
              class="fa-solid fa-home"
              aria-hidden="true"
              onmouseover="document.getElementById('homepage').style.display = 'inline';"
              onmouseout="document.getElementById('homepage').style.display = 'none';"
            ></i></a
          ><span id="homepage" class="info" style="display: none"
            >回到主页</span
          >
        </li>
        
        <li>
          <a
            class="icon"
            aria-label="上一篇"
            href="/notes/pytorch/pa2report/"
            ><i
              class="fa-solid fa-chevron-left"
              aria-hidden="true"
              onmouseover="document.getElementById('i-prev').style.display = 'inline';"
              onmouseout="document.getElementById('i-prev').style.display = 'none';"
            ></i></a
          ><span id="i-prev" class="info" style="display: none"
            >上一篇</span
          >
        </li>
         
        <li>
          <a
            class="icon"
            aria-label="下一篇"
            href="/homework/DLCE/%E5%9B%9B%E4%BD%8D%E5%8A%A0%E6%B3%95%E5%99%A8REPORT/"
            ><i
              class="fa-solid fa-chevron-right"
              aria-hidden="true"
              onmouseover="document.getElementById('i-next').style.display = 'inline';"
              onmouseout="document.getElementById('i-next').style.display = 'none';"
            ></i></a
          ><span id="i-next" class="info" style="display: none"
            >下一篇</span
          >
        </li>
        
        <li>
          <a
            class="icon"
            aria-label="返回顶部"
            href="#"
            onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"
            ><i
              class="fa-solid fa-chevron-up"
              aria-hidden="true"
              onmouseover="document.getElementById('i-top').style.display = 'inline';"
              onmouseout="document.getElementById('i-top').style.display = 'none';"
            ></i></a
          ><span id="i-top" class="info" style="display: none"
            >返回顶部</span
          >
        </li>
      </ul>
    </span>
    <br />
    <div id="share" style="display: none">
    </div>
    <div id="toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">1.</span> <span class="toc-text"> 1.准备数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AEsaveload"><span class="toc-number">1.0.1.</span> <span class="toc-text"> 读写数据:(save&amp;load)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text"> 读写模型:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81sldata%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.0.2.1.</span> <span class="toc-text"> 代码(slData模式):</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text"> 2.创建模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.0.1.</span> <span class="toc-text"> 多层感知机的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.1.1.</span> <span class="toc-text"> 例子:</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AAflattenlayer%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.2.</span> <span class="toc-text"> 一个flattenlayer的例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.3.</span> <span class="toc-text"> 一个模型的例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text"> 初始化模型参数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.</span> <span class="toc-text"> 总结:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text"> 3.定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">4.</span> <span class="toc-text"> 4.定义优化器和学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text"> 5.训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">6.</span> <span class="toc-text"> 6.使用模型进行预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">7.</span> <span class="toc-text"> 7.模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.1.</span> <span class="toc-text"> 模型欠拟合与过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%9C%B0"><span class="toc-number">7.1.1.</span> <span class="toc-text"> 简单地:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AEtraining-error%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AEgeneralization-error"><span class="toc-number">7.1.2.</span> <span class="toc-text"> 训练误差（training error）和泛化误差（generalization error）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">7.1.3.</span> <span class="toc-text"> 训练集\验证集\测试集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.2.</span> <span class="toc-text"> 应对过拟合:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8Fweight-decay"><span class="toc-number">7.2.1.</span> <span class="toc-text"> 1.权重衰减（weight decay）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="toc-number">7.2.1.1.</span> <span class="toc-text"> 实现代码：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E4%B8%A2%E5%BC%83%E6%B3%95dropout"><span class="toc-number">7.2.2.</span> <span class="toc-text"> 2.丢弃法(dropout)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.2.2.1.</span> <span class="toc-text"> 实现:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cnn"><span class="toc-number">8.</span> <span class="toc-text"> CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">8.1.</span> <span class="toc-text"> 0.前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8A%BF"><span class="toc-number">8.1.1.</span> <span class="toc-text"> 优势?</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">8.2.</span> <span class="toc-text"> 1.填充和步幅:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85padding"><span class="toc-number">8.2.0.1.</span> <span class="toc-text"> 填充padding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85stride"><span class="toc-number">8.2.0.2.</span> <span class="toc-text"> 步幅stride</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E4%B8%8E%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.</span> <span class="toc-text"> 2.多输入通道与多输出通道.</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">8.3.1.</span> <span class="toc-text"> 多输入通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.2.</span> <span class="toc-text"> 多输出通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">8.3.3.</span> <span class="toc-text"> 1*1卷积层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E6%B1%A0%E5%8C%96"><span class="toc-number">8.4.</span> <span class="toc-text"> 2.池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90"><span class="toc-number">8.5.</span> <span class="toc-text"> 一些例子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lenet"><span class="toc-number">8.5.1.</span> <span class="toc-text"> LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">8.5.1.1.</span> <span class="toc-text"> 实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#alexnet"><span class="toc-number">8.5.2.</span> <span class="toc-text"> AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A8%8D%E5%BE%AE%E7%AE%80%E5%8C%96%E7%9A%84alexnet"><span class="toc-number">8.5.2.1.</span> <span class="toc-text"> 稍微简化的AlexNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vgg%E5%9D%97"><span class="toc-number">8.6.</span> <span class="toc-text"> VGG块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nin%E5%9D%97"><span class="toc-number">8.7.</span> <span class="toc-text"> NiN块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#59-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9Cgooglenet"><span class="toc-number">8.8.</span> <span class="toc-text"> 5.9 含并行连结的网络（GoogLeNet）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">8.9.</span> <span class="toc-text"> 批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet"><span class="toc-number">8.10.</span> <span class="toc-text"> 残差网络ResNet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnn"><span class="toc-number">9.</span> <span class="toc-text"> RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E7%9A%84%E4%B8%80%E4%B8%AA%E9%87%8D%E8%A6%81%E5%BA%94%E7%94%A8%E5%B0%B1%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.1.</span> <span class="toc-text"> RNN的一个重要应用就是语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#n%E5%85%83%E8%AF%AD%E6%B3%95"><span class="toc-number">9.1.1.</span> <span class="toc-text"> n元语法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%91%E7%90%86%E8%A7%A3%E7%9A%84rnn"><span class="toc-number">9.2.</span> <span class="toc-text"> 我理解的RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-2"><span class="toc-number">9.2.1.</span> <span class="toc-text"> 例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">9.3.</span> <span class="toc-text"> 构建数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="toc-number">9.3.0.1.</span> <span class="toc-text"> 随机采样</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%B8%E9%82%BB%E9%87%87%E6%A0%B7"><span class="toc-number">9.3.0.2.</span> <span class="toc-text"> 相邻采样</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.4.</span> <span class="toc-text"> RNN的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#native-rnn_layer%E7%9A%84%E8%BE%93%E5%85%A5"><span class="toc-number">9.4.0.1.</span> <span class="toc-text"> native rnn_layer的输入</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#native-rnn_layer%E7%9A%84%E8%BE%93%E5%87%BA"><span class="toc-number">9.4.0.2.</span> <span class="toc-text"> native rnn_layer的输出</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83gru"><span class="toc-number">9.5.</span> <span class="toc-text"> 门控循环单元GRU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86lstm"><span class="toc-number">9.6.</span> <span class="toc-text"> 长短期记忆LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E5%A4%8D%E6%9D%82%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.7.</span> <span class="toc-text"> 更复杂的模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.7.1.</span> <span class="toc-text"> 深度循环神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.7.2.</span> <span class="toc-text"> 双向循环神经网络</span></a></li></ol></li></ol></li></ol></div>
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Dive-into-DL-PyTorch
    </h1>



    <div class="meta">
      <span
        class="author p-author h-card"
        itemprop="author"
        itemscope
        itemtype="http://schema.org/Person"
      >
        <span class="p-name" itemprop="name"
          >Andy Gao</span
        >
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-05-03T08:25:01.000Z" class="dt-published" itemprop="datePublished">2023-05-03</time>
        
      
    </div>

 
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/notes/">notes</a> › <a class="category-link" href="/categories/notes/PyTorch/">PyTorch</a>
    </div>

 
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/PyTorch/" rel="tag">PyTorch</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="1准备数据"><a class="markdownIt-Anchor" href="#1准备数据"></a> 1.准备数据</h2>
<p>在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。</p>
<h4 id="读写数据saveload"><a class="markdownIt-Anchor" href="#读写数据saveload"></a> 读写数据:(save&amp;load)</h4>
<ul>
<li>
<p><code>torch.save(x, 'x.pt')</code>:将x存在文件名同为<code>x.pt</code>的文件里。</p>
</li>
<li>
<p>将数据从存储的文件读回内存<code>x2 = torch.load('x.pt')</code></p>
<p>不仅是tensor.dict啥啥的数据都能这么存储.</p>
<p>无论是文件大小还是读写速度都完爆json</p>
</li>
</ul>
<h4 id="读写模型"><a class="markdownIt-Anchor" href="#读写模型"></a> 读写模型:</h4>
<ul>
<li>
<p>两个方式:仅保存和加载模型参数(<code>state_dict</code>)(推荐)；保存和加载整个模型。</p>
<p>差别就是,第一种方法在SL时候只SL参数,下次L时先建一个新model,然后把参数L进去就好了.</p>
</li>
</ul>
<h5 id="代码sldata模式"><a class="markdownIt-Anchor" href="#代码sldata模式"></a> 代码(slData模式):</h5>
<p>保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是pt或pthCopy to clipboardErrorCopied</span></span><br></pre></td></tr></table></figure>
<p>加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>一个小问题在于不同设备(CPU与GPU,不同GPU之间.具体遇到问题再说吧)</p>
<h2 id="2创建模型"><a class="markdownIt-Anchor" href="#2创建模型"></a> 2.创建模型</h2>
<p>模型是深度学习的核心，它决定了最终学习的效果。在pytorch中，可以通过继承nn.Module类来创建模型，并在其中定义前向计算函数。</p>
<blockquote>
<p>我的理解是,模型就是一个千层饼.每一层有个什么功能.下面就是一层饼的例子.</p>
<p>实际上并不是层数越多越好.</p>
<ul>
<li>当神经网络的层数较多时，模型的数值稳定性容易变差:<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0.9</mn><msup><mn>9</mn><mn>365</mn></msup><mo>=</mo><mo stretchy="false">?</mo><mo separator="true">;</mo><mn>1.0</mn><msup><mn>1</mn><mrow><mn>365</mn><mo>=</mo><mo stretchy="false">?</mo></mrow></msup></mrow><annotation encoding="application/x-tex">0.99^ {365}=?;1.01^{365=?}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord"><span class="mord">9</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">6</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mclose">?</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">0</span><span class="mord"><span class="mord">1</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">6</span><span class="mord mtight">5</span><span class="mrel mtight">=</span><span class="mclose mtight">?</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li>如何解决?批量归一化</li>
</ul>
<p>通过不停地tensor运算联立方程,是不是可以把千层饼等效为单层?</p>
</blockquote>
<h4 id="多层感知机的设计"><a class="markdownIt-Anchor" href="#多层感知机的设计"></a> 多层感知机的设计</h4>
<p>如果千层饼被等效为单层,那么对层的设计就失去了意义.上面的问题在于.</p>
<blockquote>
<p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p>
</blockquote>
<p>解决方法是我们加入一些非线性变换的层</p>
<h5 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子:</h5>
<ul>
<li>
<p>ReLU（rectified linear unit）函数</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">ReLU(x)=max(x,0).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
</li>
<li>
<p>sigmoid</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="normal">−</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">sigmoid(x)= \frac 1 {1+exp(−x)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.25744em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<img src="/img/learn_torch.assets/3.8_sigmoid.png" alt="img" style="zoom:33%;" />
<img src="/img/learn_torch.assets/3.8_sigmoid_grad.png" alt="img" style="zoom:33%;" />
</li>
<li>
<p>tanh函数(双曲正切)</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="normal">−</mi><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mi mathvariant="normal">−</mi><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="normal">−</mi><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x)= \frac{1+exp(−2x)}{1−exp(−2x)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">−</span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>图像和sigmoid差不多,但是0附近更陡峭一些</p>
</li>
</ul>
<h4 id="一个flattenlayer的例子"><a class="markdownIt-Anchor" href="#一个flattenlayer的例子"></a> 一个flattenlayer的例子</h4>
<p>这里有一个把(batchsize,*,*,…)的多维tensor转换为(batchsize,***)的二维tensor的例子.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FlattenLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="一个模型的例子"><a class="markdownIt-Anchor" href="#一个模型的例子"></a> 一个模型的例子</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict([</span><br><span class="line">        (<span class="string">&#x27;flatten&#x27;</span>, FlattenLayer()),<span class="comment">#看,就是这样一层层搭起来...</span></span><br><span class="line">        (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">    ])</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数"><a class="markdownIt-Anchor" href="#初始化模型参数"></a> 初始化模型参数:</h3>
<p>构建好了模型,我们得有初始参数啊.</p>
<p>**太好了!**PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略,一般不用我们考虑;如果你真的想知道可参考pytorch<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules">源代码)</a></p>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结:</h3>
<ol>
<li>
<p>继承Module类</p>
<p><code>Module</code>类是<code>nn</code>模块里提供的一个模型构造类，是所有神经网络模块的基类</p>
<p>一般来说,我们重载<code>Module</code>类的<code>__init__</code>函数和<code>forward</code>函数。它们分别用于创建模型参数和定义前向计算。无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的<code>backward</code>函数。</p>
<p>注意:重载<code>__init__</code>函数时应该首先调用父类<code>module</code>的初始化函数e.g.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>module的子类:还有一些<code>Sequential</code>,<code>ModuleDict</code>,<code>ModuleList</code>之类的东西.我感觉没什么用啊.好像只是让我把一堆Module以一种整齐的方式凑在一起.<code>Sequential</code>好像还挺有用的,至少他要保证层之间输入输出维度匹配,可以直接forward.</p>
</li>
<li>
<p>共享模型参数: <code>Module</code>类的<code>forward</code>函数里多次调用同一个层,层之间参数共享。此外，如果我们传入<code>Sequential</code>的模块是同一个<code>Module</code>实例的话参数也是共享的</p>
<blockquote>
<p>真的有人在乎参数究竟是什么吗?</p>
</blockquote>
</li>
<li>
<p>自定义层:见[flattenlayer](#### 一个flattenlayer的例子)的例子,这是一个不带参数的层,你当然也可以带参数.但意义是?</p>
</li>
<li>
<p>我想:module就是层的累加.一个头进一个头出.<code>人体蜈蚣.</code></p>
</li>
</ol>
<h2 id="3定义损失函数"><a class="markdownIt-Anchor" href="#3定义损失函数"></a> 3.定义损失函数</h2>
<p>常见的损失函数有交叉熵损失函数、均方误差损失函数等，可以根据不同的任务选择不同的损失函数。</p>
<blockquote>
<p>例子:</p>
<p>PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="4定义优化器和学习率"><a class="markdownIt-Anchor" href="#4定义优化器和学习率"></a> 4.定义优化器和学习率</h2>
<p>在训练的过程中，需要使用优化器来更新模型参数。常见的优化器有梯度下降法、Adam等。同时，由于学习率对训练效果影响较大，所以需要定义学习率的初始值以及变化规则。</p>
<blockquote>
<p>e.g.</p>
<p>我们使用学习率为0.1的小批量随机梯度下降作为优化算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="5训练模型"><a class="markdownIt-Anchor" href="#5训练模型"></a> 5.训练模型</h2>
<p>将数据、模型、损失函数、优化器和学习率等组合在一起，循环执行前向计算、损失计算、反向传播和参数更新等步骤，即可训练模型。在每个epoch结束后，还需要对模型进行评估。</p>
<p>:::tips</p>
<p>在训练之前可以测试输入形状比如：</p>
<p>在训练ResNet之前，我们来观察一下输入形状在ResNet不同模块之间的变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27; output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p>:::</p>
<h2 id="6使用模型进行预测"><a class="markdownIt-Anchor" href="#6使用模型进行预测"></a> 6.使用模型进行预测</h2>
<p>在训练好模型后，可以使用它对新的数据进行预测，并输出预测结果或概率。</p>
<h2 id="7模型选择"><a class="markdownIt-Anchor" href="#7模型选择"></a> 7.模型选择</h2>
<p>我们怎么评价模型的好坏呢?</p>
<h3 id="模型欠拟合与过拟合"><a class="markdownIt-Anchor" href="#模型欠拟合与过拟合"></a> 模型欠拟合与过拟合</h3>
<h4 id="简单地"><a class="markdownIt-Anchor" href="#简单地"></a> <strong>简单地:</strong></h4>
<ul>
<li><strong>给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。</strong></li>
<li><strong>过少的训练样本</strong>也会导致过拟合</li>
</ul>
<h4 id="训练误差training-error和泛化误差generalization-error"><a class="markdownIt-Anchor" href="#训练误差training-error和泛化误差generalization-error"></a> 训练误差（training error）和泛化误差（generalization error）</h4>
<p>当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确。这是为什么呢？</p>
<p>以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。</p>
<p>训练的时候是根据往年题的表现来的(通过减小训练误差).所以往年题做的好不一定代表着高考考的好.(一般来说训练误差的期望<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">\leq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719400000000001em;vertical-align:-0.13597em;"></span><span class="mrel">≤</span></span></span></span>泛化误差)</p>
<p>但我们关注的是<strong>泛化误差</strong></p>
<h4 id="训练集验证集测试集"><a class="markdownIt-Anchor" href="#训练集验证集测试集"></a> 训练集\验证集\测试集</h4>
<p>高考只能考一次,我们没有办法从训练集(看着答案做题)中知道自己的高考表现.所以我们可以拿一些测试集训练集之外的数据作为<strong>验证集</strong>(模考).</p>
<p>操作上,我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p>
<blockquote>
<p><em>K</em>折交叉验证（K-<em>K</em>-fold cross-validation）</p>
<p>分出K个子集来.这样可以测K次</p>
</blockquote>
<p>但实际上,由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊.</p>
<h3 id="应对过拟合"><a class="markdownIt-Anchor" href="#应对过拟合"></a> 应对过拟合:</h3>
<h4 id="1权重衰减weight-decay"><a class="markdownIt-Anchor" href="#1权重衰减weight-decay"></a> 1.权重衰减（weight decay）</h4>
<blockquote>
<p>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</p>
</blockquote>
<p>权重衰减等价于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数正则化（regularization）</p>
<p><strong>我的理解</strong>:就是在计算损失函数的时候加上一项<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi><mo>×</mo><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">λ\times L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>其中λ是一个超参。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：参数权重越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>越大。这样大概保证了各参数大小都差不多，不偏科。</p>
<h5 id="实现代码"><a class="markdownIt-Anchor" href="#实现代码"></a> 实现代码：</h5>
<p>直接在构造优化器实例时通过<code>weight_decay</code>参数来指定权重衰减超参数。默认下，PyTorch会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line"><span class="comment">#...在optimize时</span></span><br><span class="line"><span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br></pre></td></tr></table></figure>
<h4 id="2丢弃法dropout"><a class="markdownIt-Anchor" href="#2丢弃法dropout"></a> 2.丢弃法(dropout)</h4>
<img src="/img/learn_torch.assets/3.13_dropout.svg" alt="img" style="zoom: 80%;" />
<p>在隐藏层随机丢弃一个单元.比如上图,隐藏层原来有$h_1,h_2,…,h_5.h_2,h_5 $被丢弃了.</p>
<p>这样计算时不会过度依赖某一个隐藏单元</p>
<ul>
<li>
<blockquote>
<p>(和权重衰减的目的大概是一样的),都是保证不偏科.要不然模考物理总是很难,就你一个考100,人家都是0分.到了高考一赋分大家都考90,你傻眼了.</p>
</blockquote>
</li>
</ul>
<h5 id="实现"><a class="markdownIt-Anchor" href="#实现"></a> 实现:</h5>
<p>在PyTorch中，我们只需要在全连接层后添加<code>Dropout</code>层并指定丢弃概率。在训练模型时，<code>Dropout</code>层将以指定的丢弃概率随机丢弃上一层的输出元素</p>
<p>在测试模型时（即<code>model.eval()</code>后），<code>Dropout</code>层并不发挥作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<blockquote>
<p>丢弃法只在训练模型时使用。</p>
</blockquote>
<h2 id="cnn"><a class="markdownIt-Anchor" href="#cnn"></a> CNN</h2>
<p>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。</p>
<h3 id="0前置知识"><a class="markdownIt-Anchor" href="#0前置知识"></a> 0.前置知识</h3>
<p>在net里加上一个卷积层就形成了<strong>CNN</strong></p>
<p>一个小翻译:<code>Kernel</code>在pytorch的卷积计算时就叫做<code>filter</code>.</p>
<p>CNN的学习就是学习这个<code>kernel</code>,(相应的,上面在学习一个参数tensor)</p>
<p>那个经典的图像边缘检测实际上在互相关.kernel走一步,算一圈,下个蛋…</p>
<p>卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>.有什么区别?**没有区别!**反正都是学出来的,学出来的核再上下左右转换一下那么两种运算就交换了.所以根本不需要知道什么是卷积就能CNN.</p>
<h4 id="优势"><a class="markdownIt-Anchor" href="#优势"></a> 优势?</h4>
<p>考虑图像分类问题。每张图像高和宽均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p>
<ol>
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为1000像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状是3,000,000×2563,000,000×256：它占用了大约3 GB的内存或显存。这带来过复杂的模型和过高的存储开销。</li>
</ol>
<p>一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p>
<h3 id="1填充和步幅"><a class="markdownIt-Anchor" href="#1填充和步幅"></a> 1.填充和步幅:</h3>
<p>要想使用一个卷积层,操作上显见的问题就是我们要知道这个卷积层输入输出的形状.</p>
<p>一般来说,对于第i维来说,如果输入在第i维长度?<code>有没有更好的名字</code>是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">n_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,核长度是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">k_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>那么输出在第i维长度就是(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>−</mo><msub><mi>k</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_i-k_i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>),显然,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">k_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>不应该超过<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">n_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,否则会报错</p>
<h5 id="填充padding"><a class="markdownIt-Anchor" href="#填充padding"></a> 填充padding</h5>
<p>**太长不看:**取padding=({<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>k</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>z</mi><msub><mi>e</mi><mi>i</mi></msub><mo>−</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac {kernelsize_i-1} 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2412079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.04398em;">z</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>,…})可以保证输入输出形状相同</p>
<p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。</p>
<p>如果在第i维的两侧<strong>一共填充<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>行</strong></p>
<p>那么输出形状第i维将会是(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>−</mo><msub><mi>k</mi><mi>i</mi></msub><mo>+</mo><msub><mi>p</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_i-k_i+p_i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>)</p>
<p>所以通常可以把<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mi mathvariant="normal">设</mi><mi mathvariant="normal">置</mi><mi mathvariant="normal">为</mi><msub><mi>k</mi><mi>i</mi></msub><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p_i设置为k_i-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">设</span><span class="mord cjk_fallback">置</span><span class="mord cjk_fallback">为</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>来使输入输出具有相同形状</p>
<p>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<p><mark>注意</mark>:操作上<code>padding=(2, 1)</code>padding的参数代表着在kernel两侧分别填充多少.也就是说<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>=</mo><msub><mi>p</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">=p_i/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord">2</span></span></span></span>,所以尽量也把核的每一维长度取奇数,这样保证了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">p_i/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord">2</span></span></span></span>是个整数.否则还得考虑两边分别取floor和ceiling,<strong>麻烦死了</strong></p>
<h5 id="步幅stride"><a class="markdownIt-Anchor" href="#步幅stride"></a> 步幅stride</h5>
<p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p>
<p>步幅可以按比例缩小形状</p>
<p>如果按上一步设置理想的padding，同时如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将缩为原来的1/stride倍.其他情况自己算去,但何必为难自己.</p>
<h3 id="2多输入通道与多输出通道"><a class="markdownIt-Anchor" href="#2多输入通道与多输出通道"></a> 2.多输入通道与多输出通道.</h3>
<h4 id="多输入通道"><a class="markdownIt-Anchor" href="#多输入通道"></a> 多输入通道</h4>
<p>实际上不就是多了一维吗?比如彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。就是新增了一个长度为3(r,g,b)的一维.然后我们用三个卷积核叠一块,发现在色彩维度上insize和kernersize都是3,然后这一维长度就变成1,退化了.</p>
<p>实现含多个输入通道的互相关运算。我们只需要对每个通道做互相关运算，然后通过<code>add_n</code>函数来进行累加。</p>
<h4 id="多输出通道"><a class="markdownIt-Anchor" href="#多输出通道"></a> 多输出通道</h4>
<p>哎呀我一阵头晕目眩,不想知道它是怎么算的了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积层----------------------------------------看这里</span></span><br><span class="line">conv_layer = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">output_tensor = conv_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量形状</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)  <span class="comment"># torch.Size([16, 10, 32, 32])</span></span><br></pre></td></tr></table></figure>
<p>直接来看示例代码吧.知道它怎么算干嘛呢?</p>
<p>在上面的代码中，<code>input_tensor</code> 是一个批次大小为 16、通道数为 3、高度为 32、宽度为 32 的输入张量，<code>conv_layer</code> 是一个输出通道数为 10、卷积核大小为 3x3、步幅为 1、填充为 1 的卷积层，<code>output_tensor</code> 是卷积层的输出张量，它的形状为 <code>[16, 10, 32, 32]</code>。</p>
<p>这是gpt说的啊,我觉着挺对的.可以自己跑着试试.</p>
<h4 id="11卷积层"><a class="markdownIt-Anchor" href="#11卷积层"></a> 1*1卷积层</h4>
<p>一个长宽都是1的核</p>
<p>1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。</p>
<p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么1×1卷积层的作用与全连接层等价</strong>。</p>
<p>通过这个来调整参数的通道数,控制模型复杂度.e.g.语数外+理综to语数外+三门选科</p>
<h3 id="2池化"><a class="markdownIt-Anchor" href="#2池化"></a> 2.池化</h3>
<p><strong>缓解卷积层对位置的过度敏感性</strong>。(想想抗锯齿操作,都是通过采样来使得过渡更加平滑??)</p>
<p>池化窗口形状为p<em>×</em>q<em>的池化层称为p</em>×<em>q</em>池化层，其中的池化运算叫作p<em>×</em>q池化。</p>
<ul>
<li>池化层的输出通道数跟输入通道数相同。</li>
<li>默认情况下，<code>MaxPool2d</code>实例里步幅和池化窗口形状相同。当然可以自己设定.但是池化后的形状又要算一算了吗?</li>
</ul>
<h3 id="一些例子"><a class="markdownIt-Anchor" href="#一些例子"></a> 一些例子</h3>
<p><strong>下面的例子,即使是LeNet,都完全够应付作业了</strong>完全没有必要看</p>
<h4 id="lenet"><a class="markdownIt-Anchor" href="#lenet"></a> LeNet</h4>
<p><img src="/img/learn_torch.assets/5.5_lenet.png" alt="img" /></p>
<p><strong>卷积层块里的基本单位是卷积层后接最大池化层</strong>：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。</p>
<h5 id="实现-2"><a class="markdownIt-Anchor" href="#实现-2"></a> 实现</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>先conv再flatten,大概都是这么个套路</p>
<h4 id="alexnet"><a class="markdownIt-Anchor" href="#alexnet"></a> AlexNet</h4>
<p>很多分类工作流程是:获取数据集–&gt;获得特征–&gt;进行分类.</p>
<p>之前认为DL的工作只是使用机器学习模型对特征分类。使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p>
<p>特征可以学习吗?&gt;&gt; AlexNet</p>
<p><img src="/img/learn_torch.assets/5.6_alexnet.png" alt="img" /></p>
<h5 id="稍微简化的alexnet"><a class="markdownIt-Anchor" href="#稍微简化的alexnet"></a> 稍微简化的AlexNet</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>真的,体验上来说,LeNet做作业就完全够了…</p>
</blockquote>
<ul>
<li>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</li>
<li>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</li>
</ul>
<h3 id="vgg块"><a class="markdownIt-Anchor" href="#vgg块"></a> <a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.7_vgg?id=_571-vgg%E5%9D%97">VGG块</a></h3>
<p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×33×3的卷积层后接上一个步幅为2、窗口形状为2×22×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数。</p>
<blockquote>
<p>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p>
</blockquote>
<h3 id="nin块"><a class="markdownIt-Anchor" href="#nin块"></a> <a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.8_nin?id=_581-nin%E5%9D%97">NiN块</a></h3>
<p>我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在5.3节（多输入通道和多输出通道）里介绍的1×11×1卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×11×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。</p>
<p>上面这俩我回头再看,头晕😵‍💫😵‍💫😵‍💫😵‍💫</p>
<h3 id="59-含并行连结的网络googlenet"><a class="markdownIt-Anchor" href="#59-含并行连结的网络googlenet"></a> <a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.9_googlenet?id=_59-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88googlenet%EF%BC%89">5.9 含并行连结的网络（GoogLeNet）</a></h3>
<p>如果说上面两个是网络之间串行,那么GoogLeNet的基本块Inception块是并行的(并行里套了串行)</p>
<img src="/img/learn_torch.assets/5.9_inception.svg" alt="inception块" />
<p>Inception块里有4条并行的线路。前3条线路使用窗口大小分别是1×1、3×3和5×5的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1×1卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用3×3最大池化层，后接1×1卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p>
<p>这个更复杂了😵‍💫😵‍💫😵‍💫😵‍💫具体实现抄抄链接里的吧.我真的好奇他怎么保证每次输入输出通道匹配的</p>
<h3 id="批量归一化"><a class="markdownIt-Anchor" href="#批量归一化"></a> 批量归一化</h3>
<ul>
<li>在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</li>
<li>对全连接层和卷积层做批量归一化的方法稍有不同。</li>
<li>批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。</li>
<li>PyTorch提供了BatchNorm类方便使用。</li>
</ul>
<p>Pytorch中<code>nn</code>模块定义的<code>BatchNorm1d</code>和<code>BatchNorm2d</code>类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的<code>num_features</code>参数值。下面我们用PyTorch实现使用批量归一化的LeNet。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>可以试试把这个加到自己的module里,</p>
<blockquote>
<p>它就是个插件,可以加上它,而不用对其他东西做任何修改,我觉着挺好的</p>
</blockquote>
<h3 id="残差网络resnet"><a class="markdownIt-Anchor" href="#残差网络resnet"></a> 残差网络ResNet</h3>
<p>我饿了🥱🥱🥱不学了.</p>
<h2 id="rnn"><a class="markdownIt-Anchor" href="#rnn"></a> RNN</h2>
<h3 id="rnn的一个重要应用就是语言模型"><a class="markdownIt-Anchor" href="#rnn的一个重要应用就是语言模型"></a> RNN的一个重要应用就是语言模型</h3>
<p>假设一段长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span>的文本中的词依次为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, \ldots, w_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，那么在离散的时间序列中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>≤</mo><mi>t</mi><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">1 \leq t \leq T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7719400000000001em;vertical-align:-0.13597em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span>）可看作在时间步（time step）<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span>的输出或标签。给定一个长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span>的词的序列<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, \ldots, w_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，语言模型将计算该序列的概率：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>T</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w_1, w_2, \ldots, w_T).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
<p>假设序列<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, \ldots, w_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中的每个词是依次生成的，我们有</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>T</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1}).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
<h4 id="n元语法"><a class="markdownIt-Anchor" href="#n元语法"></a> n元语法</h4>
<p>当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>个词相关，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>阶马尔可夫链（Markov chain of order <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>）。如果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，那么有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>∣</mo><msub><mi>w</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。如果基于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>阶马尔可夫链，我们可以将语言模型改写为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>T</mi></msub><mo stretchy="false">)</mo><mo>≈</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
<h3 id="我理解的rnn"><a class="markdownIt-Anchor" href="#我理解的rnn"></a> 我理解的RNN</h3>
<p>上面的例子:把一个句子看做词序列,每个时间步产生一个词…当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。但是,我们可以通过一个隐藏变量来传递前面几个时间步的信息…该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。</p>
<h4 id="例子-2"><a class="markdownIt-Anchor" href="#例子-2"></a> 例子</h4>
<p>引入一个新的权重参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{W}_{hh} \in {R}^{h \times h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span>，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span>的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>t</mi></msub><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><msub><mi>W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><mo>+</mo><msub><mi>H</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mi>h</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{H}_t = \phi({X}_t{W}_{xh}+{H}_{t-1}{W}_{hh}+{b}_h)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">b</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">{H}_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>就是上一个时间步的隐藏变量</p>
<p><img src="/img/learn_torch.assets/6.2_rnn.svg" alt="img" /></p>
<h3 id="构建数据集"><a class="markdownIt-Anchor" href="#构建数据集"></a> 构建数据集</h3>
<p>对时序数据的采样方式有如下两种(采样就是选取batchsize个样本,如果batchsize=1,那么就不需要考虑如何采样了)</p>
<ol>
<li>
<h5 id="随机采样"><a class="markdownIt-Anchor" href="#随机采样"></a> 随机采样</h5>
<p>在数据集中随机选取batchsize个样本,由于不同样本之间是独立的,所以不能直接把上个样本计算后的隐藏状态传给下个样本作为初始隐藏状态</p>
</li>
<li>
<h5 id="相邻采样"><a class="markdownIt-Anchor" href="#相邻采样"></a> 相邻采样</h5>
<p>字面意思,在数据集中选取相邻的batchsize个样本,这样只需在每一个迭代周期开始时初始化隐藏状态.</p>
<p>但是这样计算梯度时会依赖所有序列??</p>
<p>为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</p>
</li>
</ol>
<h3 id="rnn的实现"><a class="markdownIt-Anchor" href="#rnn的实现"></a> RNN的实现</h3>
<blockquote>
<p>我终于明白了,我们好像只需要关注不同layer的输入输出形状…</p>
<p>至于我的net为什么有他们,who ™ cares??</p>
</blockquote>
<p>PyTorch中的<code>nn</code>模块提供了循环神经网络的实现。下面构造一个含单隐藏层、隐藏单元个数为256的循环神经网络层<code>rnn_layer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure>
<h5 id="native-rnn_layer的输入"><a class="markdownIt-Anchor" href="#native-rnn_layer的输入"></a> native rnn_layer的输入</h5>
<p><code>rnn_layer</code>的输入形状为(时间步数, 批量大小,<mark>input_size</mark>)</p>
<p>就是有点别扭.</p>
<ul>
<li>第一个时间步数就是句子长度</li>
<li>第二个批量大小,我真无语,为什么要把批量大小放在第二个,感觉不是很直观.所以cnn和rnn的数据不能直接通用,要view一下交换一下前两个维度?还是别的什么维度</li>
<li>第三个inut_size;这里和cnn不同,如果说cnn的input单元是句子整体.那么RNN的input实际上是词向量.所以说如果是one-hot,那么就是vocab_size;如果已经word2vec后,就是<strong>词向量的长度</strong>…这里注意一下不同教程的区别.</li>
</ul>
<h5 id="native-rnn_layer的输出"><a class="markdownIt-Anchor" href="#native-rnn_layer的输出"></a> native rnn_layer的输出</h5>
<p>形状为(时间步数, 批量大小, 隐藏单元个数)。</p>
<p>前向计算后会分别返回输出和隐藏状态h，其中输出指的是隐藏层在<strong>各个时间步</strong>上计算并输出的隐藏状态，它们通常作为后续输出层的输入。参考下图(这是LSTM,和RNN本身不很一样)</p>
<p><img src="/img/learn_torch.assets/6.5.png" alt="img" /></p>
<h3 id="门控循环单元gru"><a class="markdownIt-Anchor" href="#门控循环单元gru"></a> 门控循环单元GRU</h3>
<p>当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p>
<p><strong>GRU</strong>就是为了解决上述问题</p>
<p>GRU包含重置门和更新门,计算图如下</p>
<p><img src="/img/learn_torch.assets/6.7_gru_3.svg" alt="img" /></p>
<blockquote>
<p>简单来说,就是如果前面时间步的输入已经和这次输出没太大关系,重置门会重置隐藏状态</p>
<p>如果这次输入不太影响后面内容,更新门会把之前的隐藏状态传递下去.</p>
<ul>
<li>重置门有助于捕捉时间序列里短期的依赖关系；</li>
<li>更新门有助于捕捉时间序列里长期的依赖关系。</li>
</ul>
</blockquote>
<p><mark>注意</mark>:上面完全不需要理解.反正有native的GRU模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure>
<h3 id="长短期记忆lstm"><a class="markdownIt-Anchor" href="#长短期记忆lstm"></a> 长短期记忆LSTM</h3>
<p>另一种门控循环神经网络是LSTM（long short-term memory,长短期记忆)</p>
<blockquote>
<p>你看这名字,这不是和刚才的门干了一样的事吗??</p>
</blockquote>
<p>LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞，从而记录额外的信息。</p>
<p><mark>同样地</mark>,可以直接调用<code>rnn</code>模块中的<code>LSTM</code>类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure>
<p><img src="/img/learn_torch.assets/6.8_lstm_2.svg" alt="img" /></p>
<p>如果你想理解,我把图放这了.</p>
<p>我的理解是,这其实就是显示地干了GRU的活.</p>
<blockquote>
<ul>
<li>如果遗忘门与记忆细胞做<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">⊙</span></span></span></span>趋于1,那么就会倾向于保存下之前的记忆,如果趋于0就会遗忘</li>
<li>如果输入门与候选记忆细胞做<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">⊙</span></span></span></span>趋于1,那么就会倾向于更新记忆,如果趋于0就不会更新</li>
</ul>
</blockquote>
<h3 id="更复杂的模型"><a class="markdownIt-Anchor" href="#更复杂的模型"></a> 更复杂的模型</h3>
<h4 id="深度循环神经网络"><a class="markdownIt-Anchor" href="#深度循环神经网络"></a> 深度循环神经网络</h4>
<p>目前为止介绍的循环神经网络只有一个单向的隐藏层，在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。下图演示了一个有L个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p>
<p><img src="/img/learn_torch.assets/6.9_deep-rnn.svg" alt="img" /></p>
<h4 id="双向循环神经网络"><a class="markdownIt-Anchor" href="#双向循环神经网络"></a> 双向循环神经网络</h4>
<p>之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。</p>
<p><img src="/img/learn_torch.assets/6.10_birnn.svg" alt="img" /></p>
<blockquote>
<p>上面两个我感觉挺靠谱的,但是没有示例代码,所以我就不管了</p>
</blockquote>
<blockquote>
<p>update:其实双向循环网络就是加个参数的事</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BiRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab, embed_size, num_hiddens, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiRNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="built_in">len</span>(vocab), embed_size)</span><br><span class="line">        <span class="comment"># bidirectional设为True即得到双向循环神经网络</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size,</span><br><span class="line">                                hidden_size=num_hiddens,</span><br><span class="line">                                num_layers=num_layers,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始时间步和最终时间步的隐藏状态作为全连接层输入</span></span><br><span class="line">        self.decoder = nn.Linear(<span class="number">4</span>*num_hiddens, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后</span></span><br><span class="line">        <span class="comment"># 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)</span></span><br><span class="line">        embeddings = self.embedding(inputs.permute(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">        <span class="comment"># rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。</span></span><br><span class="line">        <span class="comment"># outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)</span></span><br><span class="line">        outputs, _ = self.encoder(embeddings) <span class="comment"># output, (h, c)</span></span><br><span class="line">        <span class="comment"># 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为</span></span><br><span class="line">        <span class="comment"># (批量大小, 4 * 隐藏单元个数)。</span></span><br><span class="line">        encoding = torch.cat((outputs[<span class="number">0</span>], outputs[-<span class="number">1</span>]), -<span class="number">1</span>)</span><br><span class="line">        outs = self.decoder(encoding)</span><br><span class="line">        <span class="keyword">return</span> outs</span><br></pre></td></tr></table></figure>
<blockquote>
<p>而多层就更不用教了…</p>
</blockquote>

  </div>
</article>
<script
  async
  src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"
></script>
<br />
<br />
<hr />
<span id="busuanzi_container_page_pv">
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</span>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/search/">搜索</a></li>
        
          <li><a href="/tags/">标签</a></li>
        
          <li><a href="/categories/">分类</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/ggx21">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">1.</span> <span class="toc-text"> 1.准备数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AEsaveload"><span class="toc-number">1.0.1.</span> <span class="toc-text"> 读写数据:(save&amp;load)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text"> 读写模型:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81sldata%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.0.2.1.</span> <span class="toc-text"> 代码(slData模式):</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text"> 2.创建模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.0.1.</span> <span class="toc-text"> 多层感知机的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.1.1.</span> <span class="toc-text"> 例子:</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AAflattenlayer%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.2.</span> <span class="toc-text"> 一个flattenlayer的例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.3.</span> <span class="toc-text"> 一个模型的例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text"> 初始化模型参数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.</span> <span class="toc-text"> 总结:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text"> 3.定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">4.</span> <span class="toc-text"> 4.定义优化器和学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text"> 5.训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">6.</span> <span class="toc-text"> 6.使用模型进行预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">7.</span> <span class="toc-text"> 7.模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.1.</span> <span class="toc-text"> 模型欠拟合与过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%9C%B0"><span class="toc-number">7.1.1.</span> <span class="toc-text"> 简单地:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AEtraining-error%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AEgeneralization-error"><span class="toc-number">7.1.2.</span> <span class="toc-text"> 训练误差（training error）和泛化误差（generalization error）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">7.1.3.</span> <span class="toc-text"> 训练集\验证集\测试集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.2.</span> <span class="toc-text"> 应对过拟合:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8Fweight-decay"><span class="toc-number">7.2.1.</span> <span class="toc-text"> 1.权重衰减（weight decay）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="toc-number">7.2.1.1.</span> <span class="toc-text"> 实现代码：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E4%B8%A2%E5%BC%83%E6%B3%95dropout"><span class="toc-number">7.2.2.</span> <span class="toc-text"> 2.丢弃法(dropout)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.2.2.1.</span> <span class="toc-text"> 实现:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cnn"><span class="toc-number">8.</span> <span class="toc-text"> CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">8.1.</span> <span class="toc-text"> 0.前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8A%BF"><span class="toc-number">8.1.1.</span> <span class="toc-text"> 优势?</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">8.2.</span> <span class="toc-text"> 1.填充和步幅:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85padding"><span class="toc-number">8.2.0.1.</span> <span class="toc-text"> 填充padding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85stride"><span class="toc-number">8.2.0.2.</span> <span class="toc-text"> 步幅stride</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E4%B8%8E%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.</span> <span class="toc-text"> 2.多输入通道与多输出通道.</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">8.3.1.</span> <span class="toc-text"> 多输入通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.2.</span> <span class="toc-text"> 多输出通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">8.3.3.</span> <span class="toc-text"> 1*1卷积层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E6%B1%A0%E5%8C%96"><span class="toc-number">8.4.</span> <span class="toc-text"> 2.池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90"><span class="toc-number">8.5.</span> <span class="toc-text"> 一些例子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lenet"><span class="toc-number">8.5.1.</span> <span class="toc-text"> LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">8.5.1.1.</span> <span class="toc-text"> 实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#alexnet"><span class="toc-number">8.5.2.</span> <span class="toc-text"> AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A8%8D%E5%BE%AE%E7%AE%80%E5%8C%96%E7%9A%84alexnet"><span class="toc-number">8.5.2.1.</span> <span class="toc-text"> 稍微简化的AlexNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vgg%E5%9D%97"><span class="toc-number">8.6.</span> <span class="toc-text"> VGG块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nin%E5%9D%97"><span class="toc-number">8.7.</span> <span class="toc-text"> NiN块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#59-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9Cgooglenet"><span class="toc-number">8.8.</span> <span class="toc-text"> 5.9 含并行连结的网络（GoogLeNet）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">8.9.</span> <span class="toc-text"> 批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet"><span class="toc-number">8.10.</span> <span class="toc-text"> 残差网络ResNet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnn"><span class="toc-number">9.</span> <span class="toc-text"> RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E7%9A%84%E4%B8%80%E4%B8%AA%E9%87%8D%E8%A6%81%E5%BA%94%E7%94%A8%E5%B0%B1%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.1.</span> <span class="toc-text"> RNN的一个重要应用就是语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#n%E5%85%83%E8%AF%AD%E6%B3%95"><span class="toc-number">9.1.1.</span> <span class="toc-text"> n元语法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%91%E7%90%86%E8%A7%A3%E7%9A%84rnn"><span class="toc-number">9.2.</span> <span class="toc-text"> 我理解的RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-2"><span class="toc-number">9.2.1.</span> <span class="toc-text"> 例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">9.3.</span> <span class="toc-text"> 构建数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="toc-number">9.3.0.1.</span> <span class="toc-text"> 随机采样</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%B8%E9%82%BB%E9%87%87%E6%A0%B7"><span class="toc-number">9.3.0.2.</span> <span class="toc-text"> 相邻采样</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.4.</span> <span class="toc-text"> RNN的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#native-rnn_layer%E7%9A%84%E8%BE%93%E5%85%A5"><span class="toc-number">9.4.0.1.</span> <span class="toc-text"> native rnn_layer的输入</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#native-rnn_layer%E7%9A%84%E8%BE%93%E5%87%BA"><span class="toc-number">9.4.0.2.</span> <span class="toc-text"> native rnn_layer的输出</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83gru"><span class="toc-number">9.5.</span> <span class="toc-text"> 门控循环单元GRU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86lstm"><span class="toc-number">9.6.</span> <span class="toc-text"> 长短期记忆LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E5%A4%8D%E6%9D%82%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.7.</span> <span class="toc-text"> 更复杂的模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.7.1.</span> <span class="toc-text"> 深度循环神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.7.2.</span> <span class="toc-text"> 双向循环神经网络</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://ggxloveslife.top/notes/pytorch/learn_torch/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&text=Dive-into-DL-PyTorch"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&is_video=false&description=Dive-into-DL-PyTorch"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Dive-into-DL-PyTorch&body=Check out this article: https://ggxloveslife.top/notes/pytorch/learn_torch/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://ggxloveslife.top/notes/pytorch/learn_torch/&name=Dive-into-DL-PyTorch&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://ggxloveslife.top/notes/pytorch/learn_torch/&t=Dive-into-DL-PyTorch"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;   2020-2024 Andy Gao
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       -->
        <li>
          <a href="/"
            >首页</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/search/"
            >搜索</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/tags/"
            >标签</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/categories/"
            >分类</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/archives/"
            >归档</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/about/"
            >关于</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a target="_blank" rel="noopener" href="http://github.com/ggx21"
            >项目</a
          >
        </li>
        <!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = 'Ggx21/comments.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = '✨';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>



  <script src='https://unpkg.com/mermaid@9.0.0/dist/mermaid.min.js'></script>
  <script>
    mermaid.initialize({theme: 'forest'});
  </script>



</body>
</html>
