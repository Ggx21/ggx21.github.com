<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>deadlines</title>
      <link href="/2023/05/30/homework/deadlines/"/>
      <url>/2023/05/30/homework/deadlines/</url>
      
        <content type="html"><![CDATA[<blockquote><p>日拱一卒未有尽，​      功不唐捐终到海。</p></blockquote><details> <summary><b>图例</summary><div class="danger"><p>时间紧，任务重，I’m f**ked up</p></div><div class="warning"><p>时间不多了,under pressure</p></div><div class="tips"><p>时间还早,totally under control</p></div><div class="success"><p>写完了😄</p></div></details><h2 id="发现红框没有被check，可以来压力我😄">发现红框没有被check，可以来压力我😄</h2><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="danger"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox0"><label for="checkbox0">毛概案例分析 十二周周日 </label><mark>周末再说吧</mark></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox1"><label for="checkbox1">第12周DLCE</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox2"><label for="checkbox2">数电12周作业</label></p></li></ul></div></details><hr><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="warning"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox3"><label for="checkbox3">网原大作业(联机实验部分)</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox4"><label for="checkbox4">计图pa3</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox5" checked="true"><label for="checkbox5">网原实验3</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox6" checked="true"><label for="checkbox6">数电实验报告</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox7" checked="true"><label for="checkbox7">数电作业</label></p></li></ul></div></details><hr><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="tips"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox8"><label for="checkbox8">习概作业 17周周日</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox9"><label for="checkbox9">计图大作业</label></p></li></ul></div></details><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="success"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox10" checked="true"><label for="checkbox10">希腊语第三次作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox11" checked="true"><label for="checkbox11">计图pa2</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox12" checked="true"><label for="checkbox12">人智导第二次作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox13" checked="true"><label for="checkbox13">网原第九次作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox14" checked="true"><label for="checkbox14">网原第十次作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox15" checked="true"><label for="checkbox15">概统第十次作业</label></p></li><li class="lvl-2"><p>[ ]</p></li></ul></div></details>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DLC12</title>
      <link href="/2023/05/08/notes/DLC/week12/"/>
      <url>/2023/05/08/notes/DLC/week12/</url>
      
        <content type="html"><![CDATA[<h1>t</h1><h1>同步时序电路的分析与设计方法</h1><ul class="lvl-0"><li class="lvl-2"><p>作原始状态表👈</p><blockquote><p>就是一大状态机？</p></blockquote></li><li class="lvl-2"><p>化简原始状态表</p><blockquote><p>就是自动机化简?</p></blockquote></li><li class="lvl-2"><p>求出激励函数</p><blockquote><p>根据化简后的状态机给出$input,Q_i与Q_i’$的关系</p><p>从而反推出$D_i$,(对于JK触发器,当然是$J_i,K_i$)</p><p>给一个JK触发器的表?–&gt;easy实际上也就是4种情况.不难</p><p>卡诺图,给出$D_i与input,Q_i$之间的关系</p></blockquote></li><li class="lvl-2"><p>画逻辑图</p><blockquote><p>给出$D_i与input,Q_i$之间的关系后,用组合逻辑电路表达这个关系</p></blockquote></li></ul><h3 id="设计计数器时应该注意的问题">设计计数器时应该注意的问题</h3><ul class="lvl-0"><li class="lvl-2"><p>将技术循环外的状态修改后,自动进入计数循环/</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> DLC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DLC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CH6-3</title>
      <link href="/2023/05/08/notes/network/week11/"/>
      <url>/2023/05/08/notes/network/week11/</url>
      
        <content type="html"><![CDATA[<p>[mpls](<a href="https://info.support.huawei.com/info-finder/encyclopedia/zh/MPLS.html">MPLS是什么？MPLS是如何工作的？ - 华为 (huawei.com)</a>)，参考电路转发，在 MPLS 域的入口处，给每一个 IP 数据报加上 标签，然后对加上标记的 IP 数据报用硬件进行转发</p><p>MPLS报文结构<br>• MPLS被称为2.5层协议<br>• “给 IP 数据报加标签”其实就是在以太网的帧首部和IP数据报的首部之间插入一个 4 字节的 MPLS 首部</p><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/network/week11.assets/image-20230508101104162.png" alt="image-20230508101104162"></p><h2 id="路由协议小结（6-3-6-6）">路由协议小结（6.3-6.6）</h2><p>• RIP：距离向量，邻居交互最优下一跳，分布式计算；收敛速度较慢<br>• OSPF：链路状态，洪泛原始信息，集中计算最短路；带宽&amp;计算开销大<br>• BGP：融合RIP和OSPF设计经验，邻居交互路径向量；支撑大网路由<br>• MPLS：借鉴电路交换，面向连接；更强控制，实现VPN和TE</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>network-E3</title>
      <link href="/2023/05/08/homework/network/network-E3/"/>
      <url>/2023/05/08/homework/network/network-E3/</url>
      
        <content type="html"><![CDATA[<h1>网原第三次小实验</h1><p>郭高旭 2021010803 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a></p><h2 id="抓包实验-1：">抓包实验 1：</h2><img src="/img/homeworks/network-E3.assets/image-20230508182726680.png" alt="image-20230508182726680" style="zoom: 80%;" /><p>Type字段为11，Code字段为0</p><h2 id="抓包实验2">抓包实验2</h2><h3 id="观察ICMPv4请求">观察ICMPv4请求</h3><img src="/img/homeworks/network-E3.assets/image-20230508184458514.png" alt="image-20230508184458514" style="zoom:67%;" /><p>Type字段是8，Code是0</p><h3 id="观察回显数据包头">观察回显数据包头</h3><img src="/img/homeworks/network-E3.assets/image-20230508184710176.png" alt="image-20230508184710176" style="zoom:67%;" /><p>Type字段为0，Code字段也为0</p><h3 id="观察一对请求和回显">观察一对请求和回显</h3><img src="/img/homeworks/network-E3.assets/image-20230508185019527.png" alt="image-20230508185019527" style="zoom:67%;" /><p>标识符，序号，Data都相等</p><h2 id="抓包实验-3：-观察-ARP-分组各式">抓包实验 3： 观察 ARP 分组各式</h2><h3 id=""><img src="/img/homeworks/network-E3.assets/image-20230508185440661.png" alt="image-20230508185440661"></h3><ol><li class="lvl-3"><p>ARP 协议在以太网帧头中载荷类型的编号是 ？</p><img src="/img/homeworks/network-E3.assets/image-20230508185700796.png" alt="image-20230508185700796" style="zoom:67%;" /><p>答：0x0806</p></li><li class="lvl-3"><p>ARP 分组头中，以太网硬件类型编号和 IP 协议类型编号分别是？</p><img src="/img/homeworks/network-E3.assets/image-20230508185717796.png" alt="image-20230508185717796" style="zoom:67%;" /><p>答：   arp.hw.type（Hardware type）：1；protocol type 0x0800</p></li><li class="lvl-3"><p>ARP 请求分组中，操作码（Opcode）值是？源 IP 地址及 MAC地址，目的 IP 地址及 MAC 地址是多少？</p>  <img src="/img/homeworks/network-E3.assets/image-20230508185831466.png" alt="image-20230508185831466" style="zoom:67%;" /><ul class="lvl-2"><li class="lvl-5"><p>Opcode是1，</p></li><li class="lvl-5"><p>源IP和MAC地址是本机地址</p></li><li class="lvl-5"><p>目的IP地址是请求的IP地址；MAC地址是广播地址</p></li></ul></li><li class="lvl-3"><p>ARP 回复分组中，操作码（Opcode）值是？源 IP 地址及 MAC地址，目的 IP 地址及 MAC 地址是多少？</p><img src="/img/homeworks/network-E3.assets/image-20230508185731913.png" alt="image-20230508185731913" style="zoom:80%;" /><p>答：</p><ul class="lvl-2"><li class="lvl-5"><p>Opcode是2</p></li><li class="lvl-5"><p>源IP和MAC是请求的IP地址和回复的MAC</p></li><li class="lvl-5"><p>目的IP和MAC是本机</p></li></ul></li></ol><h2 id="简述题：">简述题：</h2><h3 id="icmpv4的安全隐患">icmpv4的安全隐患</h3><ol><li class="lvl-3"><p>Ping Flood 攻击：黑客可以发送大量的 Ping 请求（ICMP Echo Request），导致被攻击的目标主机网络堵塞，以至于无法正常通信。</p></li><li class="lvl-3"><p>Smurf 攻击：黑客可以利用广播地址发送 ICMPv4 Request，让目标网络中的所有主机都回应，导致目标网络瘫痪。</p></li><li class="lvl-3"><p>ICMPv4 Redirect 攻击：黑客可以伪造 ICMPv4 Redirect 消息，使目标主机将数据包发送到攻击者的设备上，从而实现中间人攻击。</p></li><li class="lvl-3"><p>ICMPv4 Time Exceeded 攻击：黑客可以发送大量 ICMPv4 Time Exceeded 消息，让目标网络的路由器被迫处理大量无用信息而瘫痪。</p></li><li class="lvl-3"><p>Ping of Death 攻击：黑客可以向目标主机发送一个特别制作的超大 ICMPv4 报文（超过 65536 字节），导致目标主机崩溃或者重启。</p></li></ol><p>处于上述安全问题，很多系统都默认禁止发送某些icmpv4消息。比如</p><ol><li class="lvl-3"><p>Cisco 路由器默认情况下会禁止发送 ICMP Redirect 消息，以避免中间人攻击。</p></li><li class="lvl-3"><p>Solaris 操作系统默认情况下会禁止发送 ICMPv4 Timestamp 和 ICMPv4 Address Mask 消息。</p></li><li class="lvl-3"><p>Windows 防火墙默认情况下会阻止多个 ICMPv4 消息，比如 ICMP Echo Request（ping）、ICMP Timestamp，ICMP Traceroute 等。</p></li></ol><h3 id="ping-局域网内外的主机产生的arp报文有何不同">ping 局域网内外的主机产生的arp报文有何不同</h3><p>这是我ping局域网内（连接到我同一个路由器的ipad设备）的arp</p><p><img src="/img/homeworks/network-E3.assets/image-20230508192914176.png" alt="image-20230508192914176"></p><p>ping局域网外的主机不会产生arp报文。arp的作用范围在局域网内。</p><h3 id="arp滥用的危害及预防">arp滥用的危害及预防</h3><h4 id="arp滥用的例子">arp滥用的例子</h4><ol><li class="lvl-3"><p>网络拥塞：过量的 ARP 请求会增加网络的负载，导致网络拥塞，从而影响网络性能和吞吐量。</p></li><li class="lvl-3"><p>ARP 洪泛攻击：攻击者利用大量伪造的 ARP 请求数据包向网络广播，欺骗物理地址表中的信息，从而导致网络拥塞或服务故障，影响网络安全。</p></li><li class="lvl-3"><p>ARP 欺骗攻击：攻击者伪造 ARP 请求数据包，使其包含正确的 IP 地址和错误的 MAC 地址。当 ARP 请求被接收并响应时，攻击者就能欺骗网络设备，从而执行中间人攻击，获取敏感信息，甚至劫持通信。</p></li></ol><h4 id="预防">预防</h4><ol><li class="lvl-3"><p>监控网络流量：使用网络分析工具监测网络流量，识别异常的 ARP 请求和响应，判断是否存在 ARP 洪泛攻击或 ARP 欺骗攻击。</p></li><li class="lvl-3"><p>过滤不必要的广播：在网络设备上设置合理的 ACL（访问控制列表），过滤不必要的广播。例如，可以限制 ARP 请求的范围，只允许目标设备响应有效的 ARP 请求。</p></li><li class="lvl-3"><p>配置静态 ARP 表项：在网络设备上配置静态 ARP 表项，避免频繁地进行 ARP 请求，降低网络负载和运行的安全风险。</p></li><li class="lvl-3"><p>使用防火墙：在网络的边界处使用防火墙，限制不必要的 ARP 请求和回应，防止 ARP 洪泛攻击和 ARP 欺骗攻击。</p></li><li class="lvl-3"><p>更新网络设备和操作系统：最新版本的网络设备和操作系统中会修复已知的安全漏洞，更新这些设备和系统可以降低不必要的攻击和风险。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>network-homework-10</title>
      <link href="/2023/05/08/homework/network/network10/"/>
      <url>/2023/05/08/homework/network/network10/</url>
      
        <content type="html"><![CDATA[<h1>网原第10次作业</h1><p>郭高旭 2021010803 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a></p><h2 id="第一题">第一题</h2><blockquote><p>这是否意味着虚电路网络不需要具备将单个数据包从任意源端路由到任意接收方的能力呢？</p></blockquote><p>答:不是,如果某一源端路由到某一接收方没有建立虚电路.对于这单个数据包仍然要进行虚电路入口到出口的路由选择。</p><h2 id="第二题">第二题</h2><blockquote><p>设网络采用区分服务模型。考虑使用加速转发服务的用户。试问是否可以保证加速型数据包比常规数据包的延迟更短？为什么是，或者为什么不是？</p></blockquote><p>答:不一定能保证.</p><p>区分服务模型可能为加速转发服务预留一些带宽,所以加速型数据包通常延迟较短.</p><p>但实际情况比较复杂,不能保证加速型数据包延迟一定短,比如:</p><ul class="lvl-0"><li class="lvl-2"><p>区分服务模型为加速转发服务预留的带宽较小.</p></li><li class="lvl-2"><p>同时使用加速转发服务的用户非常多,导致网络拥塞.</p></li><li class="lvl-2"><p>常规数据包经过路径较短</p></li><li class="lvl-2"><p>链路质量、网络设备性能等因素的影响</p></li></ul><h2 id="第三题">第三题</h2><blockquote><p>IPv6 使用 16 个字节的地址。如果每隔 ps 就分配掉一百万个地址，试问整个地址空间可以持续分配多久？</p></blockquote><p>答:16个字节约有$2^{128}\approx 10<sup>{38.4}$个地址,可以分配$10</sup>{32.4}ps \approx 2.5\times 10^ {20}s \approx 10^{13}y$</p><p>大概能分配10万亿年</p><h2 id="第四题">第四题</h2><blockquote><p>IPv6 协议被引入时， ARP 协议需要作相应的改变吗？如果需要，这种改变是概念性的还是技术性的</p></blockquote><p>答:技术上的改变.</p><p>arp协议的作用是将IP翻译成MAC.概念上不发生变化.</p><p>但是IPV6的地址变长了,所以ARP协议需要区分IPv6与IPv4</p>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新的一天</title>
      <link href="/2023/05/07/Somniloquy/dream-5/"/>
      <url>/2023/05/07/Somniloquy/dream-5/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="/2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h2 id="01：30">01：30</h2><p>又是熬夜debug的一天。</p><p>头痛，哥几个在边上叽叽喳喳，为了明天的ddl而烦恼。</p><p>我在为什么烦恼呢？</p><img src="/img/dream-5.assets/43c413a0eb28b8022006e3ad03c3d6a.jpg" alt="43c413a0eb28b8022006e3ad03c3d6a" style="zoom: 33%;" /><img src="/img/dream-5.assets/494e46ac3c5119e636969a698a2db8b.jpg" alt="494e46ac3c5119e636969a698a2db8b" style="zoom:33%;" /><p>算了，睡觉吧</p><h2 id="3：30">3：30</h2><p>真没睡。。。</p><p>昨天猛干，到了4：30//</p><p>实际上效率很低，虽然没有分心，但是制造了好多bug。让我一直de了两天，或许写c++时头脑应该清醒点，，，毕竟和python不一样</p><img src="/img/dream-5.assets/7cb34bbc4c9add0cae53bfb77b3e868.jpg" alt="7cb34bbc4c9add0cae53bfb77b3e868" style="zoom:33%;" /><p>想起多年前的这个清晨。。。</p><h2 id="13：00">13：00</h2><p>她等了我，和我一起来了，但是抛下我走了。</p><p>但我是赚了的，至少我学了一下午，还de完了bug</p><p>她走是她的事，，，</p><p>我不理解//</p><p>也许陌生人也不行，她还是走了。我迟了一步、、</p><p>图书馆坐坐真不错。</p><img src="/img/dream-5.assets/3c294bef447dc6b43036c9928459409.jpg" alt="3c294bef447dc6b43036c9928459409" style="zoom: 33%;" /><p>我想给人分享我的喜悦，好久没有体验过一点点成就感了。。。</p><p>环顾了四周还是算了。给妈妈打个电话吧</p><h2 id="17：00">17：00</h2><img src="/img/dream-5.assets/6f078288c956c220d3a3cc1bf01567f.jpg" alt="6f078288c956c220d3a3cc1bf01567f" style="zoom:33%;" /><img src="/img/dream-5.assets/4cab0fe65c6b43b98bb202a27577717.jpg" alt="4cab0fe65c6b43b98bb202a27577717" style="zoom:33%;" /><p>工字厅门口这小树林真不错，以后多来散散心。</p><p>还是能约到人一起吃饭的，开心。</p><h2 id="18：30">18：30</h2><p>吃完饭发现又有新的作业了，没那么开心了。。。。</p><h2 id="23：00">23：00</h2><img src="/img/dream-5.assets/a6b8f7fbc8862d86f2244427dcbd9d1.jpg" alt="a6b8f7fbc8862d86f2244427dcbd9d1" style="zoom:33%;" /><p>relax</p><p>😋😋😋</p><p>陈粒好贵，而且这一对比那一对还少400</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cobra</title>
      <link href="/2023/05/06/others/cobra/"/>
      <url>/2023/05/06/others/cobra/</url>
      
        <content type="html"><![CDATA[<!DOCTYPE html><html>  <head>    <title>Snake Game</title>    <style>      html, body {        overflow: hidden;      }      body {        display: flex;        justify-content: center;        align-items: center;        height: 100vh;        overflow: hidden;      }      canvas {        border: 1px solid black;        background-color: #f2f2f2;        font-family: Arial, sans-serif;      }    </style>  </head>  <body>    <canvas id="canvas" width="400" height="400"></canvas>    <script src="/js/snake.js"></script>  </body></html>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> game </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2048</title>
      <link href="/2023/05/06/others/game-2048/"/>
      <url>/2023/05/06/others/game-2048/</url>
      
        <content type="html"><![CDATA[<!DOCTYPE html><html lang="en">  <head>    <meta charset="UTF-8" />    <title>2048 Game</title>    <link rel="stylesheet" type="text/css" href="/2048/style.css" />  </head>  <body>    <div class="container">      <div class="header">        <h1>2048 Game</h1>        <div class="score">Score: <span id="score">0</span></div>      </div>      <div class="game-grid"></div>      <div class="game-over">        <p>Game Over!</p>        <button id="retry" onclick="restartGame()">Retry</button>      </div>    </div>    <script src="/2048/app.js"></script>  </body></html>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> game </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy-4</title>
      <link href="/2023/05/05/Somniloquy/dream-4/"/>
      <url>/2023/05/05/Somniloquy/dream-4/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="/2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h2 id="22：18">22：18</h2><p>昨天去爬了香山。</p><p>在双清公寓坐了一会儿，有一搭没一搭地聊着。感觉很是惬意。</p><p>以后可以多去，远离屏幕远离人群</p><p>没被她从山上推下去，真是万幸。。。</p><img src="\img\dream-4\1.jpg" style="zoom: 25%;" /><img src="\img\dream-4\2.jpg" style="zoom: 25%;" /><img src="\img\dream-4\3.jpg" style="zoom: 25%;" /><p>今天起了床就写ai-pa2的文档，写了一天，啥也没干。现在是写作业呢还是开摆呢？</p><h2 id="19：00">19：00</h2><p>柯洁这棋都能输？打出那一勺后是什么感觉呢？尝试搅局的时候又是什么感觉。</p><h2 id="23：00">23：00</h2><h3 id="dd丢了">dd丢了</h3><p>dd走丢了，他去哪里了呢？她妈妈报了警。等一等看看警察叔叔是不是要来把我们抓走了。<br><img src="\img\dream-4\4.jpg" style="zoom: 25%;" /></p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人智导PA2实验报告</title>
      <link href="/2023/05/04/notes/pytorch/pa2report/"/>
      <url>/2023/05/04/notes/pytorch/pa2report/</url>
      
        <content type="html"><![CDATA[<ul class="lvl-0"><li class="lvl-2"><p><a href="https://github.com/Ggx21/IAI2">作业仓库</a></p></li><li class="lvl-2"><p><a href="https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/">pytorch学习笔记</a></p></li><li class="lvl-2"><p><a href="https://cloud.tsinghua.edu.cn/f/91b9791557cb4e1f9821/">代码文件</a></p></li></ul><p>本次实验实现了下面的module</p><table><thead><tr><th>模型名称</th><th>测试准确率</th><th>测试f值</th></tr></thead><tbody><tr><td>CNN_simple</td><td>83.46%</td><td>0.8322</td></tr><tr><td>CNN_complex</td><td>81.75%</td><td>0.8167</td></tr><tr><td>RNN</td><td>82.54%</td><td>0.8247</td></tr><tr><td>DenseNet</td><td>75.95%</td><td>0.7546</td></tr><tr><td>ResNet</td><td>77.43%</td><td>0.7725</td></tr><tr><td>LSTM</td><td>80.21%</td><td>0.8010</td></tr><tr><td>BiLSTM</td><td>81.68%</td><td>0.8158</td></tr><tr><td>GoogLeNet</td><td>74.40%</td><td>0.7367</td></tr><tr><td>MLP</td><td>70.94%</td><td>0.7047</td></tr><tr><td>MLP_with_Dropout</td><td>74.82%</td><td>0.7464</td></tr></tbody></table><h2 id="0-实验流程">0.实验流程</h2><h3 id="文件结构图">文件结构图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">D:.</span><br><span class="line"></span><br><span class="line">│  dataprocess.py#------数据预处理</span><br><span class="line">│  DLongpu.py#-----------main.py</span><br><span class="line">│  module.py#---储存,调用module的类</span><br><span class="line">│</span><br><span class="line">├─Dataset#提供的数据</span><br><span class="line">│      test.txt</span><br><span class="line">│      train.txt</span><br><span class="line">│      validation.txt</span><br><span class="line">│      wiki_word2vec_50.bin</span><br><span class="line">│      word_freq.txt</span><br><span class="line">├─input#预处理后的数据</span><br><span class="line">│      test_input.pkl</span><br><span class="line">│      train_input.pkl</span><br><span class="line">│      validation_input.pkl</span><br><span class="line">│      word2vec.pkl</span><br><span class="line">│</span><br><span class="line">├─module#储存的模型</span><br><span class="line">│      BiLSTM.pkl</span><br><span class="line">│      CNN_complex.pkl</span><br><span class="line">│      CNN_simple.pkl</span><br><span class="line">│      ...</span><br><span class="line">|</span><br><span class="line">├─plot#生成的图片</span><br><span class="line">│      all_acc_F1.png</span><br><span class="line">|...</span><br></pre></td></tr></table></figure><h3 id="执行过程">执行过程</h3><h4 id="一-数据预处理-DataProcesser-类">一.数据预处理:<strong>DataProcesser</strong>()类</h4><ol><li class="lvl-3"><p>构建词表vocab:</p><ul class="lvl-2"><li class="lvl-5">取得词频表,保留出现频率高于treshold(我取了10,一共获得8072个词)的词</li><li class="lvl-5">为词频表中的词建立索引</li><li class="lvl-5">对于词频表中每一个词,利用提供的转换文件,转换为50维词向量(如果转换表中没有这个词,我的做法是随机一个,实际上由于我们只保留了一定出现频率的词,没有出现过的词占比很少,对结果影响不大)</li></ul></li><li class="lvl-3"><p>读取句子:</p><ul class="lvl-2"><li class="lvl-5">读取句子,记录label与commet</li><li class="lvl-5">通过索引,将comment从词列表转化为索引列表(如果没有索引,转换成pad_char对应的id,这里把中性词&quot;把&quot;作为pad_char)</li><li class="lvl-5">统一句子长度:<ul class="lvl-4"><li class="lvl-7">通过统计手段,97.5%的评论次数在96个词以内,统一句子长度为96(当然也可以是别的长度)</li><li class="lvl-7">超过96的句子则截断</li><li class="lvl-7">低于96的句子,我的操作是重复这个句子直到长度为96.(另一种方法是填充pad_id,但是要注意不要训练pad_id)</li></ul></li></ul></li><li class="lvl-3"><p>转换为词向量</p><ul class="lvl-2"><li class="lvl-5">在构建词表时已经完成了索引,根据词的索引找到词向量即可.</li></ul></li></ol><p><strong>DataProcesser</strong>使用方法,执行该类示例的<code>run</code>方法即可.最终生成了<code>input</code>文件夹下转换成词向量列表的句子.(分别有训练集,测试集,验证集,和word2vec的索引)</p><h4 id="二-获得模型GetModule-类">二.获得模型<strong>GetModule</strong>()类</h4><p>module.py文件中定义了<strong>GetModule</strong>()类,它会调用参数<code>module_name</code>对应的模型,并用我调整过的参数初始化这个模型,对于某些模型起到adapter的作用,对输入加以处理,使得可以在主函数中使用统一的输入模式而不加以修改.可以在module.py文件中方便地注册更多的模型</p><h4 id="三-主函数DLongpu-py"><a href="http://xn--ehq.xn--DLongpu-m73kj3zuw4c.py">三.主函数DLongpu.py</a></h4><p>主要是定义了<strong>PredictClass</strong>()</p><ol><li class="lvl-3"><p>通过上述<strong>GetModule</strong>()初始化自己的模型(包括了对应的<em>损失函数和优化器</em>)</p></li><li class="lvl-3"><p>读取预处理的数据,并通过pytorch的Dataset和Dataloader准备好数据</p></li><li class="lvl-3"><p>开始训练epoch次数</p><ol><li class="lvl-6">分batch训练：将训练数据随机划分为多个batch，每个batch被输入到模型中进行训练。(Dataloader的功能)</li><li class="lvl-6">正向传播：将当前batch的数据输入到模型中，进行正向传播计算得到输出结果。</li><li class="lvl-6">计算loss：根据输出结果和标签，计算模型的损失函数（loss）。</li><li class="lvl-6">反向传播：根据损失函数，使用链式法则计算每个参数的梯度，然后传回模型中更新参数，以减小损失函数。</li><li class="lvl-6">重复迭代：重复以上步骤直到所有batch都被训练过一遍，一个epoch训练结束。</li><li class="lvl-6">训练结束后开启eval模式,利用验证集对效果进行验证</li></ol></li><li class="lvl-3"><p>训练结束后展示在测试集的效果</p></li><li class="lvl-3"><p>储存模型</p></li></ol><p>实际上,可以通过main函数中的Mode标签,切换成test模式,读取已经储存的模型.</p><p>此外,还可使用<strong>PredictClass</strong>()中的real_test方法,利用了jieba分词对用户真实的输入进行预测.经过我的实验,效果还挺不错的.</p><p><img src="/img/ai_pa2/image-20230505190124084.png" alt="image-20230505190124084"></p><p>​效果还是不错的</p><h2 id="1-模型的结构图">1.模型的结构图</h2><p>下面分别介绍模型的结构图</p><ul class="lvl-0"><li class="lvl-2"><h4 id="CNN-simple">CNN_simple</h4><pre class="mermaid">  graph TB    A(batch_X) -->|Conv2d| C(Batch,通道数=卷积核数目,)    C -->|BatchNorm2d| D(Batch,对每一通道数据归一化处理)    D -->|ReLU,MaxPool2d|F(Batch, kernel_num, 1, 1)    F -->|View| G(Batch, kernel_num将输出拼接起来)    G -->|Dropout| H(Batch, kernel_num)    H -->|Linear and Softmax| I(全连接,将输出长度变为2,并对输出softmax)</pre></li></ul><img src="/img/ai_pa2/image-20230505191146457.png" alt="image-20230505191146457" style="zoom:50%;" /><p>注意我和课本不同的是我使用了n个相同大小卷积核,在池化的时候由于卷积后仍是2维,所以是二维的最大池化(图源课件ppt,稍作修改)</p><h4 id="CNN-complex">CNN_complex</h4><p>使用了ppt中原始cnn模型前半部分作为一个模块cnn_net</p><img src="/img/ai_pa2/image-20230505191959273.png" alt="image-20230505191959273" style="zoom: 33%;" /><pre class="mermaid">graph LR    A(batch_X) -->C(cnn_net1)-->E(flattern)-->G(maxpool1D)-->I(view拼接)    A-->B(cnn_net2)-->F(flattern)-->H(maxpool1D)-->I-->Z(全连接+softmax)</pre><ol><li class="lvl-3"><p><em>将两个形状是(批量大小, 词数, 词向量维度)的嵌入层的输出按词向量连结</em></p></li><li class="lvl-3"><p><em>根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维(即词向量那一维)，变换到前一维</em></p></li><li class="lvl-3"><p><em>对于每个一维卷积层，在时序最大池化后会得到一个形状为(批量大小, 通道大小, 1)的</em> <em>Tensor。</em></p></li><li class="lvl-3"><p>拼接在一起后通过全连接层得到输出</p></li></ol><h4 id="RNN-我的实现也就是单层的LSTM">RNN(我的实现也就是单层的LSTM)</h4><pre class="mermaid">graph LRinput[Input] --> encoder[LSTM layer]encoder --> output[Output]encoder --> concatenation[Concatenation]concatenation --> decoder[Linear layer]decoder --> result[Result]</pre><p>具体结构解释：</p><ul class="lvl-0"><li class="lvl-2"><p>输入：输入数据，形状为 $batch \times seq_length \times embed_size$</p></li><li class="lvl-2"><p>LSTM layer：包含多层LSTM的编码器，将输入数据序列传入编码器后，得到输出序列和最终时间步的隐藏状态 -</p></li><li class="lvl-3"><p>输出：输出序列 $output_seq$，形状为 $seq_length \times batch \times num_hiddens$</p></li><li class="lvl-4"><p>Concatenation：将最终时间步的隐藏状态 $h$ 和初始时间步的隐藏状态 $c$ 沿最后一个维度拼接起来，形状为 $batch \times 2*num_hiddens$</p></li><li class="lvl-4"><p>Linear layer：线性层，将拼接后的向量输入，输出一个二元向量</p></li><li class="lvl-4"><p>Result：模型的输出，输出层的结果，形状为 $batch \times 2$</p></li></ul><img src="/img/ai_pa2/image-20230505192857489.png" alt="image-20230505192857489" style="zoom:50%;" /><p>其中depth为1,如果在torch中设置<code>bidirectional==true</code>,可以得到双向RNN</p><h4 id="MLP">MLP</h4><img src="/img/ai_pa2/image-20230505193124875.png" alt="image-20230505193124875" style="zoom:50%;" /><p>很多层的全连接+激活函数.</p><p>激活函数是必要的,否则多层会退化为单层</p><h4 id="GoogLeNet">GoogLeNet</h4><p>主要是inception模块</p><img src="/img/ai_pa2/image-20230505193239795.png" alt="image-20230505193239795" style="zoom:33%;" /><p>串联多个inception模块,最后池化+全连接</p><h4 id="DenseNet-ResNet等不再画出-完全参照ppt">DenseNet\ResNet等不再画出,完全参照ppt</h4><h2 id="2-实验结果">2.实验结果</h2><ul class="lvl-0"><li class="lvl-2"><h4 id="所有模型最终在测试集上表现的结果">所有模型最终在测试集上表现的结果.</h4></li></ul><img src="/img/ai_pa2/all_acc_F1.png" alt="all_acc_F1" style="zoom: 67%;" /><p>​可以看见CNN,RNN都达到了f值高于0.8的要求</p><ul class="lvl-0"><li class="lvl-2"><p>所有模型训练准确率随epoch的变化</p><img src="/img/ai_pa2/train_acc.png" alt="train_acc" style="zoom:72%;" /></li><li class="lvl-2"><p>所有模型训练f值随epoch的变化</p><img src="/img/ai_pa2/train_f1.png" alt="train_f1" style="zoom:67%;" /><p>本题给的数据中正负标签的比例差不多,而且我的模型看上去对正负完全不敏感.所以最终f值的计算结果和准确率差别不大.</p></li><li class="lvl-2"><p>所有模型训练f值随epoch的变化</p><img src="/img/ai_pa2/train_loss.png" alt="train_loss" style="zoom:72%;" /></li></ul><p>处于对计算力和时间的综合考虑,还有对我可怜的1650的保护,我选取的epoch比较小(16)可以看见各种模型的loss值仍有下降趋势,在对单个模型的测试过程中,增加epoch数确实可能增加训练准确率,减小loss.但有的模型已经出现比较明显的过拟合现象</p><ul class="lvl-0"><li class="lvl-2"><p>所有模型验证准确率随epoch的变化</p><img src="/img/ai_pa2/val_acc.png" alt="val_acc" style="zoom:72%;" /></li><li class="lvl-2"><p>所有模型验证准确率随epoch的变化</p><img src="/img/ai_pa2/val_f1.png" alt="val_f1" style="zoom:67%;" /><p>明显地,在验证集上,随epoch增加,准确率并不一定增加,甚至不升反降</p></li></ul><h2 id="3-使用的不同参数效果，并分析原因">3.使用的不同参数效果，并分析原因</h2><p>主要的参数</p><ul class="lvl-0"><li class="lvl-2"><p>epoch：太低达不到效果，太高出现过拟合</p></li><li class="lvl-2"><p>learrning-rate：太高波动太大，表现为准确率根本不能提升。设太低学习太慢，训练需epoch轮数多</p></li><li class="lvl-2"><p>Dropout：Dropout是指在模型训练时，随机将一部分神经元输出设置为0，从而防止模型过拟合的一种技术。在PyTorch中，我们可以通过调整Dropout概率来控制每个神经元输出为0的概率。Dropout概率过低可能会导致过拟合，而概率过高则可能会影响模型的表现。</p></li><li class="lvl-2"><p>Batch Size：指每次模型训练时参与训练的样本数量。如果Batch Size设置过小，可能会出现训练时间过长的问题；而如果Batch Size过大，可能会出现内存不足的问题。</p></li><li class="lvl-2"><p>kernel_nums：对于我表现最好的cnn-simple。我的卷积核数目增加（100–&gt;256），准确率提升较明显</p></li></ul><h2 id="4-不同模型之间的简要比较">4.不同模型之间的简要比较</h2><ul class="lvl-0"><li class="lvl-2"><p>在我对每个模型分别进行测试时,达到最高效果的实际上是CNN-complex模型</p></li><li class="lvl-2"><p>我实现的CNN,RNN模型确实在测试时表现比Baseline模型(MLP模型)强</p></li><li class="lvl-2"><p>但是MLP模型在训练集上的表现非常好,很快到达了95%+</p></li><li class="lvl-2"><p>认为MLP模型太容易过拟合,因此给他加上几个dropout,确实有点用,在测试集上表现略有变好</p></li><li class="lvl-2"><p>在我没有实现MLP的时候,实验表明我DenseNet,GoogleNet等几个复杂模型过拟合程度比较高</p></li><li class="lvl-2"><p>但是最简单的MLP的训练与测试表现差异反而最大.</p></li></ul><h2 id="5-问题思考">5.问题思考</h2><ul class="lvl-0"><li class="lvl-2"><h4 id="实验训练什么时候停止是最合适的？">实验训练什么时候停止是最合适的？</h4><p><strong>Early Stop</strong>:理论上讲可以通过验证集动态调整停止时间,在验证集效果达到最好时就停止训练.操作上可以在验证集几轮epoch都没有增长时停止.</p><p>但是earlystop有几个问题:</p><ul class="lvl-2"><li class="lvl-4"><p>比较复杂,不方便我实验框架下统一的实现.</p></li><li class="lvl-4"><p>有时候模型准确率会突变,比如googleNet</p><img src="/img/ai_pa2/The%20accuracy%20of%20GoogLeNet%20module.png" alt="The accuracy of GoogLeNet module" style="zoom: 50%;" /><p>所以我最终采用了固定epoch数的方式.</p><p>但是实际上几个我调过参数模型在epoch数为40左右表现最好</p></li></ul></li><li class="lvl-2"><h4 id="实验参数的初始化是怎么做的？">实验参数的初始化是怎么做的？</h4><ul class="lvl-2"><li class="lvl-4"><p>模型的参数基本上是pytorch默认方法.</p></li><li class="lvl-4"><p>例如：nn.Linear 和 nn.Conv2D，都是在 [-limit, limit] 之间的均匀分布（Uniform distribution），其中 limit 是 $1. / sqrt(fan_{in}) ，fan_{in}$ 是指参数张量（<a href="https://so.csdn.net/so/search?q=tensor&amp;spm=1001.2101.3001.7020">tensor</a>）的输入单元的数量。</p></li><li class="lvl-4"><p>此外对于RNN可以考虑正交初始化（Orthogonal Initialization）</p><p>主要用以解决深度网络下的梯度消失、梯度爆炸问题，是在RNN中经常使用的参数初始化方法。<code>nn.init.orthogonal</code>但实际上差别没有感觉</p></li></ul></li><li class="lvl-2"><h4 id="有什么方法可以方式训练过程陷入过拟合。">有什么方法可以方式训练过程陷入过拟合。</h4><p>下面是按照我实验中的效果明显程度排序</p><ol><li class="lvl-5"><p>选取更加合适的模型:</p><p>这个是</p></li><li class="lvl-5"><p>调整epoch,在合理时机停止</p></li><li class="lvl-5"><p>增加dropout层,调整dropout概率</p><p>看baseline模型,认为MLP模型太容易过拟合,因此给他加上几个dropout,确实有点用,在测试集上表现确实略有变好.但是对其他比较复杂的模型来说,dropout影响有限</p></li><li class="lvl-5"><p>参数正则化</p><p>计算loss时添加正则化项</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">L1_reg = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> self.module.parameters():</span><br><span class="line">     L1_reg += torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">loss += <span class="number">0.001</span> * L1_reg  <span class="comment"># lambda=0.001</span></span><br></pre></td></tr></table></figure><p>这样可以惩罚某些较大的参数.按照我的理解就是防止考试偏科.</p><p>但实在是没什么用</p></li><li class="lvl-5"><p>调整参数.</p><p>dropout\learning-rate等等都会影响过拟合</p></li><li class="lvl-5"><p>增加训练集大小</p><p>一个操作是可以通过k-折,将原有训练集与验证集合并后随机分成k份.这样可以增加训练集大小</p></li><li class="lvl-5"><p>动态停止epoch</p></li><li class="lvl-5"><p>动态调整</p></li></ol><h2 id="6-试分析CNN，RNN，全连接神经网络（MLP）三者的优缺点。">6.试分析CNN，RNN，全连接神经网络（MLP）三者的优缺点。</h2></li></ul><p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。</p><p>如果把CNN的核的大小看成和输入一样,那么实际上MLP可以看作是CNN的一个特例.</p><ul class="lvl-0"><li class="lvl-2"><p>MLP</p><ul class="lvl-2"><li class="lvl-4">全连接神经网络（MLP）的优点在于比较简单,它可以捕捉到输入数据的非线性关系，能够应用于各种领域。</li><li class="lvl-4">参数较多,单层计算起来比较复杂,容易过拟合…正则化等技术在MLP上的效果比较好</li><li class="lvl-4">单层只能表示一个维度的信息.对空间的敏感性不如CNN</li></ul></li><li class="lvl-2"><p>RNN</p><ul class="lvl-2"><li class="lvl-4"><p>对于序列数据的处理具有很强的能力，尤其在语音识别、自然语言处理和语音合成等任务中表现优异。</p></li><li class="lvl-4"><p>RNN具有记忆能力，它可以传递信息并保留之前输入的状态，从而在处理时序信息时取得较好的效果。</p></li><li class="lvl-4"><p>RNN的缺点在于，训练复杂度高，而且受梯度消失/爆炸问题的限制，其在长序列的语音和自然语言任务中表现可能较差。</p></li></ul></li><li class="lvl-2"><p>CNN</p><ul class="lvl-2"><li class="lvl-4">单层计算量比全连接小</li><li class="lvl-4">参数共享和稀疏交通,能够以较小的计算和储存成本生成大量参数。</li><li class="lvl-4">可以捕捉到多个层级的特征,比如宽高.对空间敏感性更好,因此常常应用在计算机视觉中</li><li class="lvl-4">但是CNN还是有两个非常危险的缺陷：平移不变性和池化层。我的理解是CNN对位置太敏感了,一个卷积核的神经元识别一只猫,猫动一动就认不出来了.池化层虽然可以一定程度上削弱这个问题,但是带来了数据信息丢失等新问题.</li></ul></li></ul><h2 id="7-实验心得">7.实验心得</h2><ul class="lvl-0"><li class="lvl-2"><p>利用五一假期,我从零认真学习了一波pytorch,所有的心得体会记录在了我的<a href="https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/">这篇博客</a>里</p></li><li class="lvl-2"><p>感觉这个实验解释性实在是有点低,哪个参数更好,哪个模型更好,什么层可以解决什么问题,基本上还是要靠实验.书本的知识往往并不一定适用</p></li><li class="lvl-2"><p><a href="https://github.com/Ggx21/IAI2">作业仓库</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dive-into-DL-PyTorch</title>
      <link href="/2023/05/03/notes/pytorch/learn_torch/"/>
      <url>/2023/05/03/notes/pytorch/learn_torch/</url>
      
        <content type="html"><![CDATA[<h2 id="1-准备数据">1.准备数据</h2><p>在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。</p><h4 id="读写数据-save-load">读写数据:(save&amp;load)</h4><ul class="lvl-0"><li class="lvl-2"><p><code>torch.save(x, 'x.pt')</code>:将x存在文件名同为<code>x.pt</code>的文件里。</p></li><li class="lvl-2"><p>将数据从存储的文件读回内存<code>x2 = torch.load('x.pt')</code></p><p>不仅是tensor.dict啥啥的数据都能这么存储.</p><p>无论是文件大小还是读写速度都完爆json</p></li></ul><h4 id="读写模型">读写模型:</h4><ul class="lvl-0"><li class="lvl-2"><p>两个方式:仅保存和加载模型参数(<code>state_dict</code>)(推荐)；保存和加载整个模型。</p><p>差别就是,第一种方法在SL时候只SL参数,下次L时先建一个新model,然后把参数L进去就好了.</p></li></ul><h5 id="代码-slData模式">代码(slData模式):</h5><p>保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是pt或pthCopy to clipboardErrorCopied</span></span><br></pre></td></tr></table></figure><p>加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure><p>一个小问题在于不同设备(CPU与GPU,不同GPU之间.具体遇到问题再说吧)</p><h2 id="2-创建模型">2.创建模型</h2><p>模型是深度学习的核心，它决定了最终学习的效果。在pytorch中，可以通过继承nn.Module类来创建模型，并在其中定义前向计算函数。</p><blockquote><p>我的理解是,模型就是一个千层饼.每一层有个什么功能.下面就是一层饼的例子.</p><p>实际上并不是层数越多越好.</p><ul class="lvl-1"><li class="lvl-2"><p>当神经网络的层数较多时，模型的数值稳定性容易变差:$0.99^ {365}=?;1.01^{365=?}$</p></li><li class="lvl-2"><p>如何解决?批量归一化</p></li></ul><p>通过不停地tensor运算联立方程,是不是可以把千层饼等效为单层?</p></blockquote><h4 id="多层感知机的设计">多层感知机的设计</h4><p>如果千层饼被等效为单层,那么对层的设计就失去了意义.上面的问题在于.</p><blockquote><p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p></blockquote><p>解决方法是我们加入一些非线性变换的层</p><h5 id="例子">例子:</h5><ul class="lvl-0"><li class="lvl-2"><p>ReLU（rectified linear unit）函数<br>$$<br>ReLU(x)=max(x,0).<br>$$</p></li><li class="lvl-2"><p>sigmoid<br>$$<br>sigmoid(x)= \frac 1 {1+exp(−x)}<br>$$<br><img src="/img/learn_torch.assets/3.8_sigmoid.png" alt="img" style="zoom:33%;" /></p><img src="/img/learn_torch.assets/3.8_sigmoid_grad.png" alt="img" style="zoom:33%;" /></li><li class="lvl-2"><p>tanh函数(双曲正切)<br>$$<br>tanh(x)= \frac{1+exp(−2x)}{1−exp(−2x)}<br>$$<br>图像和sigmoid差不多,但是0附近更陡峭一些</p></li></ul><h4 id="一个flattenlayer的例子">一个flattenlayer的例子</h4><p>这里有一个把(batchsize,*,*,…)的多维tensor转换为(batchsize,***)的二维tensor的例子.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FlattenLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="一个模型的例子">一个模型的例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict([</span><br><span class="line">        (<span class="string">&#x27;flatten&#x27;</span>, FlattenLayer()),<span class="comment">#看,就是这样一层层搭起来...</span></span><br><span class="line">        (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">    ])</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数">初始化模型参数:</h3><p>构建好了模型,我们得有初始参数啊.</p><p>**太好了!**PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略,一般不用我们考虑;如果你真的想知道可参考pytorch<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules">源代码)</a></p><h3 id="总结">总结:</h3><ol><li class="lvl-3"><p>继承Module类</p><p><code>Module</code>类是<code>nn</code>模块里提供的一个模型构造类，是所有神经网络模块的基类</p><p>一般来说,我们重载<code>Module</code>类的<code>__init__</code>函数和<code>forward</code>函数。它们分别用于创建模型参数和定义前向计算。无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的<code>backward</code>函数。</p><p>注意:重载<code>__init__</code>函数时应该首先调用父类<code>module</code>的初始化函数e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure></li><li class="lvl-3"><p>module的子类:还有一些<code>Sequential</code>,<code>ModuleDict</code>,<code>ModuleList</code>之类的东西.我感觉没什么用啊.好像只是让我把一堆Module以一种整齐的方式凑在一起.<code>Sequential</code>好像还挺有用的,至少他要保证层之间输入输出维度匹配,可以直接forward.</p></li><li class="lvl-3"><p>共享模型参数: <code>Module</code>类的<code>forward</code>函数里多次调用同一个层,层之间参数共享。此外，如果我们传入<code>Sequential</code>的模块是同一个<code>Module</code>实例的话参数也是共享的</p><blockquote><p>真的有人在乎参数究竟是什么吗?</p></blockquote></li><li class="lvl-3"><p>自定义层:见[flattenlayer](#### 一个flattenlayer的例子)的例子,这是一个不带参数的层,你当然也可以带参数.但意义是?</p></li><li class="lvl-3"><p>我想:module就是层的累加.一个头进一个头出.<code>人体蜈蚣.</code></p></li></ol><h2 id="3-定义损失函数">3.定义损失函数</h2><p>常见的损失函数有交叉熵损失函数、均方误差损失函数等，可以根据不同的任务选择不同的损失函数。</p><blockquote><p>例子:</p><p>PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure></blockquote><h2 id="4-定义优化器和学习率">4.定义优化器和学习率</h2><p>在训练的过程中，需要使用优化器来更新模型参数。常见的优化器有梯度下降法、Adam等。同时，由于学习率对训练效果影响较大，所以需要定义学习率的初始值以及变化规则。</p><blockquote><p>e.g.</p><p>我们使用学习率为0.1的小批量随机梯度下降作为优化算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></blockquote><h2 id="5-训练模型">5.训练模型</h2><p>将数据、模型、损失函数、优化器和学习率等组合在一起，循环执行前向计算、损失计算、反向传播和参数更新等步骤，即可训练模型。在每个epoch结束后，还需要对模型进行评估。</p><div class="tips"><p>在训练之前可以测试输入形状比如：</p><p>在训练ResNet之前，我们来观察一下输入形状在ResNet不同模块之间的变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27; output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></div><h2 id="6-使用模型进行预测">6.使用模型进行预测</h2><p>在训练好模型后，可以使用它对新的数据进行预测，并输出预测结果或概率。</p><h2 id="7-模型选择">7.模型选择</h2><p>我们怎么评价模型的好坏呢?</p><h3 id="模型欠拟合与过拟合">模型欠拟合与过拟合</h3><h4 id="简单地"><strong>简单地:</strong></h4><ul class="lvl-0"><li class="lvl-2"><p><strong>给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。</strong></p></li><li class="lvl-2"><p><strong>过少的训练样本</strong>也会导致过拟合</p></li></ul><h4 id="训练误差（training-error）和泛化误差（generalization-error）">训练误差（training error）和泛化误差（generalization error）</h4><p>当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确。这是为什么呢？</p><p>以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。</p><p>训练的时候是根据往年题的表现来的(通过减小训练误差).所以往年题做的好不一定代表着高考考的好.(一般来说训练误差的期望$\leq$泛化误差)</p><p>但我们关注的是<strong>泛化误差</strong></p><h4 id="训练集-验证集-测试集">训练集\验证集\测试集</h4><p>高考只能考一次,我们没有办法从训练集(看着答案做题)中知道自己的高考表现.所以我们可以拿一些测试集训练集之外的数据作为<strong>验证集</strong>(模考).</p><p>操作上,我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p><blockquote><p><em>K</em>折交叉验证（K-<em>K</em>-fold cross-validation）</p><p>分出K个子集来.这样可以测K次</p></blockquote><p>但实际上,由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊.</p><h3 id="应对过拟合">应对过拟合:</h3><h4 id="1-权重衰减（weight-decay）">1.权重衰减（weight decay）</h4><blockquote><p>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</p></blockquote><p>权重衰减等价于$L_2$范数正则化（regularization）</p><p><strong>我的理解</strong>:就是在计算损失函数的时候加上一项$λ\times L_2$其中λ是一个超参。$L_2$：参数权重越大，$L_2$越大。这样大概保证了各参数大小都差不多，不偏科。</p><h5 id="实现代码：">实现代码：</h5><p>直接在构造优化器实例时通过<code>weight_decay</code>参数来指定权重衰减超参数。默认下，PyTorch会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line"><span class="comment">#...在optimize时</span></span><br><span class="line"><span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br></pre></td></tr></table></figure><h4 id="2-丢弃法-dropout">2.丢弃法(dropout)</h4><img src="/img/learn_torch.assets/3.13_dropout.svg" alt="img" style="zoom: 80%;" /><p>在隐藏层随机丢弃一个单元.比如上图,隐藏层原来有$h_1,h_2,…,h_5.h_2,h_5 $被丢弃了.</p><p>这样计算时不会过度依赖某一个隐藏单元</p><ul class="lvl-0"><li class="lvl-2"><blockquote><p>(和权重衰减的目的大概是一样的),都是保证不偏科.要不然模考物理总是很难,就你一个考100,人家都是0分.到了高考一赋分大家都考90,你傻眼了.</p></blockquote></li></ul><h5 id="实现">实现:</h5><p>在PyTorch中，我们只需要在全连接层后添加<code>Dropout</code>层并指定丢弃概率。在训练模型时，<code>Dropout</code>层将以指定的丢弃概率随机丢弃上一层的输出元素</p><p>在测试模型时（即<code>model.eval()</code>后），<code>Dropout</code>层并不发挥作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><blockquote><p>丢弃法只在训练模型时使用。</p></blockquote><h2 id="CNN">CNN</h2><p>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。</p><h3 id="0-前置知识">0.前置知识</h3><p>在net里加上一个卷积层就形成了<strong>CNN</strong></p><p>一个小翻译:<code>Kernel</code>在pytorch的卷积计算时就叫做<code>filter</code>.</p><p>CNN的学习就是学习这个<code>kernel</code>,(相应的,上面在学习一个参数tensor)</p><p>那个经典的图像边缘检测实际上在互相关.kernel走一步,算一圈,下个蛋…</p><p>卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>.有什么区别?**没有区别!**反正都是学出来的,学出来的核再上下左右转换一下那么两种运算就交换了.所以根本不需要知道什么是卷积就能CNN.</p><h4 id="优势">优势?</h4><p>考虑图像分类问题。每张图像高和宽均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p><ol><li class="lvl-3"><p>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</p></li><li class="lvl-3"><p>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为1000像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状是3,000,000×2563,000,000×256：它占用了大约3 GB的内存或显存。这带来过复杂的模型和过高的存储开销。</p></li></ol><p>一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p><h3 id="1-填充和步幅">1.填充和步幅:</h3><p>要想使用一个卷积层,操作上显见的问题就是我们要知道这个卷积层输入输出的形状.</p><p>一般来说,对于第i维来说,如果输入在第i维长度?<code>有没有更好的名字</code>是$n_i$,核长度是$k_i$那么输出在第i维长度就是($n_i-k_i+1$),显然,$k_i$不应该超过$n_i$,否则会报错</p><h5 id="填充padding">填充padding</h5><p>**太长不看:**取padding=({$\frac {kernelsize_i-1} 2$,…})可以保证输入输出形状相同</p><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。</p><p>如果在第i维的两侧<strong>一共填充$p_i$行</strong></p><p>那么输出形状第i维将会是($n_i-k_i+p_i+1$)</p><p>所以通常可以把$p_i设置为k_i-1$来使输入输出具有相同形状</p><p>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p><mark>注意</mark>:操作上<code>padding=(2, 1)</code>padding的参数代表着在kernel两侧分别填充多少.也就是说$=p_i/2$,所以尽量也把核的每一维长度取奇数,这样保证了$p_i/2$是个整数.否则还得考虑两边分别取floor和ceiling,<strong>麻烦死了</strong></p><h5 id="步幅stride">步幅stride</h5><p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p>步幅可以按比例缩小形状</p><p>如果按上一步设置理想的padding，同时如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将缩为原来的1/stride倍.其他情况自己算去,但何必为难自己.</p><h3 id="2-多输入通道与多输出通道">2.多输入通道与多输出通道.</h3><h4 id="多输入通道">多输入通道</h4><p>实际上不就是多了一维吗?比如彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。就是新增了一个长度为3(r,g,b)的一维.然后我们用三个卷积核叠一块,发现在色彩维度上insize和kernersize都是3,然后这一维长度就变成1,退化了.</p><p>实现含多个输入通道的互相关运算。我们只需要对每个通道做互相关运算，然后通过<code>add_n</code>函数来进行累加。</p><h4 id="多输出通道">多输出通道</h4><p>哎呀我一阵头晕目眩,不想知道它是怎么算的了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积层----------------------------------------看这里</span></span><br><span class="line">conv_layer = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">output_tensor = conv_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量形状</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)  <span class="comment"># torch.Size([16, 10, 32, 32])</span></span><br></pre></td></tr></table></figure><p>直接来看示例代码吧.知道它怎么算干嘛呢?</p><p>在上面的代码中，<code>input_tensor</code> 是一个批次大小为 16、通道数为 3、高度为 32、宽度为 32 的输入张量，<code>conv_layer</code> 是一个输出通道数为 10、卷积核大小为 3x3、步幅为 1、填充为 1 的卷积层，<code>output_tensor</code> 是卷积层的输出张量，它的形状为 <code>[16, 10, 32, 32]</code>。</p><p>这是gpt说的啊,我觉着挺对的.可以自己跑着试试.</p><h4 id="1-1卷积层">1*1卷积层</h4><p>一个长宽都是1的核</p><p>1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。</p><p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么1×1卷积层的作用与全连接层等价</strong>。</p><p>通过这个来调整参数的通道数,控制模型复杂度.e.g.语数外+理综to语数外+三门选科</p><h3 id="2-池化">2.池化</h3><p><strong>缓解卷积层对位置的过度敏感性</strong>。(想想抗锯齿操作,都是通过采样来使得过渡更加平滑??)</p><p>池化窗口形状为p<em>×</em>q<em>的池化层称为p</em>×<em>q</em>池化层，其中的池化运算叫作p<em>×</em>q池化。</p><ul class="lvl-0"><li class="lvl-2"><p>池化层的输出通道数跟输入通道数相同。</p></li><li class="lvl-2"><p>默认情况下，<code>MaxPool2d</code>实例里步幅和池化窗口形状相同。当然可以自己设定.但是池化后的形状又要算一算了吗?</p></li></ul><h3 id="一些例子">一些例子</h3><p><strong>下面的例子,即使是LeNet,都完全够应付作业了</strong>完全没有必要看</p><h4 id="LeNet">LeNet</h4><p><img src="/img/learn_torch.assets/5.5_lenet.png" alt="img"></p><p><strong>卷积层块里的基本单位是卷积层后接最大池化层</strong>：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。</p><h5 id="实现-2">实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>先conv再flatten,大概都是这么个套路</p><h4 id="AlexNet">AlexNet</h4><p>很多分类工作流程是:获取数据集–&gt;获得特征–&gt;进行分类.</p><p>之前认为DL的工作只是使用机器学习模型对特征分类。使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p><p>特征可以学习吗?&gt;&gt; AlexNet</p><p><img src="/img/learn_torch.assets/5.6_alexnet.png" alt="img"></p><h5 id="稍微简化的AlexNet">稍微简化的AlexNet</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>真的,体验上来说,LeNet做作业就完全够了…</p></blockquote><ul class="lvl-0"><li class="lvl-2"><p>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</p></li><li class="lvl-2"><p>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</p></li></ul><h3 id="VGG块"><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.7_vgg?id=_571-vgg%E5%9D%97">VGG块</a></h3><p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×33×3的卷积层后接上一个步幅为2、窗口形状为2×22×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数。</p><blockquote><p>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p></blockquote><h3 id="NiN块"><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.8_nin?id=_581-nin%E5%9D%97">NiN块</a></h3><p>我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在5.3节（多输入通道和多输出通道）里介绍的1×11×1卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×11×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。</p><p>上面这俩我回头再看,头晕😵‍💫😵‍💫😵‍💫😵‍💫</p><h3 id="5-9-含并行连结的网络（GoogLeNet）"><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.9_googlenet?id=_59-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88googlenet%EF%BC%89">5.9 含并行连结的网络（GoogLeNet）</a></h3><p>如果说上面两个是网络之间串行,那么GoogLeNet的基本块Inception块是并行的(并行里套了串行)</p><img src="/img/learn_torch.assets/5.9_inception.svg" alt="inception块" /><p>Inception块里有4条并行的线路。前3条线路使用窗口大小分别是1×1、3×3和5×5的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1×1卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用3×3最大池化层，后接1×1卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p><p>这个更复杂了😵‍💫😵‍💫😵‍💫😵‍💫具体实现抄抄链接里的吧.我真的好奇他怎么保证每次输入输出通道匹配的</p><h3 id="批量归一化">批量归一化</h3><ul class="lvl-0"><li class="lvl-2"><p>在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p></li><li class="lvl-2"><p>对全连接层和卷积层做批量归一化的方法稍有不同。</p></li><li class="lvl-2"><p>批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。</p></li><li class="lvl-2"><p>PyTorch提供了BatchNorm类方便使用。</p></li></ul><p>Pytorch中<code>nn</code>模块定义的<code>BatchNorm1d</code>和<code>BatchNorm2d</code>类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的<code>num_features</code>参数值。下面我们用PyTorch实现使用批量归一化的LeNet。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>可以试试把这个加到自己的module里,</p><blockquote><p>它就是个插件,可以加上它,而不用对其他东西做任何修改,我觉着挺好的</p></blockquote><h3 id="残差网络ResNet">残差网络ResNet</h3><p>我饿了🥱🥱🥱不学了.</p><h2 id="RNN">RNN</h2><h3 id="RNN的一个重要应用就是语言模型">RNN的一个重要应用就是语言模型</h3><p>假设一段长度为$T$的文本中的词依次为$w_1, w_2, \ldots, w_T$，那么在离散的时间序列中，$w_t$（$1 \leq t \leq T$）可看作在时间步（time step）$t$的输出或标签。给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型将计算该序列的概率：</p><p>$$<br>P(w_1, w_2, \ldots, w_T).<br>$$<br>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，我们有</p><p>$$<br>P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1}).<br>$$</p><h4 id="n元语法">n元语法</h4><p>当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$）。如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。如果基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p><p>$$<br>P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .<br>$$</p><h3 id="我理解的RNN">我理解的RNN</h3><p>上面的例子:把一个句子看做词序列,每个时间步产生一个词…当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。但是,我们可以通过一个隐藏变量来传递前面几个时间步的信息…该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。</p><h4 id="例子-2">例子</h4><p>引入一个新的权重参数${W}_{hh} \in {R}^{h \times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p><p>$$<br>{H}<em>t = \phi({X}<em>t{W}</em>{xh}+{H}</em>{t-1}{W}_{hh}+{b}_h)<br>$$</p><p>其中${H}_{t-1}$就是上一个时间步的隐藏变量</p><p><img src="/img/learn_torch.assets/6.2_rnn.svg" alt="img"></p><h3 id="构建数据集">构建数据集</h3><p>对时序数据的采样方式有如下两种(采样就是选取batchsize个样本,如果batchsize=1,那么就不需要考虑如何采样了)</p><ol><li class="lvl-3"><h5 id="随机采样">随机采样</h5><p>在数据集中随机选取batchsize个样本,由于不同样本之间是独立的,所以不能直接把上个样本计算后的隐藏状态传给下个样本作为初始隐藏状态</p></li><li class="lvl-3"><h5 id="相邻采样">相邻采样</h5><p>字面意思,在数据集中选取相邻的batchsize个样本,这样只需在每一个迭代周期开始时初始化隐藏状态.</p><p>但是这样计算梯度时会依赖所有序列??</p><p>为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</p></li></ol><h3 id="RNN的实现">RNN的实现</h3><blockquote><p>我终于明白了,我们好像只需要关注不同layer的输入输出形状…</p><p>至于我的net为什么有他们,who ™ cares??</p></blockquote><p>PyTorch中的<code>nn</code>模块提供了循环神经网络的实现。下面构造一个含单隐藏层、隐藏单元个数为256的循环神经网络层<code>rnn_layer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><h5 id="native-rnn-layer的输入">native rnn_layer的输入</h5><p><code>rnn_layer</code>的输入形状为(时间步数, 批量大小,<mark>input_size</mark>)</p><p>就是有点别扭.</p><ul class="lvl-0"><li class="lvl-2"><p>第一个时间步数就是句子长度</p></li><li class="lvl-2"><p>第二个批量大小,我真无语,为什么要把批量大小放在第二个,感觉不是很直观.所以cnn和rnn的数据不能直接通用,要view一下交换一下前两个维度?还是别的什么维度</p></li><li class="lvl-2"><p>第三个inut_size;这里和cnn不同,如果说cnn的input单元是句子整体.那么RNN的input实际上是词向量.所以说如果是one-hot,那么就是vocab_size;如果已经word2vec后,就是<strong>词向量的长度</strong>…这里注意一下不同教程的区别.</p></li></ul><h5 id="native-rnn-layer的输出">native rnn_layer的输出</h5><p>形状为(时间步数, 批量大小, 隐藏单元个数)。</p><p>前向计算后会分别返回输出和隐藏状态h，其中输出指的是隐藏层在<strong>各个时间步</strong>上计算并输出的隐藏状态，它们通常作为后续输出层的输入。参考下图(这是LSTM,和RNN本身不很一样)</p><p><img src="/img/learn_torch.assets/6.5.png" alt="img"></p><h3 id="门控循环单元GRU">门控循环单元GRU</h3><p>当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p><p><strong>GRU</strong>就是为了解决上述问题</p><p>GRU包含重置门和更新门,计算图如下</p><p><img src="/img/learn_torch.assets/6.7_gru_3.svg" alt="img"></p><blockquote><p>简单来说,就是如果前面时间步的输入已经和这次输出没太大关系,重置门会重置隐藏状态</p><p>如果这次输入不太影响后面内容,更新门会把之前的隐藏状态传递下去.</p><ul class="lvl-1"><li class="lvl-2"><p>重置门有助于捕捉时间序列里短期的依赖关系；</p></li><li class="lvl-2"><p>更新门有助于捕捉时间序列里长期的依赖关系。</p></li></ul></blockquote><p><mark>注意</mark>:上面完全不需要理解.反正有native的GRU模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><h3 id="长短期记忆LSTM">长短期记忆LSTM</h3><p>另一种门控循环神经网络是LSTM（long short-term memory,长短期记忆)</p><blockquote><p>你看这名字,这不是和刚才的门干了一样的事吗??</p></blockquote><p>LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞，从而记录额外的信息。</p><p><mark>同样地</mark>,可以直接调用<code>rnn</code>模块中的<code>LSTM</code>类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><p><img src="/img/learn_torch.assets/6.8_lstm_2.svg" alt="img"></p><p>如果你想理解,我把图放这了.</p><p>我的理解是,这其实就是显示地干了GRU的活.</p><blockquote><ul class="lvl-1"><li class="lvl-2"><p>如果遗忘门与记忆细胞做$\odot$趋于1,那么就会倾向于保存下之前的记忆,如果趋于0就会遗忘</p></li><li class="lvl-2"><p>如果输入门与候选记忆细胞做$\odot$趋于1,那么就会倾向于更新记忆,如果趋于0就不会更新</p></li></ul></blockquote><h3 id="更复杂的模型">更复杂的模型</h3><h4 id="深度循环神经网络">深度循环神经网络</h4><p>目前为止介绍的循环神经网络只有一个单向的隐藏层，在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。下图演示了一个有L个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p><p><img src="/img/learn_torch.assets/6.9_deep-rnn.svg" alt="img"></p><h4 id="双向循环神经网络">双向循环神经网络</h4><p>之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。</p><p><img src="/img/learn_torch.assets/6.10_birnn.svg" alt="img"></p><blockquote><p>上面两个我感觉挺靠谱的,但是没有示例代码,所以我就不管了</p></blockquote><blockquote><p>update:其实双向循环网络就是加个参数的事</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BiRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab, embed_size, num_hiddens, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiRNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="built_in">len</span>(vocab), embed_size)</span><br><span class="line">        <span class="comment"># bidirectional设为True即得到双向循环神经网络</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size,</span><br><span class="line">                                hidden_size=num_hiddens,</span><br><span class="line">                                num_layers=num_layers,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始时间步和最终时间步的隐藏状态作为全连接层输入</span></span><br><span class="line">        self.decoder = nn.Linear(<span class="number">4</span>*num_hiddens, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后</span></span><br><span class="line">        <span class="comment"># 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)</span></span><br><span class="line">        embeddings = self.embedding(inputs.permute(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">        <span class="comment"># rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。</span></span><br><span class="line">        <span class="comment"># outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)</span></span><br><span class="line">        outputs, _ = self.encoder(embeddings) <span class="comment"># output, (h, c)</span></span><br><span class="line">        <span class="comment"># 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为</span></span><br><span class="line">        <span class="comment"># (批量大小, 4 * 隐藏单元个数)。</span></span><br><span class="line">        encoding = torch.cat((outputs[<span class="number">0</span>], outputs[-<span class="number">1</span>]), -<span class="number">1</span>)</span><br><span class="line">        outs = self.decoder(encoding)</span><br><span class="line">        <span class="keyword">return</span> outs</span><br></pre></td></tr></table></figure><blockquote><p>而多层就更不用教了…</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DLC</title>
      <link href="/2023/05/02/notes/DLC/%E8%A7%A6%E5%8F%91%E5%99%A8/"/>
      <url>/2023/05/02/notes/DLC/%E8%A7%A6%E5%8F%91%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="触发器">触发器</h2><h3 id="基本R-S触发器">基本R-S触发器</h3><h3 id="电位触发方式的触发器">电位触发方式的触发器</h3><h3 id="边沿触发方式的触发器">边沿触发方式的触发器</h3><ul class="lvl-0"><li class="lvl-2"><p>正边沿触发方式？</p><blockquote><p>分析电路？我还不会。什么阻塞不阻塞的</p></blockquote></li><li class="lvl-2"><p>负边沿触发方式？jk触发</p><blockquote><p>可以等同于jk触发方式吗?讨厌jk,完全不会</p></blockquote></li></ul><h3 id="主－从触发方式的触发器">主－从触发方式的触发器</h3><p>由两级电位触发器（主触发器和从触发器）串连而成</p><ul class="lvl-0"><li class="lvl-2"><p>CP=1期间，主触发器接收数据，从触发器封锁</p></li><li class="lvl-2"><p>在负跳变到来时，主触发器封锁，从触发器将接收CP负跳变时主触发器的状态。</p></li></ul><h4 id="主从R-S触发器的功能表">主从R-S触发器的功能表</h4><p>和基本R-S一样，（cp一个正脉冲，一个负脉冲）。大概只是为了保证触发器的稳定。</p><h2 id="同步时序逻辑电路的分析">同步时序逻辑电路的分析</h2><ol><li class="lvl-3"><p>列出状态关系</p><p>大概就是列出${Q_i}_{n+1}与{Q_i}_n$之间的递推关系</p><blockquote><ul class="lvl-3"><li class="lvl-2"><p>对于D触发器来说$Q_i’=D_i$</p></li><li class="lvl-2"><p>对于JK触发器来说$Q_i’=J\bar Q+\bar KQ$</p><blockquote><p>因为我并不会jk触发器,这一条实在是难以理解.死记硬背吧</p></blockquote></li><li class="lvl-2"><p>至于D\J\K等于什么就再说了</p><blockquote><p>你D可以和输入以及$Q_i$有关.J和K有必要吗?有.下标漏掉了应该是$Q_i’=J\bar Q_i+\bar KQ_i$还有$Q_j$呢</p></blockquote></li></ul></blockquote></li><li class="lvl-3"><p>列出状态表</p><p>根据上面内容可以列出状态表</p></li><li class="lvl-3"><p>画出状态图</p><ul class="lvl-2"><li class="lvl-5">如果有输入,那么就是一个状态机(自动机)</li><li class="lvl-5">否则就是一个退化了的状态机(每个结点只有一个出边)</li></ul></li><li class="lvl-3"><p>画出时序图</p><ul class="lvl-2"><li class="lvl-5">每一帧CP都是高低</li><li class="lvl-5">每一帧输入最多变一次</li><li class="lvl-5">根据状态图画时序图</li></ul><blockquote><p>这又不考虑延迟.都是理想状态,大概还挺简单的</p></blockquote></li></ol>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> DLC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DLC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>four-adder</title>
      <link href="/2023/05/02/homework/DLCE/%E5%9B%9B%E4%BD%8D%E5%8A%A0%E6%B3%95%E5%99%A8REPORT/"/>
      <url>/2023/05/02/homework/DLCE/%E5%9B%9B%E4%BD%8D%E5%8A%A0%E6%B3%95%E5%99%A8REPORT/</url>
      
        <content type="html"><![CDATA[<h1>四位加法器REPORT</h1><p>郭高旭 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a> 2021010803</p><h2 id="实验目的">实验目的</h2><ul class="lvl-0"><li class="lvl-2"><p>掌握组合逻辑电路的基本分析方法与设计方法；</p></li><li class="lvl-2"><p>理解半加器、全加器、加法器的分析与设计方法；</p></li><li class="lvl-2"><p>学会元件例化；</p></li><li class="lvl-2"><p>学会利用软件仿真实现对数字电路的验证与分析</p></li></ul><h2 id="实验内容">实验内容</h2><ul class="lvl-0"><li class="lvl-2"><p>设计半加器</p></li><li class="lvl-2"><p>利用半加器构建全加器</p></li><li class="lvl-2"><p>使用全加器构造逐次进位加法器，超前进位加法器</p></li><li class="lvl-2"><p>使用 VHDL自带的加法运算实现一个 4 位全加器</p></li><li class="lvl-2"><p>查看逐次进位加法器、超前进位加法器和 VHDL 自带加法器在CPLD中生成的电路,并比较这三者的异同</p></li></ul><h2 id="不同加法器的RTL与仿真结果">不同加法器的RTL与仿真结果</h2><h4 id="超前进位加法器">超前进位加法器</h4><h5 id="仿真结果">仿真结果</h5><img src="/img/fouradder.assets/1.png" alt="image-20230502202851109" style="zoom:65%;" /><p>超前进位加法器延迟：End-Start=17.33ns</p><h5 id="RTL">RTL</h5><img src="/img/fouradder.assets/2.png" alt="image-20230502202851109" style="zoom:65%;" /><h4 id="简单进位加法器">简单进位加法器</h4><h5 id="RTL-2">RTL</h5><img src="/img/fouradder.assets/3.png" alt="image-20230502202851109" style="zoom:65%;" />##### 仿真结果<img src="/img/fouradder.assets/4.png" alt="image-20230502202851109" style="zoom:65%;" /><p>超前进位加法器延迟：End-Start=17.96ns</p><h4 id="原生加法器">原生加法器</h4><h5 id="RTL-3">RTL</h5><img src="/img/fouradder.assets/5.png" alt="image-20230502202851109" style="zoom:65%;" /><h4 id="代码">代码</h4><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LIBRARY</span> ieee;</span><br><span class="line"><span class="keyword">USE</span> ieee.std_logic_1164.<span class="keyword">ALL</span>;</span><br><span class="line"><span class="keyword">USE</span> ieee.std_logic_arith.<span class="keyword">ALL</span>;</span><br><span class="line"><span class="keyword">USE</span> ieee.std_logic_unsigned.<span class="keyword">ALL</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTITY</span> native_adder <span class="keyword">IS</span></span><br><span class="line"><span class="keyword">PORT</span> (</span><br><span class="line">a, b : <span class="keyword">IN</span> <span class="built_in">STD_LOGIC_VECTOR</span>(<span class="number">3</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line">c0 : <span class="keyword">IN</span> <span class="built_in">STD_LOGIC</span>;</span><br><span class="line">f : <span class="keyword">OUT</span> <span class="built_in">STD_LOGIC_VECTOR</span>(<span class="number">3</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line">c3 : <span class="keyword">OUT</span> <span class="built_in">STD_LOGIC</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">END</span> nativee_adder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARCHITECTURE</span> plus <span class="keyword">OF</span> native_adder <span class="keyword">IS</span></span><br><span class="line"><span class="keyword">signal</span> buf: <span class="built_in">STD_LOGIC_VECTOR</span>(<span class="number">4</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">process</span>(a, b, c0)</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">buf &lt;= <span class="string">&quot;00000&quot;</span>+ a + b + c0;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">process</span>;</span><br><span class="line"><span class="keyword">process</span> (buf)</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">f &lt;= buf(<span class="number">3</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line">c3 &lt;= buf(<span class="number">4</span>);</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">process</span>;</span><br><span class="line"><span class="keyword">END</span> plus;</span><br></pre></td></tr></table></figure><h5 id="仿真结果-2">仿真结果</h5><img src="/img/fouradder.assets/6.png" alt="image-20230502202851109" style="zoom:65%;" /><p>原生加法器延时：10.03ns</p><h3 id="总结：">总结：</h3><p>我实现的超前进位加法器比普通进位加法器延迟略小。但原生加法器延时远小于我实现的。</p><h2 id="电路功能测试的结果">电路功能测试的结果</h2><ul class="lvl-0"><li class="lvl-2"><p>实际功能测试结果从计算结果上来说和仿真测试结果相同</p></li><li class="lvl-2"><p>实验时的延迟效果几乎都无法感知</p></li></ul><h2 id="调试中遇到的问题">调试中遇到的问题</h2><ul class="lvl-0"><li class="lvl-2"><p>对仿真操作不够熟悉，在学习仿真时花费了很多时间。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> DLCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> DLCE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy-3</title>
      <link href="/2023/05/01/Somniloquy/dream-3/"/>
      <url>/2023/05/01/Somniloquy/dream-3/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="/2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h2 id="10：18">10：18</h2><p>昨天晚上调整了本地文件的组织方式。这导致我页面的permalink基本全改变了。本来还有些评论的，这下全没了😢😢😢</p><h2 id="16：00">16：00</h2><p>太依赖gpt，已经丧失了动脑子的能力了。再也不能从代码里获得什么成就感了。是好是坏呢？总归是把graphics pa2干完了。这不就是把给的python参考翻译一遍。</p><h2 id="19：20">19：20</h2><p>桃李吃晚饭，土豆炖牛肉给我上了个夹生的。看在他给我换了两道菜的情况下就不骂了。<br><img src="\img\dream-3\1.jpg" style="zoom: 15%;" /></p><p>土豆烧牛肉换辣子鸡+西红柿炒鸡蛋，还可以吗？</p><p><strong>妈的，iai就给我92分。凭什么。</strong></p><h2 id="22-32">22:32</h2><p>心情突然很down，游戏也打不动了。意义是什么呢？在紫操溜溜，没有变好。<br>😿😿😿<br><img src="\img\dream-3\2.jpg" style="zoom: 25%;" /></p><h2 id="35-32">35:32</h2><p>爬了会儿格子，感到心情平静了些。竟然有这闲心。🚭🚭🚭</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>留言板</title>
      <link href="/2023/04/30/others/comment/"/>
      <url>/2023/04/30/others/comment/</url>
      
        <content type="html"><![CDATA[<p>这里专门为了评论.</p><p>但是你需要登录一下GitHub.</p><p>有没有更好的方式呢?</p><p>😶‍🌫️😶‍🌫️😶‍🌫️😶‍🌫️😶‍🌫️</p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> comment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy_2</title>
      <link href="/2023/04/30/Somniloquy/dream-2/"/>
      <url>/2023/04/30/Somniloquy/dream-2/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="/2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h2 id="8：00">8：00</h2><p>昨天晚上被g44拉了壮丁，练了一晚上拔河。今早上勇夺亚军。挺好的，有点事干总比天天胡思乱想好.<br><img src="/img/dream-2/1.jpg" style="zoom: 25%;" /></p><h2 id="11-36">11:36</h2><p>天天搞这前端玩物丧志，搞个<a href="/2023/04/30/homework/deadlines/">deadline页面</a>搞到现在。<br><strong>又没人看。</strong><br>一天到晚写些小学生流水帐😄中厅很安静。突然觉着这样写markdown与typora相比也挺舒服的。</p><h2 id="12：00">12：00</h2><p>看着窗外的云发呆。小时候天天看着云从这边飘到那边，现在一天到晚对着电子屏幕，为了什么呢？</p><p>昨天晚上还在和人说，感觉自己还是很有耐心。今天干了些事，发现耐心消耗殆尽了。<br>🤡🤡🤡<br><img src="/img/dream-2/2.jpg" style="zoom: 25%;" /></p><h2 id="15-05">15:05</h2><p>再写前端我就是那个<br>🐒🐒🐒🐒</p><h2 id="19：36">19：36</h2><p>本月只点了3次外卖。感觉生活变健康了。</p><p>但我是统计了红包使用次数得出来的数据，感觉大亏特亏。😿</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PA2 REPORT</title>
      <link href="/2023/04/29/homework/graphics/PA2REPORT/"/>
      <url>/2023/04/29/homework/graphics/PA2REPORT/</url>
      
        <content type="html"><![CDATA[<h1>图形学PA2 REPORT</h1><p>郭高旭 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a> 2021010803</p><h2 id="曲线性质">曲线性质</h2><h3 id="两种曲线的异同">两种曲线的异同</h3><p>相同点：</p><ol><li class="lvl-3"><p>都是基于控制点构造出的曲线；</p></li><li class="lvl-3"><p>都是多项式参数曲线，不能表示圆等曲线</p></li><li class="lvl-3"><p>都可以表示一条平滑的曲线。</p></li></ol><p>不同点：</p><ol><li class="lvl-3"><p>局部修改：</p><ul class="lvl-2"><li class="lvl-5">B 样条是局部的，容易对曲线进行修改、扩展和截断，而这些修改对曲线的其他部分几乎没有影响。</li><li class="lvl-5">Bezier曲线/曲面不支持局部的修改和编辑。</li></ul></li><li class="lvl-3"><p>拼接：B样条曲线可以拼接，Bezier曲线/曲面拼接时，满足几何连续性条件是十分困难的。</p></li><li class="lvl-3"><p>B样条曲线具有下面一系列好的性质</p><ul class="lvl-2"><li class="lvl-5">仿射不变性</li><li class="lvl-5">直线保持性</li><li class="lvl-5">凸包性</li><li class="lvl-5">变差缩减性</li><li class="lvl-5">几何不变性</li><li class="lvl-5">etc.</li></ul></li></ol><h3 id="怎样绘制一个首尾相接且接点处也有连续性质的-B-样条">怎样绘制一个首尾相接且接点处也有连续性质的 B 样条</h3><ul class="lvl-0"><li class="lvl-2"><p>首尾相接：</p><p>将控制点复制一遍，在第一组控制点前面加入最后一组控制点，最后一组控制点后面加入第一组控制点。</p></li><li class="lvl-2"><p>保持连续性质</p><p>通过设置相邻控制顶点的位置和导数值相等来保证一阶连续性</p></li></ul><h2 id="代码逻辑">代码逻辑</h2><ol><li class="lvl-3"><p>首先通过discretize获取我刚刚实现的curve（一系列点的三维坐标V以及曲线在这一点的切向量T）</p></li><li class="lvl-3"><p>指定旋转步数step，进而获得每次旋转的角度θ</p></li><li class="lvl-3"><p>旋转：</p><ol><li class="lvl-6">构造一个绕y轴（<strong>Vector3f</strong>::UP）旋转、$\theta$的rot</li><li class="lvl-6">获得曲线在该点的法向</li><li class="lvl-6">点的位置向量和法向量同rot进行乘法运算，得到新的点的位置向量和法向量</li><li class="lvl-6">循环上述过程一周</li></ol></li><li class="lvl-3"><p>将修改后控制点坐标pnew和法向量nnew存入到新构造的曲面数组(<a href="http://surface.xn--vvsurface-rw9o.vn/">surface.VV和surface.VN</a>)中，用于面片的绘制</p></li><li class="lvl-3"><p>生成的顶点和法向量，通过三角化将曲面中的所有三角面片存入一个面片数组(surface.VF)中</p></li><li class="lvl-3"><p>将面片数组输入opengl，就可以画出整个曲面</p></li></ol><h2 id="代码参考">代码参考</h2><p>本次实验代码几乎完全仿照了提供的python代码实现。</p><p>具体来说，两种曲线类has a <strong>Bernstein</strong> B；通过构造一个Bernstein类实例来进行计算。</p><p>除此之外没有和其他同学进行讨论/参考其他代码。</p>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> graphics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> graphics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>network-homework-9</title>
      <link href="/2023/04/29/homework/network/network9/"/>
      <url>/2023/04/29/homework/network/network9/</url>
      
        <content type="html"><![CDATA[<h1>网原第九次作业</h1><p>郭高旭 2021010803 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a></p><p>区域边界路由器ABR是跨越区域的，而自治系统边界路由器ASBR则不跨越AS</p><h2 id="第一题">第一题</h2><p>一个路由器刚接收到如下新的 IP 地址： 57.6.96.0/21, 57.6.104.0/21, 57.6.112.0/21 和<br>57.6.120.0/21。如果所有这些地址都使用同一条出境线路，试问它们可以被聚合吗？如果可以，它们被聚合到哪个地址上？如果不可以，请问为什么？</p><table><thead><tr><th>00111001</th><th>00000110</th><th>01100000</th><th>00000000</th></tr></thead><tbody><tr><td>00111001</td><td>00000110</td><td>01101000</td><td>00000000</td></tr><tr><td>00111001</td><td>00000110</td><td>01110000</td><td>00000000</td></tr><tr><td>00111001</td><td>00000110</td><td>01111000</td><td>00000000</td></tr></tbody></table><p>这几个地址的最长公共前缀为19，又公用一条出境线路，因此可以被聚合成57.6.96.0/19</p><h2 id="第二题">第二题</h2><ul class="lvl-0"><li class="lvl-2"><p>更新后的路由表</p><table><thead><tr><th>目的网络</th><th>距离</th><th>下一跳</th></tr></thead><tbody><tr><td>N1</td><td>7</td><td>A</td></tr><tr><td>N3</td><td>3</td><td>C</td></tr><tr><td>N4</td><td>9</td><td>C</td></tr><tr><td>N6</td><td>8</td><td>F</td></tr><tr><td>N7</td><td>5</td><td>C</td></tr><tr><td>N8</td><td>3</td><td>C</td></tr><tr><td>N9</td><td>4</td><td>D</td></tr></tbody></table><ol><li class="lvl-5"><p>路由器 B 接收到路由器 C 发送的距离向量报文，该报文包含了目的网络 N2、N3、N4、N7 和 N8 的距离信息，以及这些网络的下一跳路由器 C 的信息。</p></li><li class="lvl-5"><p>路由器 B 在更新它的路由表前，需要计算距离向量。</p></li><li class="lvl-5"><p>$$<br>\pi(i){\leftarrow} min[\pi(i),min(\pi(i)+w_{ji})]<br>$$</p></li></ol></li><li class="lvl-2"><p>路由器 B 收到从路由器 C 发往网络 N2 的 IP 分组时，会发现距离 N2 的距离为 16，显然这已经超出了最大跳数（默认最大跳数为 15），因此路由器 B 会丢弃该分组。路由器 B 会将这条信息告知其它相邻路由器，以便它们也能更新自己的路由表。</p></li></ul><h2 id="第三题">第三题</h2><table><thead><tr><th>目的网络</th><th>下一跳</th><th>接口</th></tr></thead><tbody><tr><td>AS1</td><td>153.14.3.2</td><td>S0</td></tr><tr><td>194.17.5.128/25</td><td></td><td>E0</td></tr><tr><td>194.17.20.0/23</td><td>194.17.24.2</td><td>S1</td></tr></tbody></table><ul class="lvl-0"><li class="lvl-2"><p>根据最长前缀匹配原则，通过E0接口</p></li><li class="lvl-2"><p>BGP（eBGP），TCP协议，</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>有趣的事</title>
      <link href="/2023/04/28/articles/funny-things/"/>
      <url>/2023/04/28/articles/funny-things/</url>
      
        <content type="html"><![CDATA[<h1>文采不错</h1><p>二零二一年的夏天，<s>xxx</s>十七岁，第一次鼓起勇气，和喜欢的女孩子告白。那是生命中美丽的一天，阳光还算得上明媚，楼宇间轻风阵阵，远处悠扬的蝉鸣声，抚慰少年胸怀。两人肩并肩坐在公园的长椅上，观察一张张年轻和老去的面孔。东风一劲，女孩身体靠近，发丝飞舞。几个月后，<s>xxx</s>发现对方不喜欢吃新鲜的胡萝卜，感到兴味索然，和她提出分手。霓虹灯下的哨兵 2023-04-28</p>]]></content>
      
      
      <categories>
          
          <category> articles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nsfw </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy_1</title>
      <link href="/2023/04/28/Somniloquy/dream-1/"/>
      <url>/2023/04/28/Somniloquy/dream-1/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h1>周五</h1><img src="/img/dream-talk-1.assets/rainInBeijing.jpg" style="zoom: 25%;" /><p>北京下了大暴雨。我没有带伞，淋成了落汤鸡。</p><p>最近有些无聊了，淋雨、洗澡、敲代码时明明跟自己说了很多话，结果只剩上面两句了。</p><img src="/img/dream-talk-1.assets/breakfast.jpg" style="zoom: 25%;" /><p>对了，连着两天吃了早饭，挺好的；但是连着两天四小时充足睡眠，不太妙。<em>出现幻觉头晕目眩1:11</em> 😵</p><p>同学生日，大家聚餐了。我没去。</p><p>😄</p><h2 id="light-up-my-day">light up my day</h2><img src="\img\dream-talk-1.assets\Snipaste_2023-04-28_23-46-31.png" style="zoom: 85%;" /><h2 id="life-goes-on">life goes on</h2><blockquote><p>玩物丧志</p></blockquote><ul class="lvl-0"><li class="lvl-2"><p>这一切的意义是？</p></li><li class="lvl-2"><p>可听到雷声阵阵，可感到危险来临</p></li><li class="lvl-2"><p>真是下贱</p></li><li class="lvl-2"><p>微笑着就倒在血泊中,还在担心别人比我疼</p></li><li class="lvl-2"><p>其实我是个<a href="https://tool.liumingye.cn/music/#/search/M/song/%E8%A0%A2%E8%B4%A7">蠢货</a></p></li><li class="lvl-2"><p>time is up,deal is over</p></li><li class="lvl-2"><p>我应该更关注爱我的人才对</p></li><li class="lvl-2"><p>我被这么多人爱着,我真是天底下最幸福的小孩</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy_README</title>
      <link href="/2023/04/28/Somniloquy/dream-0/"/>
      <url>/2023/04/28/Somniloquy/dream-0/</url>
      
        <content type="html"><![CDATA[<h1>README-README</h1><p>正经人谁写日记呢？</p><blockquote><p>Create this blog just in case I want to write anything (well that may never happen 😃)</p></blockquote><p>我想每周更新一个文件是一个合理的频率。</p><p>大概想到什么就写点什么。</p><p>我是不是可以给自己评论一下，这样你不登录github也能给我点赞？</p><p>我不知道。我不知道。</p><p>意义是？……</p><p>估计也不会有什么排版。</p><p>也许以后我会把梦话和梦里的内容分开。</p><p>这个category完全是梦中的情节，都是假的，不是真的。</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello 沸羊羊</title>
      <link href="/2023/04/27/articles/feiyangyang/"/>
      <url>/2023/04/27/articles/feiyangyang/</url>
      
        <content type="html"><![CDATA[<p>我是沸羊羊，一只聪明又机智的羊。在这草原上，我被称为小霸王。可是，我有一颗温柔的心，曾经深深地爱上了美羊羊。</p><p>可惜，美羊羊看不到我这颗真挚的心。她总是被那些华丽而无脑的花瓶吸引，哪怕他们只会说些毫无意义的话语。我才不会像他们那样肤浅呢。</p><p>我曾经为了她的芳心不惜一切，可是她却视而不见。我想起这段经历，不禁感叹美羊羊真是没有眼光。毕竟，在这么多羊中，我沸羊羊才是最出色的啊！</p><p>不过，没关系，因为我也许早就准备好了面对这个结果。沸羊羊也许只有美羊羊才能陪伴我，可有些羊比美羊羊更加值得我去爱。我会欣然接受这个事实，因为我自信有一个真正的爱情会出现在我的身边。</p>]]></content>
      
      
      <categories>
          
          <category> articles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 沸羊羊 </tag>
            
            <tag> 美羊羊 </tag>
            
            <tag> gpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello YUX</title>
      <link href="/2023/04/27/articles/hello-YUX/"/>
      <url>/2023/04/27/articles/hello-YUX/</url>
      
        <content type="html"><![CDATA[<h1>我在华清大学的“快乐”</h1><p>如果你问我在华清大学是否很快乐，那么我要告诉你，非常快乐。当然，这种快乐并不是所有人都能理解的。</p><p>在华清大学，我曾经快乐地熬了一个通宵写论文，因为我很享受那种极度的疲惫和奋斗的感觉。我还快乐地经历了一场期末考试，因为我发现自己完全不懂老师讲的内容，这让我有机会尝试新的解题方法。</p><p>当然，我在华清大学也曾经快乐地经历了各种各样的挫折，比如我一次考试只得了50分，就当做了一次锻炼自己的心态。还有一次莫名其妙被拒绝的实习机会，就让我意识到了自己要更加努力才能达到期望。</p><p>总之，在华清大学的这几年，我真的很快乐。虽然每天都要面对各种形形色色的压力和不确定性，但这些经历让我成长了许多。所以，我要感谢华清大学这个“快乐”的地方，让我有机会成为更好的自己。</p><!-- About Me --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>About Me</h2>        </div>    </div>    <div class="row">        <div class="col-md-4">            <img src="/img/profile.png" alt="Profile Image">        </div>        <div class="col-md-8">            <p>Hi, I am YUX and I am passionate about NOTHING. </p>        </div>    </div></div>]]></content>
      
      
      <categories>
          
          <category> articles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> THU-daily </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>crawler-sample</title>
      <link href="/2023/04/27/others/code-test/"/>
      <url>/2023/04/27/others/code-test/</url>
      
        <content type="html"><![CDATA[<h1>code-test</h1><blockquote><p>这是一个用来测试代码显示效果的页面。</p><p>你也可以用它爬点东西</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.service <span class="keyword">import</span> Service <span class="keyword">as</span> ChromeService</span><br><span class="line"><span class="keyword">from</span> webdriver_manager.chrome <span class="keyword">import</span> ChromeDriverManager</span><br><span class="line"><span class="comment"># from fake_useragent import UserAgent</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DynCrawler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,url,theme</span>):</span><br><span class="line">        self.url = url</span><br><span class="line">        self.theme = theme</span><br><span class="line">        self.options=webdriver.ChromeOptions() <span class="comment"># 无头模式</span></span><br><span class="line">        self.options.add_argument(<span class="string">&#x27;--headless&#x27;</span>)</span><br><span class="line">        self.driver = webdriver.Chrome(service=ChromeService(executable_path=ChromeDriverManager().install()),options=self.options)</span><br><span class="line">        self.href_list=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_whole_page</span>(<span class="params">self</span>):</span><br><span class="line">        html_page=self.driver.find_elements(By.XPATH,<span class="string">&#x27;//*[@class=&quot;list-t&quot;]/a&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> html_page:</span><br><span class="line">            url =page.get_attribute(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">            title=page.text</span><br><span class="line">            data = &#123;<span class="string">&quot;url&quot;</span>:<span class="built_in">str</span>(url),<span class="string">&quot;title&quot;</span>:<span class="built_in">str</span>(title)&#125;</span><br><span class="line">            self.href_list.append(data)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;get page: &quot;</span>,url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_images</span>(<span class="params">self,url,title</span>):</span><br><span class="line">        self.driver.get(url)</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        images=self.driver.find_elements(By.XPATH,<span class="string">&#x27;//div[@class=&quot;content&quot;]/a/img&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> index,image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">            image_url=image.get_attribute(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(image_url)</span><br><span class="line">            <span class="keyword">if</span> image_url:</span><br><span class="line">                image_name=index+<span class="number">1</span></span><br><span class="line">                image_name=<span class="built_in">str</span>(image_name)+<span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">                image_path=<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>/<span class="subst">&#123;image_name&#125;</span>&#x27;</span></span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(image_path,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        f.write(requests.get(image_url).content)</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_title</span>(<span class="params">self,url</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;get title: &quot;</span>,url)</span><br><span class="line">        self.driver.get(url)</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        title=self.driver.find_element(By.XPATH,<span class="string">&#x27;//*[@id=&quot;app&quot;]/div/div[3]/div[1]/div[1]/h1&#x27;</span>)</span><br><span class="line">        title=title.text</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;title: &quot;</span>,title)</span><br><span class="line">        <span class="keyword">return</span> title</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.theme):</span><br><span class="line">            os.mkdir(self.theme)</span><br><span class="line">        self.driver.get(self.url)</span><br><span class="line">        self.driver.implicitly_wait(<span class="number">2</span>)</span><br><span class="line">        self.get_whole_page()</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> self.href_list:</span><br><span class="line">            url=data[<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">            title=data[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">            title=title.replace(<span class="string">&#x27;/&#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            title=title.replace(<span class="string">&#x27;|&#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            title=title.replace(<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            title=title.replace(<span class="string">&#x27;：&#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            <span class="comment"># make dir</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>&#x27;</span>):</span><br><span class="line">                os.mkdir(<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>&#x27;</span>)</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>/url.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(url)</span><br><span class="line">                self.get_images(url,title)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;请输入歌手名字：&#x27;</span>)</span><br><span class="line">    singername=<span class="built_in">input</span>()</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">8</span>):</span><br><span class="line">        url=<span class="string">f&#x27;http://www.echangwang.com/singer/<span class="subst">&#123;singername&#125;</span>_<span class="subst">&#123;index&#125;</span>.html&#x27;</span></span><br><span class="line">        crawler=DynCrawler(url,singername)</span><br><span class="line">        crawler.run()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fool</title>
      <link href="/2023/04/27/others/fool/"/>
      <url>/2023/04/27/others/fool/</url>
      
        <content type="html"><![CDATA[<div class="danger"><p>时间紧，任务重，I’m f**ked up</p></div><div class="warning"><p>时间不多了,under pressure</p></div><div class="tips"><p>时间还早,totally under control</p></div><div class="success"><p>写完了😄</p></div><ul class="lvl-0"><li class="lvl-2"><p><input type="checkbox" id="checkbox17"><label for="checkbox17">uog</label><br><input type="checkbox" id="checkbox16"><label for="checkbox16">sdf</label></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test-markdown</title>
      <link href="/2023/04/27/others/ful-test/"/>
      <url>/2023/04/27/others/ful-test/</url>
      
        <content type="html"><![CDATA[<h1>标题示例</h1><h2 id="二级标题">二级标题</h2><h3 id="三级标题">三级标题</h3><h4 id="四级标题">四级标题</h4><h5 id="五级标题">五级标题</h5><h6 id="六级标题">六级标题</h6><h2 id="段落和换行">段落和换行</h2><p>这是一个段落，可以写一些文字。这个段落有两行。</p><p>这是第二个段落。使用两个以上的空格加上一个回车可以进行换行。</p><p>这是一个段落，可以写一些文字。这个段落有两行。</p><p>这是第二个段落。使用两个以上的空格加上一个回车可以进行换行。</p><p>使用 <code>**</code> 包裹的文字会变成 <strong>加粗</strong>。使用 <code>_</code> 包裹的文字会变成 <em>斜体</em>。可以在一段文字中混合使用。</p><p>使用 <code>~</code> 包裹的文字会变成 <s>删除线</s>。</p><p>使用 <code>==</code> 包裹的文字会变成高亮，如： <mark>高亮</mark>。</p><h2 id="列表示例">列表示例</h2><h3 id="无序列表">无序列表</h3><ul class="lvl-0"><li class="lvl-2"><p>项目一</p></li><li class="lvl-2"><p>项目二</p></li><li class="lvl-2"><p>项目三</p></li></ul><h3 id="有序列表">有序列表</h3><ol><li class="lvl-3"><p>第一项</p></li><li class="lvl-3"><p>第二项</p></li><li class="lvl-3"><p>第三项</p></li></ol><h3 id="可以使用嵌套列表：">可以使用嵌套列表：</h3><ol><li class="lvl-3"><p>第一项</p><ul class="lvl-2"><li class="lvl-5">嵌套项目</li><li class="lvl-5">嵌套项目</li></ul></li><li class="lvl-3"><p>第二项</p><ol><li class="lvl-6">嵌套有序项目</li><li class="lvl-6">嵌套有序项目</li></ol></li><li class="lvl-3"><p>第三项</p></li></ol><h2 id="引用示例">引用示例</h2><blockquote><p>这是一个引用。引用可以有多行。</p><blockquote><p>引用可以嵌套。</p></blockquote></blockquote><h2 id="粗体和斜体示例">粗体和斜体示例</h2><p>这里是 <strong>粗体</strong> 和 <em>斜体</em> 的示例。同样可以使用 <strong>粗体</strong> 和 <em>斜体</em>。</p><h2 id="链接和图片示例">链接和图片示例</h2><h3 id="链接">链接</h3><p>这是一个<a href="https://www.example.com/">链接</a>。</p><p>可以给链接加上标题：</p><p>这是一个<a href="https://www.example.com/" title="示例链接">链接</a>。</p><h3 id="图片">图片</h3><p>这是一个图片：</p><p><img src="/img/test.png" alt="图片描述"></p><p>也可以给图片加上标题：</p><p><img src="/img/test_profile.png" alt="图片描述" title="示例图片"></p><h2 id="代码块和公式块示例">代码块和公式块示例</h2><p>这是一个代码块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hello</span>():</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Hello World!&quot;</span>)</span><br></pre></td></tr></table></figure><p>也可以使用单行代码块： <code>print(&quot;Hello World!&quot;)</code></p><p>这是一个公式块：<br>$$<br>f(x) = \frac{1}{\sqrt{2\pi}\sigma}e<sup>{-\frac{(x-\mu)</sup>2}{2\sigma^2}}<br>$$</p><h2 id="分隔线示例">分隔线示例</h2><p>这是一条分隔线：</p><hr><h2 id="表格示例">表格示例</h2><table><thead><tr><th>名称</th><th>价格</th></tr></thead><tbody><tr><td>商品 A</td><td>$20</td></tr><tr><td>商品 B</td><td>$30</td></tr><tr><td>商品 C</td><td>$40</td></tr></tbody></table><h2 id="文内链接">文内链接</h2><p>这是一个 <a href="#%E6%A0%87%E9%A2%98%E7%A4%BA%E4%BE%8B">内部链接</a> 到标题示例。</p><h2 id="目录示例">目录示例</h2><p>可以使用 <code>[toc]</code> 来生成目录，需要使用一些 Markdown 工具来渲染生成的目录。</p><p>[toc]</p><h2 id="表情符号示例">表情符号示例</h2><p>可以使用表情符号来增加一些个性化的元素：</p><p>👍 ✨ 🐫 🎉 🚀 🤘 :octocat:</p><h2 id="HTML-语法示例">HTML 语法示例</h2><p>Markdown 还支持 HTML 语法，如下面的加粗文字：</p><p><b>这是一段加粗的文字</b></p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2023/04/27/others/hello-world-1/"/>
      <url>/2023/04/27/others/hello-world-1/</url>
      
        <content type="html"><![CDATA[<!-- Banner Image --><div class="banner">    <img src="/img/banner.png" alt="Banner Image">    <div class="banner-text">        <h1>Hello World</h1>        <p>A warm welcome to my blog!</p>    </div></div><!-- Introduction --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>Introduction</h2>            <p>Welcome to my blog! My name is YUX and I am passionate about NOTHING. This blog is my platform to share my thoughts and experiences with you.</p>        </div>    </div></div><!-- About Me --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>About Me</h2>        </div>    </div>    <div class="row">        <div class="col-md-4">            <img src="/img/profile.png" alt="Profile Image">        </div>        <div class="col-md-8">            <p>Hi, I am YUX and I am passionate about NOTHING. </p>        </div>    </div></div>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>tips</title>
      <link href="/2023/04/27/others/tips/"/>
      <url>/2023/04/27/others/tips/</url>
      
        <content type="html"><![CDATA[<h2 id="终端设置代理">终端设置代理</h2><p>要永久设置HTTP代理和HTTPS代理，可以将这些命令添加到PowerShell的配置文件中。</p><p>以下是在PowerShell中设置永久HTTP代理和HTTPS代理的步骤：</p><ol><li class="lvl-3"><p>打开PowerShell命令提示符，输入以下命令：<code>notepad $PROFILE</code></p></li><li class="lvl-3"><p>在打开的文件中，添加以下两行命令：</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$Env:http_proxy=&quot;http://127.0.0.1:&lt;你的代理端口&gt;&quot;</span><br><span class="line">$Env:https_proxy=&quot;http://127.0.0.1:&lt;你的代理端口&gt;&quot;</span><br></pre></td></tr></table></figure><ol start="3"><li class="lvl-3"><p>保存并关闭文件。</p></li></ol><p>现在，每次打开新的PowerShell窗口时，都会自动加载此配置文件，并在环境变量中设置HTTP代理和HTTPS代理。</p><p>请注意，如果您使用的是PowerShell Core，配置文件的位置可能不同，并且可能需要使用其他命令来编辑它。</p>]]></content>
      
      
      <categories>
          
          <category> tips </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2023/04/27/notes/greece/%CE%B5%CE%BB%CE%BB%CE%B7%CE%BD%CE%B9%CE%BA%CE%AC/"/>
      <url>/2023/04/27/notes/greece/%CE%B5%CE%BB%CE%BB%CE%B7%CE%BD%CE%B9%CE%BA%CE%AC/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>το ποτήρι 🥃</th><th>水杯</th><th>το ψωμί 🥖</th><th>面包</th></tr></thead><tbody><tr><td><s>ο δίσκος 🛸</s></td><td>碟片</td><td>το τραπέζι</td><td>桌子</td></tr><tr><td><s>το τασάκι</s></td><td>烟灰缸</td><td><s>η ζώνη</s></td><td>皮带</td></tr><tr><td>η σοκολάτα 🍫</td><td>巧克力</td><td>το παγωτό 🍦</td><td>冰淇淋</td></tr></tbody></table><p>注意在回答对方&quot;Πως λέγεται αυτό το πράγμα στα ελληνικά;&quot;时回答不需要加冠词，同时要加书名号或双引号。注意Πώς要加重音符号</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> ελληνικά </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生词表 </tag>
            
            <tag> ελληνικά </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ελληνικά</title>
      <link href="/2023/04/27/notes/greece/%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%B8%8C%E8%85%8A%E8%AF%AD%E7%B3%BB%E5%8A%A8%E8%AF%8D%E5%92%8C%CE%B1,%20%CE%B21,%20%CE%B22%E5%9E%8B%E5%8A%A8%E8%AF%8D%E5%9C%A8%E5%85%AD%E7%A7%8D%E4%BA%BA%E7%A7%B0%E4%B8%8B%E7%9A%84%E5%8F%98%E4%BD%8D%E8%A7%84%E5%88%99%EF%BC%9A/"/>
      <url>/2023/04/27/notes/greece/%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%B8%8C%E8%85%8A%E8%AF%AD%E7%B3%BB%E5%8A%A8%E8%AF%8D%E5%92%8C%CE%B1,%20%CE%B21,%20%CE%B22%E5%9E%8B%E5%8A%A8%E8%AF%8D%E5%9C%A8%E5%85%AD%E7%A7%8D%E4%BA%BA%E7%A7%B0%E4%B8%8B%E7%9A%84%E5%8F%98%E4%BD%8D%E8%A7%84%E5%88%99%EF%BC%9A/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>数字</th><th>希腊语</th><th>数字</th><th>希腊语</th></tr></thead><tbody><tr><td>1</td><td>μία (mía)</td><td>20</td><td>είκοσι (íkosi)</td></tr><tr><td>2</td><td>δύο (dýo)</td><td>30</td><td>τριάντα (triánta)</td></tr><tr><td>3</td><td>τρία (tría)</td><td>40</td><td>σαράντα (saránta)</td></tr><tr><td>4</td><td>τέσσερα (téssera)</td><td>50</td><td>πενήντα (penínta)</td></tr><tr><td>5</td><td>πέντε (pénte)</td><td>60</td><td>εξήντα (exínta)</td></tr><tr><td>6</td><td>έξι (éxi)</td><td>70</td><td>εβδομήντα (evdomínta)</td></tr><tr><td>7</td><td>εφτά (eftá)</td><td>80</td><td>ογδόντα (ogdónta)</td></tr><tr><td>8</td><td>οχτώ (ochtó)</td><td>90</td><td>ενενήντα (enenínta)</td></tr><tr><td>9</td><td>εννιά (ennéa)</td><td>100</td><td>εκατό (ekató)</td></tr><tr><td>10</td><td>δέκα (déka)</td><td>200</td><td>διακόσια (diakósia)</td></tr><tr><td>11</td><td>έντεκα (énteka)</td><td>300</td><td>τριακόσια (triakósia)</td></tr><tr><td>12</td><td>δώδεκα (dódeka)</td><td>400</td><td>τετρακόσια (tetrakósia)</td></tr><tr><td>13</td><td>δεκατρία (dekatría)</td><td>500</td><td>πεντακόσια (pentakósia)</td></tr><tr><td>14</td><td>δεκατέσσερα (dekatéssera)</td><td>600</td><td>εξακόσια (hexakósia)</td></tr><tr><td>15</td><td>δεκαπέντε (dekarpénte)</td><td>700</td><td>εφτακόσια (eftakósia)</td></tr><tr><td>16</td><td>δεκαέξι (dekaéxi)</td><td>800</td><td>οκτακόσια (oktakósia)</td></tr><tr><td>17</td><td>δεκαεφτά (dekaeftá)</td><td>900</td><td>εννιακόσια (enniakósia)</td></tr><tr><td>18</td><td>δεκαοχτώ (dekaochtó)</td><td>1000</td><td>χίλια</td></tr><tr><td>19</td><td>δεκαεννιά (dekaennéa)</td><td>0</td><td>μηδέν</td></tr></tbody></table><h3 id="以下是希腊语系动词和α-β1-β2型动词在六种人称下的变位规则：">以下是希腊语系动词和α, β1, β2型动词在六种人称下的变位规则：</h3><h4 id="系动词-μαι">系动词 (μαι)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>εγώ είμαι</td><td>εσύ είσαι</td><td>αυτός/αυτή/αυτό είναι</td></tr><tr><td>复数</td><td>εμείς είμαστε</td><td>εσείς είστε</td><td>αυτοί/αυτές είναι</td></tr></tbody></table><h4 id="α动词-μαθαίνω，ω结尾，重音不在其上">α动词 (μαθαίνω，ω结尾，重音不在其上)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>μαθαίνω -ω</td><td>μαθαίνεις -εις</td><td>μαθαίνει -ει</td></tr><tr><td>复数</td><td>μαθαίνουμε -ουμε</td><td>μαθαίνετε -ετε</td><td>μαθαίνουν -ουν</td></tr></tbody></table><h4 id="β1动词-γράφω，-άω结尾">β1动词 (γράφω，-άω结尾)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>μιλάω -άω</td><td>μιλάς -άς</td><td>μιλάει -άει</td></tr><tr><td>复数</td><td>μιλάμε -άμε</td><td>μιλάτε -άτε</td><td>μιλάνε -άνε</td></tr></tbody></table><h4 id="β2动词-φέρω">β2动词 (φέρω)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>φέρω (féro)</td><td>φέρεις (féreis)</td><td>φέρει (férei)</td></tr><tr><td>复数</td><td>φέρουμε (féroume)</td><td>φέρετε (férete)</td><td>φέρουν (féroun)</td></tr></tbody></table><h3 id="物主代词">物主代词</h3><table><thead><tr><th style="text-align:center">人称代词</th><th style="text-align:center">单数形式</th><th style="text-align:center">复数形式</th></tr></thead><tbody><tr><td style="text-align:center">第一人称</td><td style="text-align:center">μου (mou)</td><td style="text-align:center">μας (mas)</td></tr><tr><td style="text-align:center">第二人称</td><td style="text-align:center">σου (sou)</td><td style="text-align:center">σας (sas)</td></tr><tr><td style="text-align:center">第三人称</td><td style="text-align:center">του (tou)/της/του</td><td style="text-align:center">τους (tous)</td></tr></tbody></table><h3 id="希腊语问好">希腊语问好</h3><table><thead><tr><th>希腊语</th><th>中文</th><th>发音</th></tr></thead><tbody><tr><td>γεια σου</td><td>你好（单数）</td><td>yea soo</td></tr><tr><td>γεια σας</td><td>您好（复数或正式场合）</td><td>yea sas</td></tr><tr><td>καλημέρα</td><td>早上好 / 上午好</td><td>kah-lee-MER-ah</td></tr><tr><td>καλησπέρα</td><td>下午好 / 晚上好</td><td>ka-lee-SPER-ah</td></tr><tr><td>καληνύχτα</td><td>晚安</td><td>kah-lee-NEECH-tah</td></tr><tr><td>αντίο</td><td>再见</td><td></td></tr></tbody></table><h3 id="国家">国家</h3><table><thead><tr><th>希腊文</th><th>中文</th><th>希腊文</th><th>中文</th></tr></thead><tbody><tr><td>Γερμανία/γερμανικά</td><td>德国</td><td>Καναδά</td><td>加拿大</td></tr><tr><td>Ιαπωνία/γιαπωνέζικα</td><td>日本</td><td>Λίβανο</td><td>黎巴嫩</td></tr><tr><td>Κίνα/κινέζικα</td><td>中国</td><td>Αγγλία/αγγλικά</td><td>英国</td></tr><tr><td>Κορέα</td><td>韩国</td><td>Γαλλία</td><td>法国</td></tr><tr><td>Ιταλία\ιταλικά</td><td>意大利</td><td>Ρωσία</td><td>俄罗斯</td></tr><tr><td>Ισπανία\ισπανικά</td><td>西班牙</td><td>Αμερική</td><td>美国</td></tr><tr><td>Τουρκία</td><td>土耳其</td><td>Αυστραλία</td><td>澳大利亚</td></tr><tr><td>Κροατία</td><td>克罗地亚</td><td>Βέλγιο</td><td>比利时</td></tr><tr><td>Ελβετία</td><td>瑞士</td><td>Μεξικό</td><td>墨西哥</td></tr><tr><td>Κύπρος</td><td>塞浦路斯</td><td>Μαρόκο</td><td>摩洛哥</td></tr><tr><td>Ελλάδα/ελληνικά</td><td>希腊</td><td>Ιράν</td><td>伊朗</td></tr></tbody></table><table><thead><tr><th>亲人</th><th>单数形式</th><th>带有&quot;του&quot;所有格的名词</th><th>带有&quot;της&quot;所有格的名词</th></tr></thead><tbody><tr><td>父亲</td><td>πατέρας (patéras)</td><td>του πατέρα (tou patéra)</td><td>της μητέρας (tis miterás)</td></tr><tr><td>母亲</td><td>μητέρα (mitéra)</td><td>της μητέρας (tis miterás)</td><td>του πατέρα (tou patéra)</td></tr><tr><td>儿子</td><td>γιος (yios)\αγόρι</td><td>του γιου (tou yiou)</td><td>της κόρης (tis kóris)</td></tr><tr><td>女儿</td><td>κόρη (kóri)\Κορίτσι</td><td>της κόρης (tis kóris)</td><td>του γιου (tou yiou)</td></tr><tr><td>兄弟</td><td>αδελφός (adelfós)</td><td>του αδελφού (tou adelfoú)</td><td>της αδελφής (tis adelfís)</td></tr><tr><td>姐妹</td><td>αδελφή (adelfí)</td><td>της αδελφής (tis adelfís)</td><td>του αδελφού (tou adelfoú)</td></tr><tr><td>表哥/表姐</td><td>ξάδερφος/ξαδέρφη (ksáderfos/ksadérfi)</td><td>του ξαδέρφη/ξάδερφου (tou ksadérfou/ksáderfou)</td><td>της ξαδέρφης/ξαδέρφης (tis ksadérfis/ksadérfis)</td></tr><tr><td>丈夫</td><td>άντρας (ántras)</td><td>του άντρα (tou ántra)</td><td>της γυναίκας (tis gynaíkas)</td></tr><tr><td>妻子</td><td>γυναίκα (gynaíka)</td><td>της γυναίκας (tis gynaíkas)</td><td>του άντρα (tou ántra)</td></tr></tbody></table><p>同样的，希腊语中家庭成员名词的词形变化与所有格有关，需要根据具体语境而定。κορίτσια  ，αγόρι</p><h3 id="生词表">生词表</h3><table><thead><tr><th style="text-align:left">Καλήμέρα (σας)</th><th></th><th>Καλησπέρα (σας)</th><th></th></tr></thead><tbody><tr><td style="text-align:left">Γεια σας</td><td></td><td>Καληνύχτα</td><td></td></tr><tr><td style="text-align:left">Χαίρετε</td><td>Hello</td><td></td><td></td></tr><tr><td style="text-align:left">Αντίο (σας)</td><td>goodbye</td><td></td><td></td></tr><tr><td style="text-align:left">τώρα</td><td>现在</td><td></td><td></td></tr><tr><td style="text-align:left">δουλεύω</td><td>工作</td><td></td><td></td></tr><tr><td style="text-align:left">μένω</td><td>住</td><td></td><td></td></tr><tr><td style="text-align:left">παντρεμένη</td><td>已婚女性</td><td></td><td></td></tr><tr><td style="text-align:left">άντρας</td><td>男人</td><td></td><td></td></tr><tr><td style="text-align:left"><strong>Από ποιο μέρος;</strong></td><td>你来自哪里？</td><td></td><td></td></tr><tr><td style="text-align:left">πως λεγεστε</td><td>你叫什么名字？</td><td></td><td></td></tr><tr><td style="text-align:left">τηλέφωνο</td><td>电话</td><td></td><td></td></tr><tr><td style="text-align:left">παρακαλώ</td><td>请</td><td></td><td></td></tr><tr><td style="text-align:left">ακριβώς</td><td>正好</td><td></td><td></td></tr><tr><td style="text-align:left">Εντάξει</td><td>好</td><td></td><td></td></tr><tr><td style="text-align:left">Επίσης λέμε:</td><td>同样也说：</td><td></td><td></td></tr><tr><td style="text-align:left">μήπως</td><td>难道</td><td></td><td></td></tr><tr><td style="text-align:left">κινητό</td><td>手机</td><td></td><td></td></tr><tr><td style="text-align:left">λεπτό</td><td>分钟</td><td></td><td></td></tr><tr><td style="text-align:left">Συγνώμη</td><td>对不起</td><td></td><td></td></tr><tr><td style="text-align:left">καθηγητής/καθηγήτρια</td><td>教授</td><td>γιατρός</td><td>医生</td></tr><tr><td style="text-align:left">πωλητής/πωλήτρια</td><td>销售员</td><td>φωτογράφος</td><td>摄影师</td></tr><tr><td style="text-align:left">δάσκαλος/δασκάλα</td><td>老师</td><td>μηχανικός</td><td>工程师</td></tr><tr><td style="text-align:left">μαθητής/μαθήτρια</td><td>学生</td><td>γραμματέας</td><td>秘书</td></tr><tr><td style="text-align:left">εργάτης/εργάτρια</td><td>工人</td><td>συνταξιούχος</td><td>退休人员</td></tr><tr><td style="text-align:left">άνεργος/άνεργη</td><td>失业人员</td><td>φωτομοντέλο (!)</td><td>时装模特儿</td></tr><tr><td style="text-align:left">φοιτητής/φοιτήτρια</td><td>大学生</td><td></td><td></td></tr><tr><td style="text-align:left">Παντρεμένος ή <strong>ελεύθερος</strong>/χωρισμένη</td><td>已婚/未婚/离异</td><td></td><td></td></tr><tr><td style="text-align:left">Λίγο.</td><td>一点点</td><td></td><td></td></tr><tr><td style="text-align:left">καθόλου.</td><td>一点也不</td><td></td><td></td></tr><tr><td style="text-align:left">δε μιλάω καθόλου αραβικά.</td><td>我一点也不会阿拉伯语。</td><td></td><td></td></tr><tr><td style="text-align:left"><strong>διεύθυνσή</strong> τούς.</td><td>他们的地址</td><td></td><td></td></tr><tr><td style="text-align:left">νοσοκομείο.</td><td>医院</td><td></td><td></td></tr><tr><td style="text-align:left">καταλαωαίνει.</td><td>理解</td><td></td><td></td></tr><tr><td style="text-align:left">αρκετά.</td><td>相当的</td><td></td><td></td></tr><tr><td style="text-align:left">ακόμα.</td><td>还是</td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr></tbody></table><ul class="lvl-0"><li class="lvl-2"><p>yu语法</p><ul class="lvl-2"><li class="lvl-4"><p>σε+την=στην</p></li><li class="lvl-4"><p>p53:路名</p></li><li class="lvl-4"><p>+ = συν / και</p><p>— = πλην</p></li><li class="lvl-4"><p>Το τηλέφωνό μου στο σπίτι είναι 210 9511203 调和音节（如果一个名词的重音位置在倒数第三音节，且其后是表示所属的物主代词）</p><ul class="lvl-4"><li class="lvl-6">(καθηγήτριά σού)</li><li class="lvl-6">διεύθνσή τούς</li></ul></li><li class="lvl-4"><p>-ι结尾变复数：加α</p><table><thead><tr><th>ένα παιδί</th><th>δύο/τρία… παιδιά</th></tr></thead><tbody><tr><td>ένα κορίτσι</td><td>δύο/τρία… κορίτσια</td></tr></tbody></table></li></ul></li><li class="lvl-2"><p>例句</p><ol><li class="lvl-5">Η Κάρεν είναι από την Αγγλία;</li><li class="lvl-5">Μιλάει ελληνικά;</li><li class="lvl-5">Τι δουλειά κάνει;</li><li class="lvl-5">Πού δουλεύει τώρα;</li><li class="lvl-5">Ο Γιώργος είναι καθηγητής;</li><li class="lvl-5">Πού μένειη Κάρεν τώρα;</li><li class="lvl-5">Είναι παντρεμένη;</li><li class="lvl-5">Έχει παιδιά;</li><li class="lvl-5">Πού είναι ο άντρας της;</li><li class="lvl-6">Είναι παντρεμένη με τον Χουάν κι έχουν δύο παιδιά,ένα κορίτσι , την Ντολόρες , κι ένα αγόρι, τον<br>Αντόνιο.</li></ol></li></ul><p>你好，我是peter。我来自中国，北京。现在住在希腊，雅典，xx路xx号。我是一名大学生。我目前单身。但是我有一个姐姐，她的名字是海伦，她已婚，有两个孩子，一个儿子，一个女儿。我姐姐的工作是医生。我会说希腊语，我的中文说得很好。但是我的英语很糟糕，我能理解英语但是我不会说。我和我的妈妈住在一起。我们的家庭电话号码是xxxxx。我也有手机，它的号码是xxxx。</p><p>Γεια σου, είμαι ο Peter. Είμαι από την Κίνα, το Πεκίνο. Τώρα μένω στην Αθήνα, στη διεύθυνση xx, στην οδό xx. Είμαι φοιτητής. Παρόλα αυτά, είμαι ανύπαντρος. Έχω όμως μια αδελφή, την Ελένη. Είναι παντρεμένη και έχει δύο παιδιά, ένα γιο και μια κόρη. Η δουλειά της είναι γιατρός. Μιλάω Ελληνικά, ενώ τα Κινέζικά μου είναι πολύ καλά. Ωστόσο, τα Αγγλικά μου είναι πολύ άσχημα και δεν μπορώ να μιλήσω καθόλου. Ζω με τη μητέρα μου. Ο οικιακός τηλέφωνός μας είναι xxxx και έχω επίσης ένα κινητό τηλέφωνο με τον αριθμό xxxx.</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> ελληνικά </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ελληνικά </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>network-homework-5</title>
      <link href="/2023/04/27/homework/network/network5/"/>
      <url>/2023/04/27/homework/network/network5/</url>
      
        <content type="html"><![CDATA[<h1>网原第五次作业</h1><p>郭高旭 2021010803 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a></p><h3 id="1-地球与一个遥远行星通信">1.地球与一个遥远行星通信</h3><ol><li class="lvl-3"><p>路线单向延时<br>$$<br>T=9\frac{10<sup>{10}}{3\times10</sup>8}=300s<br>$$<br>采用停等协议，利用率为<br>$$<br>32 * 1000 * 86/(32 * 1000 * 8b + 2 * 64 * 1000000bps * 300s) = 6.66\times10^{-6}<br>$$</p></li><li class="lvl-3"><p>采用滑动窗口协议：</p><p>n = 2 * 300s/(32 * 1000 * 8b/64 * 1000000bps) + 1 = 150001</p></li></ol><h3 id="2-地球同步卫星的最大信道利用率">2.地球同步卫星的最大信道利用率</h3><ol><li class="lvl-3"><p>停等式</p><p>1000/(1000 + 2 * 1000000 * 270 + 1000) = 1/542</p></li><li class="lvl-3"><p>协议5</p><p>7 * 1000/(1000 + 2 * 1000000 * 270 + 1000) = 7/542</p></li><li class="lvl-3"><p>协议6</p><p>4 * 1000/(1000 + 2 * 1000000 * 270 + 1000) = 7/542</p></li></ol><h3 id="3-卫星信道最大吞吐量">3.卫星信道最大吞吐量</h3><p>512字节占⽤信道时间为T = 512 * 86/(64 * 1000bps) = 0.064s</p><ul class="lvl-0"><li class="lvl-2"><p>窗口大小为1</p><p>512 * 86/(0.064 + 0.270 * 2) = 6781bps</p></li><li class="lvl-2"><p>窗口大小为7</p><p>7 * 512 * 86/(0.064 + 0.270 * 2) = 47467bps</p></li><li class="lvl-2"><p>窗口大小15</p><p>由于信道最多容量为n = [(0.270 * 2 + 0.064)/0.064] =9  故吞吐量为64kbps</p></li><li class="lvl-2"><p>窗口大小127</p><p>同上，吞吐量为64kbs</p></li></ul><h3 id="4-主机之间帧序号的比特数">4.主机之间帧序号的比特数</h3><p>数据帧传输时间范围：$t_{min}$=$128\times{8b/32kbps}=0.032s$ ;$t_{max}$=$512\times{8b/32kbps}=0.128s$</p><p>最小窗口大小范围<br>$$<br>l_{min}=2\times0.3s/t_{max}=6;l_{max}=2\times0.3s/t_{min}=20<br>$$<br>故帧序号最少需要5个比特</p>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>README</title>
      <link href="/2020/04/27/README/"/>
      <url>/2020/04/27/README/</url>
      
        <content type="html"><![CDATA[<h1>README</h1><h3 id="我为什么在这里">我为什么在这里</h3><p>也许这里以后会补上我创建这个博客的原因</p><h3 id="你为什么在这里">你为什么在这里</h3><p>等待戈多</p><h3 id="其他">其他</h3><p>我从来都是白底界面亮度拉满。这次为什么是黑色的主题呢？其实它也提供了白色主题。</p><p>文章创建日期是可以随便改的&gt;&gt;&gt;</p><!-- About Me --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>About Me</h2>        </div>    </div>    <div class="row">        <div class="col-md-4">            <img src="/img/profile.png" alt="Profile Image">        </div>        <div class="col-md-8">            <p>Hi, I am YUX and I am passionate about NOTHING. </p>        </div>    </div></div><div class="danger"><p>时间紧，任务重，I’m f**ked up</p></div><div class="warning"><p>时间不多了,under pressure</p></div><div class="tips"><p>时间还早,totally under control</p></div><div class="success"><p>写完了😄</p></div>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> README </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
