<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Dive-into-DL-PyTorch</title>
      <link href="/2023/05/03/notes/pytorch/learn_torch/"/>
      <url>/2023/05/03/notes/pytorch/learn_torch/</url>
      
        <content type="html"><![CDATA[<h2 id="1-准备数据">1.准备数据</h2><p>在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。</p><h4 id="读写数据-save-load">读写数据:(save&amp;load)</h4><ul class="lvl-0"><li class="lvl-2"><p><code>torch.save(x, 'x.pt')</code>:将x存在文件名同为<code>x.pt</code>的文件里。</p></li><li class="lvl-2"><p>将数据从存储的文件读回内存<code>x2 = torch.load('x.pt')</code></p><p>不仅是tensor.dict啥啥的数据都能这么存储.</p><p>无论是文件大小还是读写速度都完爆json</p></li></ul><h4 id="读写模型">读写模型:</h4><ul class="lvl-0"><li class="lvl-2"><p>两个方式:仅保存和加载模型参数(<code>state_dict</code>)(推荐)；保存和加载整个模型。</p><p>差别就是,第一种方法在SL时候只SL参数,下次L时先建一个新model,然后把参数L进去就好了.</p></li></ul><h5 id="代码-slData模式">代码(slData模式):</h5><p>保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是pt或pthCopy to clipboardErrorCopied</span></span><br></pre></td></tr></table></figure><p>加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure><p>一个小问题在于不同设备(CPU与GPU,不同GPU之间.具体遇到问题再说吧)</p><h2 id="2-创建模型">2.创建模型</h2><p>模型是深度学习的核心，它决定了最终学习的效果。在pytorch中，可以通过继承nn.Module类来创建模型，并在其中定义前向计算函数。</p><blockquote><p>我的理解是,模型就是一个千层饼.每一层有个什么功能.下面就是一层饼的例子.</p><p>实际上并不是层数越多越好.</p><ul class="lvl-1"><li class="lvl-2"><p>当神经网络的层数较多时，模型的数值稳定性容易变差:$0.99^ {365}=?;1.01^{365=?}$</p></li><li class="lvl-2"><p>如何解决?批量归一化</p></li></ul><p>通过不停地tensor运算联立方程,是不是可以把千层饼等效为单层?</p></blockquote><h4 id="多层感知机的设计">多层感知机的设计</h4><p>如果千层饼被等效为单层,那么对层的设计就失去了意义.上面的问题在于.</p><blockquote><p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p></blockquote><p>解决方法是我们加入一些非线性变换的层</p><h5 id="例子">例子:</h5><ul class="lvl-0"><li class="lvl-2"><p>ReLU（rectified linear unit）函数<br>$$<br>ReLU(x)=max(x,0).<br>$$</p></li><li class="lvl-2"><p>sigmoid<br>$$<br>sigmoid(x)= \frac 1 {1+exp(−x)}<br>$$<br><img src="/img/learn_torch.assets/3.8_sigmoid.png" alt="img" style="zoom:33%;" /></p><img src="/img/learn_torch.assets/3.8_sigmoid_grad.png" alt="img" style="zoom:33%;" /></li><li class="lvl-2"><p>tanh函数(双曲正切)<br>$$<br>tanh(x)= \frac{1+exp(−2x)}{1−exp(−2x)}<br>$$<br>图像和sigmoid差不多,但是0附近更陡峭一些</p></li></ul><h4 id="一个flattenlayer的例子">一个flattenlayer的例子</h4><p>这里有一个把(batchsize,*,*,…)的多维tensor转换为(batchsize,***)的二维tensor的例子.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FlattenLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="一个模型的例子">一个模型的例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict([</span><br><span class="line">        (<span class="string">&#x27;flatten&#x27;</span>, FlattenLayer()),<span class="comment">#看,就是这样一层层搭起来...</span></span><br><span class="line">        (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">    ])</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数">初始化模型参数:</h3><p>构建好了模型,我们得有初始参数啊.</p><p>**太好了!**PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略,一般不用我们考虑;如果你真的想知道可参考pytorch<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules">源代码)</a></p><h3 id="总结">总结:</h3><ol><li class="lvl-3"><p>继承Module类</p><p><code>Module</code>类是<code>nn</code>模块里提供的一个模型构造类，是所有神经网络模块的基类</p><p>一般来说,我们重载<code>Module</code>类的<code>__init__</code>函数和<code>forward</code>函数。它们分别用于创建模型参数和定义前向计算。无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的<code>backward</code>函数。</p><p>注意:重载<code>__init__</code>函数时应该首先调用父类<code>module</code>的初始化函数e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure></li><li class="lvl-3"><p>module的子类:还有一些<code>Sequential</code>,<code>ModuleDict</code>,<code>ModuleList</code>之类的东西.我感觉没什么用啊.好像只是让我把一堆Module以一种整齐的方式凑在一起.<code>Sequential</code>好像还挺有用的,至少他要保证层之间输入输出维度匹配,可以直接forward.</p></li><li class="lvl-3"><p>共享模型参数: <code>Module</code>类的<code>forward</code>函数里多次调用同一个层,层之间参数共享。此外，如果我们传入<code>Sequential</code>的模块是同一个<code>Module</code>实例的话参数也是共享的</p><blockquote><p>真的有人在乎参数究竟是什么吗?</p></blockquote></li><li class="lvl-3"><p>自定义层:见[flattenlayer](#### 一个flattenlayer的例子)的例子,这是一个不带参数的层,你当然也可以带参数.但意义是?</p></li><li class="lvl-3"><p>我想:module就是层的累加.一个头进一个头出.<code>人体蜈蚣.</code></p></li></ol><h2 id="3-定义损失函数">3.定义损失函数</h2><p>常见的损失函数有交叉熵损失函数、均方误差损失函数等，可以根据不同的任务选择不同的损失函数。</p><blockquote><p>例子:</p><p>PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure></blockquote><h2 id="4-定义优化器和学习率">4.定义优化器和学习率</h2><p>在训练的过程中，需要使用优化器来更新模型参数。常见的优化器有梯度下降法、Adam等。同时，由于学习率对训练效果影响较大，所以需要定义学习率的初始值以及变化规则。</p><blockquote><p>e.g.</p><p>我们使用学习率为0.1的小批量随机梯度下降作为优化算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></blockquote><h2 id="5-训练模型">5.训练模型</h2><p>将数据、模型、损失函数、优化器和学习率等组合在一起，循环执行前向计算、损失计算、反向传播和参数更新等步骤，即可训练模型。在每个epoch结束后，还需要对模型进行评估。</p><h2 id="6-使用模型进行预测">6.使用模型进行预测</h2><p>在训练好模型后，可以使用它对新的数据进行预测，并输出预测结果或概率。</p><h2 id="7-模型选择">7.模型选择</h2><p>我们怎么评价模型的好坏呢?</p><h3 id="模型欠拟合与过拟合">模型欠拟合与过拟合</h3><h4 id="简单地"><strong>简单地:</strong></h4><ul class="lvl-0"><li class="lvl-2"><p><strong>给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。</strong></p></li><li class="lvl-2"><p><strong>过少的训练样本</strong>也会导致过拟合</p></li></ul><h4 id="训练误差（training-error）和泛化误差（generalization-error）">训练误差（training error）和泛化误差（generalization error）</h4><p>当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确。这是为什么呢？</p><p>以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。</p><p>训练的时候是根据往年题的表现来的(通过减小训练误差).所以往年题做的好不一定代表着高考考的好.(一般来说训练误差的期望$\leq$泛化误差)</p><p>但我们关注的是<strong>泛化误差</strong></p><h4 id="训练集-验证集-测试集">训练集\验证集\测试集</h4><p>高考只能考一次,我们没有办法从训练集(看着答案做题)中知道自己的高考表现.所以我们可以拿一些测试集训练集之外的数据作为<strong>验证集</strong>(模考).</p><p>操作上,我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p><blockquote><p><em>K</em>折交叉验证（K-<em>K</em>-fold cross-validation）</p><p>分出K个子集来.这样可以测K次</p></blockquote><p>但实际上,由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊.</p><h3 id="应对过拟合">应对过拟合:</h3><h4 id="1-权重衰减（weight-decay）">1.权重衰减（weight decay）</h4><blockquote><p>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</p></blockquote><p>权重衰减等价于$L_2$范数正则化（regularization）</p><p><strong>我的理解</strong>:就是在计算损失函数的时候加上一项$λ\times L_2$其中λ是一个超参。$L_2$：参数权重越大，$L_2$越大。这样大概保证了各参数大小都差不多，不偏科。</p><h5 id="实现代码：">实现代码：</h5><p>直接在构造优化器实例时通过<code>weight_decay</code>参数来指定权重衰减超参数。默认下，PyTorch会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line"><span class="comment">#...在optimize时</span></span><br><span class="line"><span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br></pre></td></tr></table></figure><h4 id="2-丢弃法-dropout">2.丢弃法(dropout)</h4><img src="/img/learn_torch.assets/3.13_dropout.svg" alt="img" style="zoom: 80%;" /><p>在隐藏层随机丢弃一个单元.比如上图,隐藏层原来有$h_1,h_2,…,h_5.h_2,h_5 $被丢弃了.</p><p>这样计算时不会过度依赖某一个隐藏单元</p><ul class="lvl-0"><li class="lvl-2"><blockquote><p>(和权重衰减的目的大概是一样的),都是保证不偏科.要不然模考物理总是很难,就你一个考100,人家都是0分.到了高考一赋分大家都考90,你傻眼了.</p></blockquote></li></ul><h5 id="实现">实现:</h5><p>在PyTorch中，我们只需要在全连接层后添加<code>Dropout</code>层并指定丢弃概率。在训练模型时，<code>Dropout</code>层将以指定的丢弃概率随机丢弃上一层的输出元素</p><p>在测试模型时（即<code>model.eval()</code>后），<code>Dropout</code>层并不发挥作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><blockquote><p>丢弃法只在训练模型时使用。</p></blockquote><h2 id="CNN">CNN</h2><p>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。</p><h3 id="0-前置知识">0.前置知识</h3><p>在net里加上一个卷积层就形成了<strong>CNN</strong></p><p>一个小翻译:<code>Kernel</code>在pytorch的卷积计算时就叫做<code>filter</code>.</p><p>CNN的学习就是学习这个<code>kernel</code>,(相应的,上面在学习一个参数tensor)</p><p>那个经典的图像边缘检测实际上在互相关.kernel走一步,算一圈,下个蛋…</p><p>卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>.有什么区别?**没有区别!**反正都是学出来的,学出来的核再上下左右转换一下那么两种运算就交换了.所以根本不需要知道什么是卷积就能CNN.</p><h4 id="优势">优势?</h4><p>考虑图像分类问题。每张图像高和宽均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p><ol><li class="lvl-3"><p>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</p></li><li class="lvl-3"><p>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为1000像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状是3,000,000×2563,000,000×256：它占用了大约3 GB的内存或显存。这带来过复杂的模型和过高的存储开销。</p></li></ol><p>一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p><h3 id="1-填充和步幅">1.填充和步幅:</h3><p>要想使用一个卷积层,操作上显见的问题就是我们要知道这个卷积层输入输出的形状.</p><p>一般来说,对于第i维来说,如果输入在第i维长度?<code>有没有更好的名字</code>是$n_i$,核长度是$k_i$那么输出在第i维长度就是($n_i-k_i+1$),显然,$k_i$不应该超过$n_i$,否则会报错</p><h5 id="填充padding">填充padding</h5><p>**太长不看:**取padding=({$\frac {kernelsize_i-1} 2$,…})可以保证输入输出形状相同</p><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。</p><p>如果在第i维的两侧<strong>一共填充$p_i$行</strong></p><p>那么输出形状第i维将会是($n_i-k_i+p_i+1$)</p><p>所以通常可以把$p_i设置为k_i-1$来使输入输出具有相同形状</p><p>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p><mark>注意</mark>:操作上<code>padding=(2, 1)</code>padding的参数代表着在kernel两侧分别填充多少.也就是说$=p_i/2$,所以尽量也把核的每一维长度取奇数,这样保证了$p_i/2$是个整数.否则还得考虑两边分别取floor和ceiling,<strong>麻烦死了</strong></p><h5 id="步幅stride">步幅stride</h5><p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p>步幅可以按比例缩小形状</p><p>如果按上一步设置理想的padding，同时如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将缩为原来的1/stride倍.其他情况自己算去,但何必为难自己.</p><h3 id="2-多输入通道与多输出通道">2.多输入通道与多输出通道.</h3><h4 id="多输入通道">多输入通道</h4><p>实际上不就是多了一维吗?比如彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。就是新增了一个长度为3(r,g,b)的一维.然后我们用三个卷积核叠一块,发现在色彩维度上insize和kernersize都是3,然后这一维长度就变成1,退化了.</p><p>实现含多个输入通道的互相关运算。我们只需要对每个通道做互相关运算，然后通过<code>add_n</code>函数来进行累加。</p><h4 id="多输出通道">多输出通道</h4><p>哎呀我一阵头晕目眩,不想知道它是怎么算的了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积层----------------------------------------看这里</span></span><br><span class="line">conv_layer = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">output_tensor = conv_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量形状</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)  <span class="comment"># torch.Size([16, 10, 32, 32])</span></span><br></pre></td></tr></table></figure><p>直接来看示例代码吧.知道它怎么算干嘛呢?</p><p>在上面的代码中，<code>input_tensor</code> 是一个批次大小为 16、通道数为 3、高度为 32、宽度为 32 的输入张量，<code>conv_layer</code> 是一个输出通道数为 10、卷积核大小为 3x3、步幅为 1、填充为 1 的卷积层，<code>output_tensor</code> 是卷积层的输出张量，它的形状为 <code>[16, 10, 32, 32]</code>。</p><p>这是gpt说的啊,我觉着挺对的.可以自己跑着试试.</p><h4 id="1-1卷积层">1*1卷积层</h4><p>一个长宽都是1的核</p><p>1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。</p><p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么1×1卷积层的作用与全连接层等价</strong>。</p><p>通过这个来调整参数的通道数,控制模型复杂度.e.g.语数外+理综to语数外+三门选科</p><h3 id="2-池化">2.池化</h3><p><strong>缓解卷积层对位置的过度敏感性</strong>。(想想抗锯齿操作,都是通过采样来使得过渡更加平滑??)</p><p>池化窗口形状为p<em>×</em>q<em>的池化层称为p</em>×<em>q</em>池化层，其中的池化运算叫作p<em>×</em>q池化。</p><ul class="lvl-0"><li class="lvl-2"><p>池化层的输出通道数跟输入通道数相同。</p></li><li class="lvl-2"><p>默认情况下，<code>MaxPool2d</code>实例里步幅和池化窗口形状相同。当然可以自己设定.但是池化后的形状又要算一算了吗?</p></li></ul><h3 id="一些例子">一些例子</h3><p><strong>下面的例子,即使是LeNet,都完全够应付作业了</strong>完全没有必要看</p><h4 id="LeNet">LeNet</h4><p><img src="/img/learn_torch.assets/5.5_lenet.png" alt="img"></p><p><strong>卷积层块里的基本单位是卷积层后接最大池化层</strong>：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。</p><h5 id="实现-2">实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>先conv再flatten,大概都是这么个套路</p><h4 id="AlexNet">AlexNet</h4><p>很多分类工作流程是:获取数据集–&gt;获得特征–&gt;进行分类.</p><p>之前认为DL的工作只是使用机器学习模型对特征分类。使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p><p>特征可以学习吗?&gt;&gt; AlexNet</p><p><img src="/img/learn_torch.assets/5.6_alexnet.png" alt="img"></p><h5 id="稍微简化的AlexNet">稍微简化的AlexNet</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>真的,体验上来说,LeNet做作业就完全够了…</p></blockquote><ul class="lvl-0"><li class="lvl-2"><p>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</p></li><li class="lvl-2"><p>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</p></li></ul><h3 id="VGG块"><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.7_vgg?id=_571-vgg%E5%9D%97">VGG块</a></h3><p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×33×3的卷积层后接上一个步幅为2、窗口形状为2×22×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数。</p><blockquote><p>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p></blockquote><h3 id="NiN块"><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.8_nin?id=_581-nin%E5%9D%97">NiN块</a></h3><p>我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在5.3节（多输入通道和多输出通道）里介绍的1×11×1卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×11×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。</p><p>上面这俩我回头再看,头晕😵‍💫😵‍💫😵‍💫😵‍💫</p><h3 id="5-9-含并行连结的网络（GoogLeNet）"><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.9_googlenet?id=_59-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88googlenet%EF%BC%89">5.9 含并行连结的网络（GoogLeNet）</a></h3><p>如果说上面两个是网络之间串行,那么GoogLeNet的基本块Inception块是并行的(并行里套了串行)</p><img src="/img/learn_torch.assets/5.9_inception.svg" alt="inception块" /><p>Inception块里有4条并行的线路。前3条线路使用窗口大小分别是1×1、3×3和5×5的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1×1卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用3×3最大池化层，后接1×1卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p><p>这个更复杂了😵‍💫😵‍💫😵‍💫😵‍💫具体实现抄抄链接里的吧.我真的好奇他怎么保证每次输入输出通道匹配的</p><h3 id="批量归一化">批量归一化</h3><ul class="lvl-0"><li class="lvl-2"><p>在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p></li><li class="lvl-2"><p>对全连接层和卷积层做批量归一化的方法稍有不同。</p></li><li class="lvl-2"><p>批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。</p></li><li class="lvl-2"><p>PyTorch提供了BatchNorm类方便使用。</p></li></ul><p>Pytorch中<code>nn</code>模块定义的<code>BatchNorm1d</code>和<code>BatchNorm2d</code>类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的<code>num_features</code>参数值。下面我们用PyTorch实现使用批量归一化的LeNet。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>可以试试把这个加到自己的module里,</p><blockquote><p>它就是个插件,可以加上它,而不用对其他东西做任何修改,我觉着挺好的</p></blockquote><h3 id="残差网络ResNet">残差网络ResNet</h3><p>我饿了🥱🥱🥱不学了.</p><h2 id="RNN">RNN</h2><h3 id="RNN的一个重要应用就是语言模型">RNN的一个重要应用就是语言模型</h3><p>假设一段长度为$T$的文本中的词依次为$w_1, w_2, \ldots, w_T$，那么在离散的时间序列中，$w_t$（$1 \leq t \leq T$）可看作在时间步（time step）$t$的输出或标签。给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型将计算该序列的概率：</p><p>$$<br>P(w_1, w_2, \ldots, w_T).<br>$$<br>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，我们有</p><p>$$<br>P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1}).<br>$$</p><h4 id="n元语法">n元语法</h4><p>当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$）。如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。如果基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p><p>$$<br>P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .<br>$$</p><h3 id="我理解的RNN">我理解的RNN</h3><p>上面的例子:把一个句子看做词序列,每个时间步产生一个词…当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。但是,我们可以通过一个隐藏变量来传递前面几个时间步的信息…该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。</p><h4 id="例子-2">例子</h4><p>引入一个新的权重参数$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p><p>$$<br>\boldsymbol{H}<em>t = \phi(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xh} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}<em>h)<br>$$<br>其中$\boldsymbol{H}</em>{t-1}$就是上一个时间步的隐藏变量</p><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/pytorch/learn_torch.assets/6.2_rnn.svg" alt="img"></p><h3 id="构建数据集">构建数据集</h3><p>对时序数据的采样方式有如下两种(采样就是选取batchsize个样本,如果batchsize=1,那么就不需要考虑如何采样了)</p><ol><li class="lvl-3"><h5 id="随机采样">随机采样</h5><p>在数据集中随机选取batchsize个样本,由于不同样本之间是独立的,所以不能直接把上个样本计算后的隐藏状态传给下个样本作为初始隐藏状态</p></li><li class="lvl-3"><h5 id="相邻采样">相邻采样</h5><p>字面意思,在数据集中选取相邻的batchsize个样本,这样只需在每一个迭代周期开始时初始化隐藏状态.</p><p>但是这样计算梯度时会依赖所有序列??</p><p>为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</p></li></ol><h3 id="RNN的实现">RNN的实现</h3><blockquote><p>我终于明白了,我们好像只需要关注不同layer的输入输出形状…</p><p>至于我的net为什么有他们,who ™ cares??</p></blockquote><p>PyTorch中的<code>nn</code>模块提供了循环神经网络的实现。下面构造一个含单隐藏层、隐藏单元个数为256的循环神经网络层<code>rnn_layer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><h5 id="native-rnn-layer的输入">native rnn_layer的输入</h5><p><code>rnn_layer</code>的输入形状为(时间步数, 批量大小,<mark>input_size</mark>)</p><p>就是有点别扭.</p><ul class="lvl-0"><li class="lvl-2"><p>第一个时间步数就是句子长度</p></li><li class="lvl-2"><p>第二个批量大小,我真无语,为什么要把批量大小放在第二个,感觉不是很直观.所以cnn和rnn的数据不能直接通用,要view一下交换一下前两个维度?还是别的什么维度</p></li><li class="lvl-2"><p>第三个inut_size;这里和cnn不同,如果说cnn的input单元是句子整体.那么RNN的input实际上是词向量.所以说如果是one-hot,那么就是vocab_size;如果已经word2vec后,就是<strong>词向量的长度</strong>…这里注意一下不同教程的区别.</p></li></ul><h5 id="native-rnn-layer的输出">native rnn_layer的输出</h5><p>形状为(时间步数, 批量大小, 隐藏单元个数)。</p><p>前向计算后会分别返回输出和隐藏状态h，其中输出指的是隐藏层在<strong>各个时间步</strong>上计算并输出的隐藏状态，它们通常作为后续输出层的输入。参考下图(这是LSTM,和RNN本身不很一样)</p><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/pytorch/learn_torch.assets/6.5.png" alt="img"></p><h3 id="门控循环单元GRU">门控循环单元GRU</h3><p>当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p><p><strong>GRU</strong>就是为了解决上述问题</p><p>GRU包含重置门和更新门,计算图如下</p><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/pytorch/learn_torch.assets/6.7_gru_3.svg" alt="img"></p><p>具体来说，假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\boldsymbol{X}<em>t \in \mathbb{R}^{n \times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\boldsymbol{H}</em>{t-1} \in \mathbb{R}^{n \times h}$。重置门$\boldsymbol{R}_t \in \mathbb{R}^{n \times h}$和更新门$\boldsymbol{Z}_t \in \mathbb{R}^{n \times h}$的计算如下：</p><p>$$<br>\begin{aligned}<br>\boldsymbol{R}<em>t = \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xr} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}_{hr} + \boldsymbol{b}<em>r),\<br>\boldsymbol{Z}<em>t = \sigma(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xz} + \boldsymbol{H}</em>{t-1} \boldsymbol{W}</em>{hz} + \boldsymbol{b}_z),<br>\end{aligned}<br>$$</p><p>其中$\boldsymbol{W}<em>{xr}, \boldsymbol{W}</em>{xz} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}<em>{hr}, \boldsymbol{W}</em>{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_r, \boldsymbol{b}_z \in \mathbb{R}^{1 \times h}$是偏差参数。3.8节（多层感知机）节中介绍过，sigmoid函数可以将元素的值变换到0和1之间。因此，重置门$\boldsymbol{R}_t$和更新门$\boldsymbol{Z}_t$中每个元素的值域都是$[0, 1]$。</p><h4 id="候选隐藏状态-tilde-boldsymbol-H-t">候选隐藏状态$\tilde{\boldsymbol{H}}_t$</h4><p>重置门负责给出候选隐藏状态<br>$$<br>\tilde{\boldsymbol{H}}<em>t = \text{tanh}(\boldsymbol{X}<em>t \boldsymbol{W}</em>{xh} + \left(\boldsymbol{R}<em>t \odot \boldsymbol{H}</em>{t-1}\right) \boldsymbol{W}</em>{hh} + \boldsymbol{b}<em>h)<br>$$<br>其中$\boldsymbol{W}</em>{xh} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$是偏差参数。$\odot$是元素乘法.从上面这个公式可以看出，重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃与预测无关的历史信息。</p><h4 id="隐藏状态">隐藏状态</h4><p>最后，时间步$t$的隐藏状态$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$的计算使用当前时间步的更新门$\boldsymbol{Z}<em>t$来对上一时间步的隐藏状态$\boldsymbol{H}</em>{t-1}$和当前时间步的候选隐藏状态$\tilde{\boldsymbol{H}}_t$做组合：</p><p>$$<br>\boldsymbol{H}_t = \boldsymbol{Z}<em>t \odot \boldsymbol{H}</em>{t-1}  + (1 - \boldsymbol{Z}_t) \odot \tilde{\boldsymbol{H}}_t<br>$$</p><blockquote><p>简单来说,就是如果前面时间步的输入已经和这次输出没太大关系,重置门会重置隐藏状态</p><p>如果这次输入不太影响后面内容,更新门会把之前的隐藏状态传递下去.</p><ul class="lvl-1"><li class="lvl-2"><p>重置门有助于捕捉时间序列里短期的依赖关系；</p></li><li class="lvl-2"><p>更新门有助于捕捉时间序列里长期的依赖关系。</p></li></ul></blockquote><p><mark>注意</mark>:上面完全不需要理解.反正有native的GRU模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><h3 id="长短期记忆LSTM">长短期记忆LSTM</h3><p>另一种门控循环神经网络是LSTM（long short-term memory,长短期记忆)</p><blockquote><p>你看这名字,这不是和刚才的门干了一样的事吗??</p></blockquote><p>LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞，从而记录额外的信息。</p><p><mark>同样地</mark>,可以直接调用<code>rnn</code>模块中的<code>LSTM</code>类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/pytorch/learn_torch.assets/6.8_lstm_2.svg" alt="img"></p><p>如果你想理解,我把图放这了.</p><p>我的理解是,这其实就是显示地干了GRU的活.</p><blockquote><ul class="lvl-1"><li class="lvl-2"><p>如果遗忘门与记忆细胞做$\odot$趋于1,那么就会倾向于保存下之前的记忆,如果趋于0就会遗忘</p></li><li class="lvl-2"><p>如果输入门与候选记忆细胞做$\odot$趋于1,那么就会倾向于更新记忆,如果趋于0就不会更新</p></li></ul></blockquote><h3 id="更复杂的模型">更复杂的模型</h3><h4 id="深度循环神经网络">深度循环神经网络</h4><p>目前为止介绍的循环神经网络只有一个单向的隐藏层，在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。下图演示了一个有L个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/pytorch/learn_torch.assets/6.9_deep-rnn.svg" alt="img"></p><h4 id="双向循环神经网络">双向循环神经网络</h4><p>之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。</p><p><img src="D:/py_works/new_blog/ggx21.github.com/docs/source/_posts/notes/pytorch/learn_torch.assets/6.10_birnn.svg" alt="img"></p><blockquote><p>上面两个我感觉挺靠谱的,但是没有示例代码,所以我就不管了</p></blockquote><blockquote><p>update:其实双向循环网络就是加个参数的事</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BiRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab, embed_size, num_hiddens, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiRNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="built_in">len</span>(vocab), embed_size)</span><br><span class="line">        <span class="comment"># bidirectional设为True即得到双向循环神经网络</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size, </span><br><span class="line">                                hidden_size=num_hiddens, </span><br><span class="line">                                num_layers=num_layers,</span><br><span class="line">                                bidirectional=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始时间步和最终时间步的隐藏状态作为全连接层输入</span></span><br><span class="line">        self.decoder = nn.Linear(<span class="number">4</span>*num_hiddens, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><blockquote><p>而多层就更不用教了…</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>four-adder</title>
      <link href="/2023/05/02/homework/DLCE/%E5%9B%9B%E4%BD%8D%E5%8A%A0%E6%B3%95%E5%99%A8REPORT/"/>
      <url>/2023/05/02/homework/DLCE/%E5%9B%9B%E4%BD%8D%E5%8A%A0%E6%B3%95%E5%99%A8REPORT/</url>
      
        <content type="html"><![CDATA[<h1>四位加法器REPORT</h1><p>郭高旭 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a> 2021010803</p><h2 id="实验目的">实验目的</h2><ul class="lvl-0"><li class="lvl-2"><p>掌握组合逻辑电路的基本分析方法与设计方法；</p></li><li class="lvl-2"><p>理解半加器、全加器、加法器的分析与设计方法；</p></li><li class="lvl-2"><p>学会元件例化；</p></li><li class="lvl-2"><p>学会利用软件仿真实现对数字电路的验证与分析</p></li></ul><h2 id="实验内容">实验内容</h2><ul class="lvl-0"><li class="lvl-2"><p>设计半加器</p></li><li class="lvl-2"><p>利用半加器构建全加器</p></li><li class="lvl-2"><p>使用全加器构造逐次进位加法器，超前进位加法器</p></li><li class="lvl-2"><p>使用 VHDL自带的加法运算实现一个 4 位全加器</p></li><li class="lvl-2"><p>查看逐次进位加法器、超前进位加法器和 VHDL 自带加法器在CPLD中生成的电路,并比较这三者的异同</p></li></ul><h2 id="不同加法器的RTL与仿真结果">不同加法器的RTL与仿真结果</h2><h4 id="超前进位加法器">超前进位加法器</h4><h5 id="仿真结果">仿真结果</h5><img src="/img/fouradder.assets/1.png" alt="image-20230502202851109" style="zoom:65%;" /><p>超前进位加法器延迟：End-Start=17.33ns</p><h5 id="RTL">RTL</h5><img src="/img/fouradder.assets/2.png" alt="image-20230502202851109" style="zoom:65%;" /><h4 id="简单进位加法器">简单进位加法器</h4><h5 id="RTL-2">RTL</h5><img src="/img/fouradder.assets/3.png" alt="image-20230502202851109" style="zoom:65%;" />##### 仿真结果<img src="/img/fouradder.assets/4.png" alt="image-20230502202851109" style="zoom:65%;" /><p>超前进位加法器延迟：End-Start=17.96ns</p><h4 id="原生加法器">原生加法器</h4><h5 id="RTL-3">RTL</h5><img src="/img/fouradder.assets/5.png" alt="image-20230502202851109" style="zoom:65%;" /><h4 id="代码">代码</h4><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LIBRARY</span> ieee;</span><br><span class="line"><span class="keyword">USE</span> ieee.std_logic_1164.<span class="keyword">ALL</span>;</span><br><span class="line"><span class="keyword">USE</span> ieee.std_logic_arith.<span class="keyword">ALL</span>;</span><br><span class="line"><span class="keyword">USE</span> ieee.std_logic_unsigned.<span class="keyword">ALL</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTITY</span> native_adder <span class="keyword">IS</span></span><br><span class="line"><span class="keyword">PORT</span> (</span><br><span class="line">a, b : <span class="keyword">IN</span> <span class="built_in">STD_LOGIC_VECTOR</span>(<span class="number">3</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line">c0 : <span class="keyword">IN</span> <span class="built_in">STD_LOGIC</span>;</span><br><span class="line">f : <span class="keyword">OUT</span> <span class="built_in">STD_LOGIC_VECTOR</span>(<span class="number">3</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line">c3 : <span class="keyword">OUT</span> <span class="built_in">STD_LOGIC</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">END</span> nativee_adder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARCHITECTURE</span> plus <span class="keyword">OF</span> native_adder <span class="keyword">IS</span></span><br><span class="line"><span class="keyword">signal</span> buf: <span class="built_in">STD_LOGIC_VECTOR</span>(<span class="number">4</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">process</span>(a, b, c0)</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">buf &lt;= <span class="string">&quot;00000&quot;</span>+ a + b + c0;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">process</span>;</span><br><span class="line"><span class="keyword">process</span> (buf)</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">f &lt;= buf(<span class="number">3</span> <span class="keyword">DOWNTO</span> <span class="number">0</span>);</span><br><span class="line">c3 &lt;= buf(<span class="number">4</span>);</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">process</span>;</span><br><span class="line"><span class="keyword">END</span> plus;</span><br></pre></td></tr></table></figure><h5 id="仿真结果-2">仿真结果</h5><img src="/img/fouradder.assets/6.png" alt="image-20230502202851109" style="zoom:65%;" /><p>原生加法器延时：10.03ns</p><h3 id="总结：">总结：</h3><p>我实现的超前进位加法器比普通进位加法器延迟略小。但原生加法器延时远小于我实现的。</p><h2 id="电路功能测试的结果">电路功能测试的结果</h2><ul class="lvl-0"><li class="lvl-2"><p>实际功能测试结果从计算结果上来说和仿真测试结果相同</p></li><li class="lvl-2"><p>实验时的延迟效果几乎都无法感知</p></li></ul><h2 id="调试中遇到的问题">调试中遇到的问题</h2><ul class="lvl-0"><li class="lvl-2"><p>对仿真操作不够熟悉，在学习仿真时花费了很多时间。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> DLCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> DLCE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DLC</title>
      <link href="/2023/05/02/notes/DLC/%E8%A7%A6%E5%8F%91%E5%99%A8/"/>
      <url>/2023/05/02/notes/DLC/%E8%A7%A6%E5%8F%91%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="触发器">触发器</h2><h3 id="基本R-S触发器">基本R-S触发器</h3><h3 id="电位触发方式的触发器">电位触发方式的触发器</h3><h3 id="边沿触发方式的触发器">边沿触发方式的触发器</h3><ul class="lvl-0"><li class="lvl-2"><p>正边沿触发方式？</p><blockquote><p>分析电路？我还不会。什么阻塞不阻塞的</p></blockquote></li><li class="lvl-2"><p>负边沿触发方式？jk触发</p><blockquote><p>可以等同于jk触发方式吗?讨厌jk,完全不会</p></blockquote></li></ul><h3 id="主－从触发方式的触发器">主－从触发方式的触发器</h3><p>由两级电位触发器（主触发器和从触发器）串连而成</p><ul class="lvl-0"><li class="lvl-2"><p>CP=1期间，主触发器接收数据，从触发器封锁</p></li><li class="lvl-2"><p>在负跳变到来时，主触发器封锁，从触发器将接收CP负跳变时主触发器的状态。</p></li></ul><h4 id="主从R-S触发器的功能表">主从R-S触发器的功能表</h4><p>和基本R-S一样，（cp一个正脉冲，一个负脉冲）。大概只是为了保证触发器的稳定。</p><h2 id="同步时序逻辑电路的分析">同步时序逻辑电路的分析</h2><ol><li class="lvl-3"><p>列出状态关系</p><p>大概就是列出${Q_i}_{n+1}与{Q_i}_n$之间的递推关系</p><blockquote><ul class="lvl-3"><li class="lvl-2"><p>对于D触发器来说$Q_i’=D_i$</p></li><li class="lvl-2"><p>对于JK触发器来说$Q_i’=J\bar Q+\bar KQ$</p><blockquote><p>因为我并不会jk触发器,这一条实在是难以理解.死记硬背吧</p></blockquote></li><li class="lvl-2"><p>至于D\J\K等于什么就再说了</p><blockquote><p>你D可以和输入以及$Q_i$有关.J和K有必要吗?有.下标漏掉了应该是$Q_i’=J\bar Q_i+\bar KQ_i$还有$Q_j$呢</p></blockquote></li></ul></blockquote></li><li class="lvl-3"><p>列出状态表</p><p>根据上面内容可以列出状态表</p></li><li class="lvl-3"><p>画出状态图</p><ul class="lvl-2"><li class="lvl-5">如果有输入,那么就是一个状态机(自动机)</li><li class="lvl-5">否则就是一个退化了的状态机(每个结点只有一个出边)</li></ul></li><li class="lvl-3"><p>画出时序图</p><ul class="lvl-2"><li class="lvl-5">每一帧CP都是高低</li><li class="lvl-5">每一帧输入最多变一次</li><li class="lvl-5">根据状态图画时序图</li></ul><blockquote><p>这又不考虑延迟.都是理想状态,大概还挺简单的</p></blockquote></li></ol>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> DLC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DLC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新的一天</title>
      <link href="/2023/05/01/Somniloquy/dream-3/"/>
      <url>/2023/05/01/Somniloquy/dream-3/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="/2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h2 id="10：18">10：18</h2><p>昨天晚上调整了本地文件的组织方式。这导致我页面的permalink基本全改变了。本来还有些评论的，这下全没了😢😢😢</p><h2 id="16：00">16：00</h2><p>太依赖gpt，已经丧失了动脑子的能力了。再也不能从代码里获得什么成就感了。是好是坏呢？总归是把graphics pa2干完了。这不就是把给的python参考翻译一遍。</p><h2 id="19：20">19：20</h2><p>桃李吃晚饭，土豆炖牛肉给我上了个夹生的。看在他给我换了两道菜的情况下就不骂了。<br><img src="\img\dream-3\1.jpg" style="zoom: 15%;" /></p><p>土豆烧牛肉换辣子鸡+西红柿炒鸡蛋，还可以吗？</p><p><strong>妈的，iai就给我92分。凭什么。</strong></p><h2 id="22-32">22:32</h2><p>心情突然很down，游戏也打不动了。意义是什么呢？在紫操溜溜，没有变好。<br>😿😿😿<br><img src="\img\dream-3\2.jpg" style="zoom: 25%;" /></p><h2 id="35-32">35:32</h2><p>爬了会儿格子，感到心情平静了些。竟然有这闲心。🚭🚭🚭</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>留言板</title>
      <link href="/2023/04/30/others/comment/"/>
      <url>/2023/04/30/others/comment/</url>
      
        <content type="html"><![CDATA[<p>这里专门为了评论.</p><p>但是你需要登录一下GitHub.</p><p>有没有更好的方式呢?</p><p>😶‍🌫️😶‍🌫️😶‍🌫️😶‍🌫️😶‍🌫️</p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> comment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy_2</title>
      <link href="/2023/04/30/Somniloquy/dream-2/"/>
      <url>/2023/04/30/Somniloquy/dream-2/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="/2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h2 id="8：00">8：00</h2><p>昨天晚上被g44拉了壮丁，练了一晚上拔河。今早上勇夺亚军。挺好的，有点事干总比天天胡思乱想好.<br><img src="/img/dream-2/1.jpg" style="zoom: 25%;" /></p><h2 id="11-36">11:36</h2><p>天天搞这前端玩物丧志，搞个<a href="/2023/04/30/homework/deadlines/">deadline页面</a>搞到现在。<br><strong>又没人看。</strong><br>一天到晚写些小学生流水帐😄中厅很安静。突然觉着这样写markdown与typora相比也挺舒服的。</p><h2 id="12：00">12：00</h2><p>看着窗外的云发呆。小时候天天看着云从这边飘到那边，现在一天到晚对着电子屏幕，为了什么呢？</p><p>昨天晚上还在和人说，感觉自己还是很有耐心。今天干了些事，发现耐心消耗殆尽了。<br>🤡🤡🤡<br><img src="/img/dream-2/2.jpg" style="zoom: 25%;" /></p><h2 id="15-05">15:05</h2><p>再写前端我就是那个<br>🐒🐒🐒🐒</p><h2 id="19：36">19：36</h2><p>本月只点了3次外卖。感觉生活变健康了。</p><p>但我是统计了红包使用次数得出来的数据，感觉大亏特亏。😿</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deadlines</title>
      <link href="/2023/04/30/homework/deadlines/"/>
      <url>/2023/04/30/homework/deadlines/</url>
      
        <content type="html"><![CDATA[<blockquote><p>日拱一卒未有尽，​      功不唐捐终到海。</p></blockquote><details> <summary><b>图例</summary><div class="danger"><p>时间紧，任务重，I’m f**ked up</p></div><div class="warning"><p>时间不多了,under pressure</p></div><div class="tips"><p>时间还早,totally under control</p></div><div class="success"><p>写完了😄</p></div></details><h2 id="发现红框没有被check，可以来压力我😄">发现红框没有被check，可以来压力我😄</h2><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="danger"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox0" checked="true"><label for="checkbox0">希腊语第三次作业 本周</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox1" checked="true"><label for="checkbox1">计图pa2</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox2"><label for="checkbox2">人智导第二次作业  本周 </label><mark>next</mark></p></li></ul></div></details><hr><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="warning"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox3" checked="true"><label for="checkbox3">网原第九次作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox4" checked="true"><label for="checkbox4">概统第十次作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox5"><label for="checkbox5">毛概案例分析 十二周周日</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox6"><label for="checkbox6">网原大作业</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox7" checked="true"><label for="checkbox7">数电实验报告</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox8" checked="true"><label for="checkbox8">数电作业</label></p></li></ul></div></details><hr><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="tips"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox9"><label for="checkbox9">习概作业 17周周日</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox10"><label for="checkbox10">计图pa3</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox11"><label for="checkbox11">计图大作业</label></p></li></ul></div></details><details open><summary><span class="pre-summary">&nbsp;</span>点击折叠</summary><div class="success"><ul class="lvl-2"><li class="lvl-2"><p><input type="checkbox" id="checkbox12"><label for="checkbox12">2</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox13"><label for="checkbox13">3</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox14"><label for="checkbox14">1</label></p></li><li class="lvl-2"><p><input type="checkbox" id="checkbox15"><label for="checkbox15">2</label></p></li></ul></div></details>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PA2 REPORT</title>
      <link href="/2023/04/29/homework/graphics/PA2REPORT/"/>
      <url>/2023/04/29/homework/graphics/PA2REPORT/</url>
      
        <content type="html"><![CDATA[<h1>图形学PA2 REPORT</h1><p>郭高旭 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a> 2021010803</p><h2 id="曲线性质">曲线性质</h2><h3 id="两种曲线的异同">两种曲线的异同</h3><p>相同点：</p><ol><li class="lvl-3"><p>都是基于控制点构造出的曲线；</p></li><li class="lvl-3"><p>都是多项式参数曲线，不能表示圆等曲线</p></li><li class="lvl-3"><p>都可以表示一条平滑的曲线。</p></li></ol><p>不同点：</p><ol><li class="lvl-3"><p>局部修改：</p><ul class="lvl-2"><li class="lvl-5">B 样条是局部的，容易对曲线进行修改、扩展和截断，而这些修改对曲线的其他部分几乎没有影响。</li><li class="lvl-5">Bezier曲线/曲面不支持局部的修改和编辑。</li></ul></li><li class="lvl-3"><p>拼接：B样条曲线可以拼接，Bezier曲线/曲面拼接时，满足几何连续性条件是十分困难的。</p></li><li class="lvl-3"><p>B样条曲线具有下面一系列好的性质</p><ul class="lvl-2"><li class="lvl-5">仿射不变性</li><li class="lvl-5">直线保持性</li><li class="lvl-5">凸包性</li><li class="lvl-5">变差缩减性</li><li class="lvl-5">几何不变性</li><li class="lvl-5">etc.</li></ul></li></ol><h3 id="怎样绘制一个首尾相接且接点处也有连续性质的-B-样条">怎样绘制一个首尾相接且接点处也有连续性质的 B 样条</h3><ul class="lvl-0"><li class="lvl-2"><p>首尾相接：</p><p>将控制点复制一遍，在第一组控制点前面加入最后一组控制点，最后一组控制点后面加入第一组控制点。</p></li><li class="lvl-2"><p>保持连续性质</p><p>通过设置相邻控制顶点的位置和导数值相等来保证一阶连续性</p></li></ul><h2 id="代码逻辑">代码逻辑</h2><ol><li class="lvl-3"><p>首先通过discretize获取我刚刚实现的curve（一系列点的三维坐标V以及曲线在这一点的切向量T）</p></li><li class="lvl-3"><p>指定旋转步数step，进而获得每次旋转的角度θ</p></li><li class="lvl-3"><p>旋转：</p><ol><li class="lvl-6">构造一个绕y轴（<strong>Vector3f</strong>::UP）旋转、$\theta$的rot</li><li class="lvl-6">获得曲线在该点的法向</li><li class="lvl-6">点的位置向量和法向量同rot进行乘法运算，得到新的点的位置向量和法向量</li><li class="lvl-6">循环上述过程一周</li></ol></li><li class="lvl-3"><p>将修改后控制点坐标pnew和法向量nnew存入到新构造的曲面数组(<a href="http://surface.xn--vvsurface-rw9o.vn/">surface.VV和surface.VN</a>)中，用于面片的绘制</p></li><li class="lvl-3"><p>生成的顶点和法向量，通过三角化将曲面中的所有三角面片存入一个面片数组(surface.VF)中</p></li><li class="lvl-3"><p>将面片数组输入opengl，就可以画出整个曲面</p></li></ol><h2 id="代码参考">代码参考</h2><p>本次实验代码几乎完全仿照了提供的python代码实现。</p><p>具体来说，两种曲线类has a <strong>Bernstein</strong> B；通过构造一个Bernstein类实例来进行计算。</p><p>除此之外没有和其他同学进行讨论/参考其他代码。</p>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> graphics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> graphics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>network-homework-9</title>
      <link href="/2023/04/29/homework/network/network9/"/>
      <url>/2023/04/29/homework/network/network9/</url>
      
        <content type="html"><![CDATA[<h1>网原第九次作业</h1><p>郭高旭 2021010803 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a></p><p>区域边界路由器ABR是跨越区域的，而自治系统边界路由器ASBR则不跨越AS</p><h2 id="第一题">第一题</h2><p>一个路由器刚接收到如下新的 IP 地址： 57.6.96.0/21, 57.6.104.0/21, 57.6.112.0/21 和<br>57.6.120.0/21。如果所有这些地址都使用同一条出境线路，试问它们可以被聚合吗？如果可以，它们被聚合到哪个地址上？如果不可以，请问为什么？</p><table><thead><tr><th>00111001</th><th>00000110</th><th>01100000</th><th>00000000</th></tr></thead><tbody><tr><td>00111001</td><td>00000110</td><td>01101000</td><td>00000000</td></tr><tr><td>00111001</td><td>00000110</td><td>01110000</td><td>00000000</td></tr><tr><td>00111001</td><td>00000110</td><td>01111000</td><td>00000000</td></tr></tbody></table><p>这几个地址的最长公共前缀为19，又公用一条出境线路，因此可以被聚合成57.6.96.0/19</p><h2 id="第二题">第二题</h2><ul class="lvl-0"><li class="lvl-2"><p>更新后的路由表</p><table><thead><tr><th>目的网络</th><th>距离</th><th>下一跳</th></tr></thead><tbody><tr><td>N1</td><td>7</td><td>A</td></tr><tr><td>N3</td><td>3</td><td>C</td></tr><tr><td>N4</td><td>9</td><td>C</td></tr><tr><td>N6</td><td>8</td><td>F</td></tr><tr><td>N7</td><td>5</td><td>C</td></tr><tr><td>N8</td><td>3</td><td>C</td></tr><tr><td>N9</td><td>4</td><td>D</td></tr></tbody></table><ol><li class="lvl-5"><p>路由器 B 接收到路由器 C 发送的距离向量报文，该报文包含了目的网络 N2、N3、N4、N7 和 N8 的距离信息，以及这些网络的下一跳路由器 C 的信息。</p></li><li class="lvl-5"><p>路由器 B 在更新它的路由表前，需要计算距离向量。</p></li><li class="lvl-5"><p>$$<br>\pi(i){\leftarrow} min[\pi(i),min(\pi(i)+w_{ji})]<br>$$</p></li></ol></li><li class="lvl-2"><p>路由器 B 收到从路由器 C 发往网络 N2 的 IP 分组时，会发现距离 N2 的距离为 16，显然这已经超出了最大跳数（默认最大跳数为 15），因此路由器 B 会丢弃该分组。路由器 B 会将这条信息告知其它相邻路由器，以便它们也能更新自己的路由表。</p></li></ul><h2 id="第三题">第三题</h2><table><thead><tr><th>目的网络</th><th>下一跳</th><th>接口</th></tr></thead><tbody><tr><td>AS1</td><td>153.14.3.2</td><td>S0</td></tr><tr><td>194.17.5.128/25</td><td></td><td>E0</td></tr><tr><td>194.17.20.0/23</td><td>194.17.24.2</td><td>S1</td></tr></tbody></table><ul class="lvl-0"><li class="lvl-2"><p>根据最长前缀匹配原则，通过E0接口</p></li><li class="lvl-2"><p>BGP（eBGP），TCP协议，</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>有趣的事</title>
      <link href="/2023/04/28/articles/funny-things/"/>
      <url>/2023/04/28/articles/funny-things/</url>
      
        <content type="html"><![CDATA[<h1>文采不错</h1><p>二零二一年的夏天，<s>xxx</s>十七岁，第一次鼓起勇气，和喜欢的女孩子告白。那是生命中美丽的一天，阳光还算得上明媚，楼宇间轻风阵阵，远处悠扬的蝉鸣声，抚慰少年胸怀。两人肩并肩坐在公园的长椅上，观察一张张年轻和老去的面孔。东风一劲，女孩身体靠近，发丝飞舞。几个月后，<s>xxx</s>发现对方不喜欢吃新鲜的胡萝卜，感到兴味索然，和她提出分手。霓虹灯下的哨兵 2023-04-28</p>]]></content>
      
      
      <categories>
          
          <category> articles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nsfw </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy_1</title>
      <link href="/2023/04/28/Somniloquy/dream-1/"/>
      <url>/2023/04/28/Somniloquy/dream-1/</url>
      
        <content type="html"><![CDATA[<p>这是什么?<a href="2023/04/28/Somniloquy/dream-0/">这篇README</a>大概也不能告诉你.</p><h1>周五</h1><img src="/img/dream-talk-1.assets/rainInBeijing.jpg" style="zoom: 25%;" /><p>北京下了大暴雨。我没有带伞，淋成了落汤鸡。</p><p>最近有些无聊了，淋雨、洗澡、敲代码时明明跟自己说了很多话，结果只剩上面两句了。</p><img src="/img/dream-talk-1.assets/breakfast.jpg" style="zoom: 25%;" /><p>对了，连着两天吃了早饭，挺好的；但是连着两天四小时充足睡眠，不太妙。<em>出现幻觉头晕目眩1:11</em> 😵</p><p>同学生日，大家聚餐了。我没去。</p><p>😄</p><h2 id="light-up-my-day">light up my day</h2><img src="\img\dream-talk-1.assets\Snipaste_2023-04-28_23-46-31.png" style="zoom: 85%;" /><h2 id="life-goes-on">life goes on</h2><blockquote><p>玩物丧志</p></blockquote><ul class="lvl-0"><li class="lvl-2"><p>这一切的意义是？</p></li><li class="lvl-2"><p>可听到雷声阵阵，可感到危险来临</p></li><li class="lvl-2"><p>真是下贱</p></li><li class="lvl-2"><p>微笑着就倒在血泊中,还在担心别人比我疼</p></li><li class="lvl-2"><p>其实我是个<a href="https://tool.liumingye.cn/music/#/search/M/song/%E8%A0%A2%E8%B4%A7">蠢货</a></p></li><li class="lvl-2"><p>time is up,deal is over</p></li><li class="lvl-2"><p>我应该更关注爱我的人才对</p></li><li class="lvl-2"><p>我被这么多人爱着,我真是天底下最幸福的小孩</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Somniloquy_README</title>
      <link href="/2023/04/28/Somniloquy/dream-0/"/>
      <url>/2023/04/28/Somniloquy/dream-0/</url>
      
        <content type="html"><![CDATA[<h1>README-README</h1><p>正经人谁写日记呢？</p><blockquote><p>Create this blog just in case I want to write anything (well that may never happen 😃)</p></blockquote><p>我想每周更新一个文件是一个合理的频率。</p><p>大概想到什么就写点什么。</p><p>我是不是可以给自己评论一下，这样你不登录github也能给我点赞？</p><p>我不知道。我不知道。</p><p>意义是？……</p><p>估计也不会有什么排版。</p><p>也许以后我会把梦话和梦里的内容分开。</p><p>这个category完全是梦中的情节，都是假的，不是真的。</p>]]></content>
      
      
      <categories>
          
          <category> Somniloquy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> Somniloquy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello 沸羊羊</title>
      <link href="/2023/04/27/articles/feiyangyang/"/>
      <url>/2023/04/27/articles/feiyangyang/</url>
      
        <content type="html"><![CDATA[<p>我是沸羊羊，一只聪明又机智的羊。在这草原上，我被称为小霸王。可是，我有一颗温柔的心，曾经深深地爱上了美羊羊。</p><p>可惜，美羊羊看不到我这颗真挚的心。她总是被那些华丽而无脑的花瓶吸引，哪怕他们只会说些毫无意义的话语。我才不会像他们那样肤浅呢。</p><p>我曾经为了她的芳心不惜一切，可是她却视而不见。我想起这段经历，不禁感叹美羊羊真是没有眼光。毕竟，在这么多羊中，我沸羊羊才是最出色的啊！</p><p>不过，没关系，因为我也许早就准备好了面对这个结果。沸羊羊也许只有美羊羊才能陪伴我，可有些羊比美羊羊更加值得我去爱。我会欣然接受这个事实，因为我自信有一个真正的爱情会出现在我的身边。</p>]]></content>
      
      
      <categories>
          
          <category> articles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 沸羊羊 </tag>
            
            <tag> 美羊羊 </tag>
            
            <tag> gpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello YUX</title>
      <link href="/2023/04/27/articles/hello-YUX/"/>
      <url>/2023/04/27/articles/hello-YUX/</url>
      
        <content type="html"><![CDATA[<h1>我在华清大学的“快乐”</h1><p>如果你问我在华清大学是否很快乐，那么我要告诉你，非常快乐。当然，这种快乐并不是所有人都能理解的。</p><p>在华清大学，我曾经快乐地熬了一个通宵写论文，因为我很享受那种极度的疲惫和奋斗的感觉。我还快乐地经历了一场期末考试，因为我发现自己完全不懂老师讲的内容，这让我有机会尝试新的解题方法。</p><p>当然，我在华清大学也曾经快乐地经历了各种各样的挫折，比如我一次考试只得了50分，就当做了一次锻炼自己的心态。还有一次莫名其妙被拒绝的实习机会，就让我意识到了自己要更加努力才能达到期望。</p><p>总之，在华清大学的这几年，我真的很快乐。虽然每天都要面对各种形形色色的压力和不确定性，但这些经历让我成长了许多。所以，我要感谢华清大学这个“快乐”的地方，让我有机会成为更好的自己。</p><!-- About Me --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>About Me</h2>        </div>    </div>    <div class="row">        <div class="col-md-4">            <img src="/img/profile.png" alt="Profile Image">        </div>        <div class="col-md-8">            <p>Hi, I am YUX and I am passionate about NOTHING. </p>        </div>    </div></div>]]></content>
      
      
      <categories>
          
          <category> articles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THU </tag>
            
            <tag> THU-daily </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>crawler-sample</title>
      <link href="/2023/04/27/others/code-test/"/>
      <url>/2023/04/27/others/code-test/</url>
      
        <content type="html"><![CDATA[<h1>code-test</h1><blockquote><p>这是一个用来测试代码显示效果的页面。</p><p>你也可以用它爬点东西</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.service <span class="keyword">import</span> Service <span class="keyword">as</span> ChromeService</span><br><span class="line"><span class="keyword">from</span> webdriver_manager.chrome <span class="keyword">import</span> ChromeDriverManager</span><br><span class="line"><span class="comment"># from fake_useragent import UserAgent</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DynCrawler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,url,theme</span>):</span><br><span class="line">        self.url = url</span><br><span class="line">        self.theme = theme</span><br><span class="line">        self.options=webdriver.ChromeOptions() <span class="comment"># 无头模式</span></span><br><span class="line">        self.options.add_argument(<span class="string">&#x27;--headless&#x27;</span>)</span><br><span class="line">        self.driver = webdriver.Chrome(service=ChromeService(executable_path=ChromeDriverManager().install()),options=self.options)</span><br><span class="line">        self.href_list=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_whole_page</span>(<span class="params">self</span>):</span><br><span class="line">        html_page=self.driver.find_elements(By.XPATH,<span class="string">&#x27;//*[@class=&quot;list-t&quot;]/a&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> html_page:</span><br><span class="line">            url =page.get_attribute(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">            title=page.text</span><br><span class="line">            data = &#123;<span class="string">&quot;url&quot;</span>:<span class="built_in">str</span>(url),<span class="string">&quot;title&quot;</span>:<span class="built_in">str</span>(title)&#125;</span><br><span class="line">            self.href_list.append(data)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;get page: &quot;</span>,url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_images</span>(<span class="params">self,url,title</span>):</span><br><span class="line">        self.driver.get(url)</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        images=self.driver.find_elements(By.XPATH,<span class="string">&#x27;//div[@class=&quot;content&quot;]/a/img&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> index,image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">            image_url=image.get_attribute(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(image_url)</span><br><span class="line">            <span class="keyword">if</span> image_url:</span><br><span class="line">                image_name=index+<span class="number">1</span></span><br><span class="line">                image_name=<span class="built_in">str</span>(image_name)+<span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">                image_path=<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>/<span class="subst">&#123;image_name&#125;</span>&#x27;</span></span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(image_path,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        f.write(requests.get(image_url).content)</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_title</span>(<span class="params">self,url</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;get title: &quot;</span>,url)</span><br><span class="line">        self.driver.get(url)</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        title=self.driver.find_element(By.XPATH,<span class="string">&#x27;//*[@id=&quot;app&quot;]/div/div[3]/div[1]/div[1]/h1&#x27;</span>)</span><br><span class="line">        title=title.text</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;title: &quot;</span>,title)</span><br><span class="line">        <span class="keyword">return</span> title</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.theme):</span><br><span class="line">            os.mkdir(self.theme)</span><br><span class="line">        self.driver.get(self.url)</span><br><span class="line">        self.driver.implicitly_wait(<span class="number">2</span>)</span><br><span class="line">        self.get_whole_page()</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> self.href_list:</span><br><span class="line">            url=data[<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">            title=data[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">            title=title.replace(<span class="string">&#x27;/&#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            title=title.replace(<span class="string">&#x27;|&#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            title=title.replace(<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            title=title.replace(<span class="string">&#x27;：&#x27;</span>,<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            <span class="comment"># make dir</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>&#x27;</span>):</span><br><span class="line">                os.mkdir(<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>&#x27;</span>)</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;./<span class="subst">&#123;self.theme&#125;</span>/<span class="subst">&#123;title&#125;</span>/url.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(url)</span><br><span class="line">                self.get_images(url,title)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;请输入歌手名字：&#x27;</span>)</span><br><span class="line">    singername=<span class="built_in">input</span>()</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">8</span>):</span><br><span class="line">        url=<span class="string">f&#x27;http://www.echangwang.com/singer/<span class="subst">&#123;singername&#125;</span>_<span class="subst">&#123;index&#125;</span>.html&#x27;</span></span><br><span class="line">        crawler=DynCrawler(url,singername)</span><br><span class="line">        crawler.run()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fool</title>
      <link href="/2023/04/27/others/fool/"/>
      <url>/2023/04/27/others/fool/</url>
      
        <content type="html"><![CDATA[<div class="danger"><p>时间紧，任务重，I’m f**ked up</p></div><div class="warning"><p>时间不多了,under pressure</p></div><div class="tips"><p>时间还早,totally under control</p></div><div class="success"><p>写完了😄</p></div><ul class="lvl-0"><li class="lvl-2"><p><input type="checkbox" id="checkbox17"><label for="checkbox17">uog</label><br><input type="checkbox" id="checkbox16"><label for="checkbox16">sdf</label></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test-markdown</title>
      <link href="/2023/04/27/others/ful-test/"/>
      <url>/2023/04/27/others/ful-test/</url>
      
        <content type="html"><![CDATA[<h1>标题示例</h1><h2 id="二级标题">二级标题</h2><h3 id="三级标题">三级标题</h3><h4 id="四级标题">四级标题</h4><h5 id="五级标题">五级标题</h5><h6 id="六级标题">六级标题</h6><h2 id="段落和换行">段落和换行</h2><p>这是一个段落，可以写一些文字。这个段落有两行。</p><p>这是第二个段落。使用两个以上的空格加上一个回车可以进行换行。</p><p>这是一个段落，可以写一些文字。这个段落有两行。</p><p>这是第二个段落。使用两个以上的空格加上一个回车可以进行换行。</p><p>使用 <code>**</code> 包裹的文字会变成 <strong>加粗</strong>。使用 <code>_</code> 包裹的文字会变成 <em>斜体</em>。可以在一段文字中混合使用。</p><p>使用 <code>~</code> 包裹的文字会变成 <s>删除线</s>。</p><p>使用 <code>==</code> 包裹的文字会变成高亮，如： <mark>高亮</mark>。</p><h2 id="列表示例">列表示例</h2><h3 id="无序列表">无序列表</h3><ul class="lvl-0"><li class="lvl-2"><p>项目一</p></li><li class="lvl-2"><p>项目二</p></li><li class="lvl-2"><p>项目三</p></li></ul><h3 id="有序列表">有序列表</h3><ol><li class="lvl-3"><p>第一项</p></li><li class="lvl-3"><p>第二项</p></li><li class="lvl-3"><p>第三项</p></li></ol><h3 id="可以使用嵌套列表：">可以使用嵌套列表：</h3><ol><li class="lvl-3"><p>第一项</p><ul class="lvl-2"><li class="lvl-5">嵌套项目</li><li class="lvl-5">嵌套项目</li></ul></li><li class="lvl-3"><p>第二项</p><ol><li class="lvl-6">嵌套有序项目</li><li class="lvl-6">嵌套有序项目</li></ol></li><li class="lvl-3"><p>第三项</p></li></ol><h2 id="引用示例">引用示例</h2><blockquote><p>这是一个引用。引用可以有多行。</p><blockquote><p>引用可以嵌套。</p></blockquote></blockquote><h2 id="粗体和斜体示例">粗体和斜体示例</h2><p>这里是 <strong>粗体</strong> 和 <em>斜体</em> 的示例。同样可以使用 <strong>粗体</strong> 和 <em>斜体</em>。</p><h2 id="链接和图片示例">链接和图片示例</h2><h3 id="链接">链接</h3><p>这是一个<a href="https://www.example.com/">链接</a>。</p><p>可以给链接加上标题：</p><p>这是一个<a href="https://www.example.com/" title="示例链接">链接</a>。</p><h3 id="图片">图片</h3><p>这是一个图片：</p><p><img src="/img/test.png" alt="图片描述"></p><p>也可以给图片加上标题：</p><p><img src="/img/test_profile.png" alt="图片描述" title="示例图片"></p><h2 id="代码块和公式块示例">代码块和公式块示例</h2><p>这是一个代码块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hello</span>():</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Hello World!&quot;</span>)</span><br></pre></td></tr></table></figure><p>也可以使用单行代码块： <code>print(&quot;Hello World!&quot;)</code></p><p>这是一个公式块：<br>$$<br>f(x) = \frac{1}{\sqrt{2\pi}\sigma}e<sup>{-\frac{(x-\mu)</sup>2}{2\sigma^2}}<br>$$</p><h2 id="分隔线示例">分隔线示例</h2><p>这是一条分隔线：</p><hr><h2 id="表格示例">表格示例</h2><table><thead><tr><th>名称</th><th>价格</th></tr></thead><tbody><tr><td>商品 A</td><td>$20</td></tr><tr><td>商品 B</td><td>$30</td></tr><tr><td>商品 C</td><td>$40</td></tr></tbody></table><h2 id="文内链接">文内链接</h2><p>这是一个 <a href="#%E6%A0%87%E9%A2%98%E7%A4%BA%E4%BE%8B">内部链接</a> 到标题示例。</p><h2 id="目录示例">目录示例</h2><p>可以使用 <code>[toc]</code> 来生成目录，需要使用一些 Markdown 工具来渲染生成的目录。</p><p>[toc]</p><h2 id="表情符号示例">表情符号示例</h2><p>可以使用表情符号来增加一些个性化的元素：</p><p>👍 ✨ 🐫 🎉 🚀 🤘 :octocat:</p><h2 id="HTML-语法示例">HTML 语法示例</h2><p>Markdown 还支持 HTML 语法，如下面的加粗文字：</p><p><b>这是一段加粗的文字</b></p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2023/04/27/others/hello-world-1/"/>
      <url>/2023/04/27/others/hello-world-1/</url>
      
        <content type="html"><![CDATA[<!-- Banner Image --><div class="banner">    <img src="/img/banner.png" alt="Banner Image">    <div class="banner-text">        <h1>Hello World</h1>        <p>A warm welcome to my blog!</p>    </div></div><!-- Introduction --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>Introduction</h2>            <p>Welcome to my blog! My name is YUX and I am passionate about NOTHING. This blog is my platform to share my thoughts and experiences with you.</p>        </div>    </div></div><!-- About Me --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>About Me</h2>        </div>    </div>    <div class="row">        <div class="col-md-4">            <img src="/img/profile.png" alt="Profile Image">        </div>        <div class="col-md-8">            <p>Hi, I am YUX and I am passionate about NOTHING. </p>        </div>    </div></div>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>tips</title>
      <link href="/2023/04/27/others/tips/"/>
      <url>/2023/04/27/others/tips/</url>
      
        <content type="html"><![CDATA[<h2 id="终端设置代理">终端设置代理</h2><p>要永久设置HTTP代理和HTTPS代理，可以将这些命令添加到PowerShell的配置文件中。</p><p>以下是在PowerShell中设置永久HTTP代理和HTTPS代理的步骤：</p><ol><li class="lvl-3"><p>打开PowerShell命令提示符，输入以下命令：<code>notepad $PROFILE</code></p></li><li class="lvl-3"><p>在打开的文件中，添加以下两行命令：</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$Env:http_proxy=&quot;http://127.0.0.1:&lt;你的代理端口&gt;&quot;</span><br><span class="line">$Env:https_proxy=&quot;http://127.0.0.1:&lt;你的代理端口&gt;&quot;</span><br></pre></td></tr></table></figure><ol start="3"><li class="lvl-3"><p>保存并关闭文件。</p></li></ol><p>现在，每次打开新的PowerShell窗口时，都会自动加载此配置文件，并在环境变量中设置HTTP代理和HTTPS代理。</p><p>请注意，如果您使用的是PowerShell Core，配置文件的位置可能不同，并且可能需要使用其他命令来编辑它。</p>]]></content>
      
      
      <categories>
          
          <category> tips </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>network-homework-5</title>
      <link href="/2023/04/27/homework/network/network5/"/>
      <url>/2023/04/27/homework/network/network5/</url>
      
        <content type="html"><![CDATA[<h1>网原第五次作业</h1><p>郭高旭 2021010803 <a href="mailto:ggx21@mails.tsinghua.edu.cn">ggx21@mails.tsinghua.edu.cn</a></p><h3 id="1-地球与一个遥远行星通信">1.地球与一个遥远行星通信</h3><ol><li class="lvl-3"><p>路线单向延时<br>$$<br>T=9\frac{10<sup>{10}}{3\times10</sup>8}=300s<br>$$<br>采用停等协议，利用率为<br>$$<br>32 * 1000 * 86/(32 * 1000 * 8b + 2 * 64 * 1000000bps * 300s) = 6.66\times10^{-6}<br>$$</p></li><li class="lvl-3"><p>采用滑动窗口协议：</p><p>n = 2 * 300s/(32 * 1000 * 8b/64 * 1000000bps) + 1 = 150001</p></li></ol><h3 id="2-地球同步卫星的最大信道利用率">2.地球同步卫星的最大信道利用率</h3><ol><li class="lvl-3"><p>停等式</p><p>1000/(1000 + 2 * 1000000 * 270 + 1000) = 1/542</p></li><li class="lvl-3"><p>协议5</p><p>7 * 1000/(1000 + 2 * 1000000 * 270 + 1000) = 7/542</p></li><li class="lvl-3"><p>协议6</p><p>4 * 1000/(1000 + 2 * 1000000 * 270 + 1000) = 7/542</p></li></ol><h3 id="3-卫星信道最大吞吐量">3.卫星信道最大吞吐量</h3><p>512字节占⽤信道时间为T = 512 * 86/(64 * 1000bps) = 0.064s</p><ul class="lvl-0"><li class="lvl-2"><p>窗口大小为1</p><p>512 * 86/(0.064 + 0.270 * 2) = 6781bps</p></li><li class="lvl-2"><p>窗口大小为7</p><p>7 * 512 * 86/(0.064 + 0.270 * 2) = 47467bps</p></li><li class="lvl-2"><p>窗口大小15</p><p>由于信道最多容量为n = [(0.270 * 2 + 0.064)/0.064] =9  故吞吐量为64kbps</p></li><li class="lvl-2"><p>窗口大小127</p><p>同上，吞吐量为64kbs</p></li></ul><h3 id="4-主机之间帧序号的比特数">4.主机之间帧序号的比特数</h3><p>数据帧传输时间范围：$t_{min}$=$128\times{8b/32kbps}=0.032s$ ;$t_{max}$=$512\times{8b/32kbps}=0.128s$</p><p>最小窗口大小范围<br>$$<br>l_{min}=2\times0.3s/t_{max}=6;l_{max}=2\times0.3s/t_{min}=20<br>$$<br>故帧序号最少需要5个比特</p>]]></content>
      
      
      <categories>
          
          <category> homework </category>
          
          <category> network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2023/04/27/notes/greece/%CE%B5%CE%BB%CE%BB%CE%B7%CE%BD%CE%B9%CE%BA%CE%AC/"/>
      <url>/2023/04/27/notes/greece/%CE%B5%CE%BB%CE%BB%CE%B7%CE%BD%CE%B9%CE%BA%CE%AC/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>το ποτήρι 🥃</th><th>水杯</th><th>το ψωμί 🥖</th><th>面包</th></tr></thead><tbody><tr><td><s>ο δίσκος 🛸</s></td><td>碟片</td><td>το τραπέζι</td><td>桌子</td></tr><tr><td><s>το τασάκι</s></td><td>烟灰缸</td><td><s>η ζώνη</s></td><td>皮带</td></tr><tr><td>η σοκολάτα 🍫</td><td>巧克力</td><td>το παγωτό 🍦</td><td>冰淇淋</td></tr></tbody></table><p>注意在回答对方&quot;Πως λέγεται αυτό το πράγμα στα ελληνικά;&quot;时回答不需要加冠词，同时要加书名号或双引号。注意Πώς要加重音符号</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> ελληνικά </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生词表 </tag>
            
            <tag> ελληνικά </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ελληνικά</title>
      <link href="/2023/04/27/notes/greece/%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%B8%8C%E8%85%8A%E8%AF%AD%E7%B3%BB%E5%8A%A8%E8%AF%8D%E5%92%8C%CE%B1,%20%CE%B21,%20%CE%B22%E5%9E%8B%E5%8A%A8%E8%AF%8D%E5%9C%A8%E5%85%AD%E7%A7%8D%E4%BA%BA%E7%A7%B0%E4%B8%8B%E7%9A%84%E5%8F%98%E4%BD%8D%E8%A7%84%E5%88%99%EF%BC%9A/"/>
      <url>/2023/04/27/notes/greece/%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%B8%8C%E8%85%8A%E8%AF%AD%E7%B3%BB%E5%8A%A8%E8%AF%8D%E5%92%8C%CE%B1,%20%CE%B21,%20%CE%B22%E5%9E%8B%E5%8A%A8%E8%AF%8D%E5%9C%A8%E5%85%AD%E7%A7%8D%E4%BA%BA%E7%A7%B0%E4%B8%8B%E7%9A%84%E5%8F%98%E4%BD%8D%E8%A7%84%E5%88%99%EF%BC%9A/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>数字</th><th>希腊语</th><th>数字</th><th>希腊语</th></tr></thead><tbody><tr><td>1</td><td>μία (mía)</td><td>20</td><td>είκοσι (íkosi)</td></tr><tr><td>2</td><td>δύο (dýo)</td><td>30</td><td>τριάντα (triánta)</td></tr><tr><td>3</td><td>τρία (tría)</td><td>40</td><td>σαράντα (saránta)</td></tr><tr><td>4</td><td>τέσσερα (téssera)</td><td>50</td><td>πενήντα (penínta)</td></tr><tr><td>5</td><td>πέντε (pénte)</td><td>60</td><td>εξήντα (exínta)</td></tr><tr><td>6</td><td>έξι (éxi)</td><td>70</td><td>εβδομήντα (evdomínta)</td></tr><tr><td>7</td><td>εφτά (eftá)</td><td>80</td><td>ογδόντα (ogdónta)</td></tr><tr><td>8</td><td>οχτώ (ochtó)</td><td>90</td><td>ενενήντα (enenínta)</td></tr><tr><td>9</td><td>εννιά (ennéa)</td><td>100</td><td>εκατό (ekató)</td></tr><tr><td>10</td><td>δέκα (déka)</td><td>200</td><td>διακόσια (diakósia)</td></tr><tr><td>11</td><td>έντεκα (énteka)</td><td>300</td><td>τριακόσια (triakósia)</td></tr><tr><td>12</td><td>δώδεκα (dódeka)</td><td>400</td><td>τετρακόσια (tetrakósia)</td></tr><tr><td>13</td><td>δεκατρία (dekatría)</td><td>500</td><td>πεντακόσια (pentakósia)</td></tr><tr><td>14</td><td>δεκατέσσερα (dekatéssera)</td><td>600</td><td>εξακόσια (hexakósia)</td></tr><tr><td>15</td><td>δεκαπέντε (dekarpénte)</td><td>700</td><td>εφτακόσια (eftakósia)</td></tr><tr><td>16</td><td>δεκαέξι (dekaéxi)</td><td>800</td><td>οκτακόσια (oktakósia)</td></tr><tr><td>17</td><td>δεκαεφτά (dekaeftá)</td><td>900</td><td>εννιακόσια (enniakósia)</td></tr><tr><td>18</td><td>δεκαοχτώ (dekaochtó)</td><td>1000</td><td>χίλια</td></tr><tr><td>19</td><td>δεκαεννιά (dekaennéa)</td><td>0</td><td>μηδέν</td></tr></tbody></table><h3 id="以下是希腊语系动词和α-β1-β2型动词在六种人称下的变位规则：">以下是希腊语系动词和α, β1, β2型动词在六种人称下的变位规则：</h3><h4 id="系动词-μαι">系动词 (μαι)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>εγώ είμαι</td><td>εσύ είσαι</td><td>αυτός/αυτή/αυτό είναι</td></tr><tr><td>复数</td><td>εμείς είμαστε</td><td>εσείς είστε</td><td>αυτοί/αυτές είναι</td></tr></tbody></table><h4 id="α动词-μαθαίνω，ω结尾，重音不在其上">α动词 (μαθαίνω，ω结尾，重音不在其上)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>μαθαίνω -ω</td><td>μαθαίνεις -εις</td><td>μαθαίνει -ει</td></tr><tr><td>复数</td><td>μαθαίνουμε -ουμε</td><td>μαθαίνετε -ετε</td><td>μαθαίνουν -ουν</td></tr></tbody></table><h4 id="β1动词-γράφω，-άω结尾">β1动词 (γράφω，-άω结尾)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>μιλάω -άω</td><td>μιλάς -άς</td><td>μιλάει -άει</td></tr><tr><td>复数</td><td>μιλάμε -άμε</td><td>μιλάτε -άτε</td><td>μιλάνε -άνε</td></tr></tbody></table><h4 id="β2动词-φέρω">β2动词 (φέρω)</h4><table><thead><tr><th>人称</th><th>一</th><th>二</th><th>三</th></tr></thead><tbody><tr><td>单数</td><td>φέρω (féro)</td><td>φέρεις (féreis)</td><td>φέρει (férei)</td></tr><tr><td>复数</td><td>φέρουμε (féroume)</td><td>φέρετε (férete)</td><td>φέρουν (féroun)</td></tr></tbody></table><h3 id="物主代词">物主代词</h3><table><thead><tr><th style="text-align:center">人称代词</th><th style="text-align:center">单数形式</th><th style="text-align:center">复数形式</th></tr></thead><tbody><tr><td style="text-align:center">第一人称</td><td style="text-align:center">μου (mou)</td><td style="text-align:center">μας (mas)</td></tr><tr><td style="text-align:center">第二人称</td><td style="text-align:center">σου (sou)</td><td style="text-align:center">σας (sas)</td></tr><tr><td style="text-align:center">第三人称</td><td style="text-align:center">του (tou)/της/του</td><td style="text-align:center">τους (tous)</td></tr></tbody></table><h3 id="希腊语问好">希腊语问好</h3><table><thead><tr><th>希腊语</th><th>中文</th><th>发音</th></tr></thead><tbody><tr><td>γεια σου</td><td>你好（单数）</td><td>yea soo</td></tr><tr><td>γεια σας</td><td>您好（复数或正式场合）</td><td>yea sas</td></tr><tr><td>καλημέρα</td><td>早上好 / 上午好</td><td>kah-lee-MER-ah</td></tr><tr><td>καλησπέρα</td><td>下午好 / 晚上好</td><td>ka-lee-SPER-ah</td></tr><tr><td>καληνύχτα</td><td>晚安</td><td>kah-lee-NEECH-tah</td></tr><tr><td>αντίο</td><td>再见</td><td></td></tr></tbody></table><h3 id="国家">国家</h3><table><thead><tr><th>希腊文</th><th>中文</th><th>希腊文</th><th>中文</th></tr></thead><tbody><tr><td>Γερμανία/γερμανικά</td><td>德国</td><td>Καναδά</td><td>加拿大</td></tr><tr><td>Ιαπωνία/γιαπωνέζικα</td><td>日本</td><td>Λίβανο</td><td>黎巴嫩</td></tr><tr><td>Κίνα/κινέζικα</td><td>中国</td><td>Αγγλία/αγγλικά</td><td>英国</td></tr><tr><td>Κορέα</td><td>韩国</td><td>Γαλλία</td><td>法国</td></tr><tr><td>Ιταλία\ιταλικά</td><td>意大利</td><td>Ρωσία</td><td>俄罗斯</td></tr><tr><td>Ισπανία\ισπανικά</td><td>西班牙</td><td>Αμερική</td><td>美国</td></tr><tr><td>Τουρκία</td><td>土耳其</td><td>Αυστραλία</td><td>澳大利亚</td></tr><tr><td>Κροατία</td><td>克罗地亚</td><td>Βέλγιο</td><td>比利时</td></tr><tr><td>Ελβετία</td><td>瑞士</td><td>Μεξικό</td><td>墨西哥</td></tr><tr><td>Κύπρος</td><td>塞浦路斯</td><td>Μαρόκο</td><td>摩洛哥</td></tr><tr><td>Ελλάδα/ελληνικά</td><td>希腊</td><td>Ιράν</td><td>伊朗</td></tr></tbody></table><table><thead><tr><th>亲人</th><th>单数形式</th><th>带有&quot;του&quot;所有格的名词</th><th>带有&quot;της&quot;所有格的名词</th></tr></thead><tbody><tr><td>父亲</td><td>πατέρας (patéras)</td><td>του πατέρα (tou patéra)</td><td>της μητέρας (tis miterás)</td></tr><tr><td>母亲</td><td>μητέρα (mitéra)</td><td>της μητέρας (tis miterás)</td><td>του πατέρα (tou patéra)</td></tr><tr><td>儿子</td><td>γιος (yios)\αγόρι</td><td>του γιου (tou yiou)</td><td>της κόρης (tis kóris)</td></tr><tr><td>女儿</td><td>κόρη (kóri)\Κορίτσι</td><td>της κόρης (tis kóris)</td><td>του γιου (tou yiou)</td></tr><tr><td>兄弟</td><td>αδελφός (adelfós)</td><td>του αδελφού (tou adelfoú)</td><td>της αδελφής (tis adelfís)</td></tr><tr><td>姐妹</td><td>αδελφή (adelfí)</td><td>της αδελφής (tis adelfís)</td><td>του αδελφού (tou adelfoú)</td></tr><tr><td>表哥/表姐</td><td>ξάδερφος/ξαδέρφη (ksáderfos/ksadérfi)</td><td>του ξαδέρφη/ξάδερφου (tou ksadérfou/ksáderfou)</td><td>της ξαδέρφης/ξαδέρφης (tis ksadérfis/ksadérfis)</td></tr><tr><td>丈夫</td><td>άντρας (ántras)</td><td>του άντρα (tou ántra)</td><td>της γυναίκας (tis gynaíkas)</td></tr><tr><td>妻子</td><td>γυναίκα (gynaíka)</td><td>της γυναίκας (tis gynaíkas)</td><td>του άντρα (tou ántra)</td></tr></tbody></table><p>同样的，希腊语中家庭成员名词的词形变化与所有格有关，需要根据具体语境而定。κορίτσια  ，αγόρι</p><h3 id="生词表">生词表</h3><table><thead><tr><th style="text-align:left">Καλήμέρα (σας)</th><th></th><th>Καλησπέρα (σας)</th><th></th></tr></thead><tbody><tr><td style="text-align:left">Γεια σας</td><td></td><td>Καληνύχτα</td><td></td></tr><tr><td style="text-align:left">Χαίρετε</td><td>Hello</td><td></td><td></td></tr><tr><td style="text-align:left">Αντίο (σας)</td><td>goodbye</td><td></td><td></td></tr><tr><td style="text-align:left">τώρα</td><td>现在</td><td></td><td></td></tr><tr><td style="text-align:left">δουλεύω</td><td>工作</td><td></td><td></td></tr><tr><td style="text-align:left">μένω</td><td>住</td><td></td><td></td></tr><tr><td style="text-align:left">παντρεμένη</td><td>已婚女性</td><td></td><td></td></tr><tr><td style="text-align:left">άντρας</td><td>男人</td><td></td><td></td></tr><tr><td style="text-align:left"><strong>Από ποιο μέρος;</strong></td><td>你来自哪里？</td><td></td><td></td></tr><tr><td style="text-align:left">πως λεγεστε</td><td>你叫什么名字？</td><td></td><td></td></tr><tr><td style="text-align:left">τηλέφωνο</td><td>电话</td><td></td><td></td></tr><tr><td style="text-align:left">παρακαλώ</td><td>请</td><td></td><td></td></tr><tr><td style="text-align:left">ακριβώς</td><td>正好</td><td></td><td></td></tr><tr><td style="text-align:left">Εντάξει</td><td>好</td><td></td><td></td></tr><tr><td style="text-align:left">Επίσης λέμε:</td><td>同样也说：</td><td></td><td></td></tr><tr><td style="text-align:left">μήπως</td><td>难道</td><td></td><td></td></tr><tr><td style="text-align:left">κινητό</td><td>手机</td><td></td><td></td></tr><tr><td style="text-align:left">λεπτό</td><td>分钟</td><td></td><td></td></tr><tr><td style="text-align:left">Συγνώμη</td><td>对不起</td><td></td><td></td></tr><tr><td style="text-align:left">καθηγητής/καθηγήτρια</td><td>教授</td><td>γιατρός</td><td>医生</td></tr><tr><td style="text-align:left">πωλητής/πωλήτρια</td><td>销售员</td><td>φωτογράφος</td><td>摄影师</td></tr><tr><td style="text-align:left">δάσκαλος/δασκάλα</td><td>老师</td><td>μηχανικός</td><td>工程师</td></tr><tr><td style="text-align:left">μαθητής/μαθήτρια</td><td>学生</td><td>γραμματέας</td><td>秘书</td></tr><tr><td style="text-align:left">εργάτης/εργάτρια</td><td>工人</td><td>συνταξιούχος</td><td>退休人员</td></tr><tr><td style="text-align:left">άνεργος/άνεργη</td><td>失业人员</td><td>φωτομοντέλο (!)</td><td>时装模特儿</td></tr><tr><td style="text-align:left">φοιτητής/φοιτήτρια</td><td>大学生</td><td></td><td></td></tr><tr><td style="text-align:left">Παντρεμένος ή <strong>ελεύθερος</strong>/χωρισμένη</td><td>已婚/未婚/离异</td><td></td><td></td></tr><tr><td style="text-align:left">Λίγο.</td><td>一点点</td><td></td><td></td></tr><tr><td style="text-align:left">καθόλου.</td><td>一点也不</td><td></td><td></td></tr><tr><td style="text-align:left">δε μιλάω καθόλου αραβικά.</td><td>我一点也不会阿拉伯语。</td><td></td><td></td></tr><tr><td style="text-align:left"><strong>διεύθυνσή</strong> τούς.</td><td>他们的地址</td><td></td><td></td></tr><tr><td style="text-align:left">νοσοκομείο.</td><td>医院</td><td></td><td></td></tr><tr><td style="text-align:left">καταλαωαίνει.</td><td>理解</td><td></td><td></td></tr><tr><td style="text-align:left">αρκετά.</td><td>相当的</td><td></td><td></td></tr><tr><td style="text-align:left">ακόμα.</td><td>还是</td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr><tr><td style="text-align:left"></td><td></td><td></td><td></td></tr></tbody></table><ul class="lvl-0"><li class="lvl-2"><p>yu语法</p><ul class="lvl-2"><li class="lvl-4"><p>σε+την=στην</p></li><li class="lvl-4"><p>p53:路名</p></li><li class="lvl-4"><p>+ = συν / και</p><p>— = πλην</p></li><li class="lvl-4"><p>Το τηλέφωνό μου στο σπίτι είναι 210 9511203 调和音节（如果一个名词的重音位置在倒数第三音节，且其后是表示所属的物主代词）</p><ul class="lvl-4"><li class="lvl-6">(καθηγήτριά σού)</li><li class="lvl-6">διεύθνσή τούς</li></ul></li><li class="lvl-4"><p>-ι结尾变复数：加α</p><table><thead><tr><th>ένα παιδί</th><th>δύο/τρία… παιδιά</th></tr></thead><tbody><tr><td>ένα κορίτσι</td><td>δύο/τρία… κορίτσια</td></tr></tbody></table></li></ul></li><li class="lvl-2"><p>例句</p><ol><li class="lvl-5">Η Κάρεν είναι από την Αγγλία;</li><li class="lvl-5">Μιλάει ελληνικά;</li><li class="lvl-5">Τι δουλειά κάνει;</li><li class="lvl-5">Πού δουλεύει τώρα;</li><li class="lvl-5">Ο Γιώργος είναι καθηγητής;</li><li class="lvl-5">Πού μένειη Κάρεν τώρα;</li><li class="lvl-5">Είναι παντρεμένη;</li><li class="lvl-5">Έχει παιδιά;</li><li class="lvl-5">Πού είναι ο άντρας της;</li><li class="lvl-6">Είναι παντρεμένη με τον Χουάν κι έχουν δύο παιδιά,ένα κορίτσι , την Ντολόρες , κι ένα αγόρι, τον<br>Αντόνιο.</li></ol></li></ul><p>你好，我是peter。我来自中国，北京。现在住在希腊，雅典，xx路xx号。我是一名大学生。我目前单身。但是我有一个姐姐，她的名字是海伦，她已婚，有两个孩子，一个儿子，一个女儿。我姐姐的工作是医生。我会说希腊语，我的中文说得很好。但是我的英语很糟糕，我能理解英语但是我不会说。我和我的妈妈住在一起。我们的家庭电话号码是xxxxx。我也有手机，它的号码是xxxx。</p><p>Γεια σου, είμαι ο Peter. Είμαι από την Κίνα, το Πεκίνο. Τώρα μένω στην Αθήνα, στη διεύθυνση xx, στην οδό xx. Είμαι φοιτητής. Παρόλα αυτά, είμαι ανύπαντρος. Έχω όμως μια αδελφή, την Ελένη. Είναι παντρεμένη και έχει δύο παιδιά, ένα γιο και μια κόρη. Η δουλειά της είναι γιατρός. Μιλάω Ελληνικά, ενώ τα Κινέζικά μου είναι πολύ καλά. Ωστόσο, τα Αγγλικά μου είναι πολύ άσχημα και δεν μπορώ να μιλήσω καθόλου. Ζω με τη μητέρα μου. Ο οικιακός τηλέφωνός μας είναι xxxx και έχω επίσης ένα κινητό τηλέφωνο με τον αριθμό xxxx.</p>]]></content>
      
      
      <categories>
          
          <category> notes </category>
          
          <category> ελληνικά </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ελληνικά </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>README</title>
      <link href="/2020/04/27/README/"/>
      <url>/2020/04/27/README/</url>
      
        <content type="html"><![CDATA[<h1>README</h1><h3 id="我为什么在这里">我为什么在这里</h3><p>也许这里以后会补上我创建这个博客的原因</p><h3 id="你为什么在这里">你为什么在这里</h3><p>等待戈多</p><h3 id="其他">其他</h3><p>我从来都是白底界面亮度拉满。这次为什么是黑色的主题呢？其实它也提供了白色主题。</p><p>文章创建日期是可以随便改的&gt;&gt;&gt;</p><!-- About Me --><div class="container">    <div class="row">        <div class="col-md-12">            <h2>About Me</h2>        </div>    </div>    <div class="row">        <div class="col-md-4">            <img src="/img/profile.png" alt="Profile Image">        </div>        <div class="col-md-8">            <p>Hi, I am YUX and I am passionate about NOTHING. </p>        </div>    </div></div><div class="danger"><p>时间紧，任务重，I’m f**ked up</p></div><div class="warning"><p>时间不多了,under pressure</p></div><div class="tips"><p>时间还早,totally under control</p></div><div class="success"><p>写完了😄</p></div>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> README </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
