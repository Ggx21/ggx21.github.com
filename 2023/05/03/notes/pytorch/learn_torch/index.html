<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="1.准备数据 在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。 读写数据:(save&amp;load)   torch.save(x, &#39;x.pt&#39;):将x存在文件名同为x.pt的文件里。   将数据从存储的文件读回内存x2 &#x3D; torch.load(&#39;x.pt&#39;)">
<meta property="og:type" content="article">
<meta property="og:title" content="Dive-into-DL-PyTorch">
<meta property="og:url" content="https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/index.html">
<meta property="og:site_name" content="Hello From YUX">
<meta property="og:description" content="1.准备数据 在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。 读写数据:(save&amp;load)   torch.save(x, &#39;x.pt&#39;):将x存在文件名同为x.pt的文件里。   将数据从存储的文件读回内存x2 &#x3D; torch.load(&#39;x.pt&#39;)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ggx21.github.io/img/learn_torch.assets/3.8_sigmoid.png">
<meta property="og:image" content="https://ggx21.github.io/img/learn_torch.assets/3.8_sigmoid_grad.png">
<meta property="og:image" content="https://ggx21.github.io/img/learn_torch.assets/3.13_dropout.svg">
<meta property="article:published_time" content="2023-05-03T08:25:01.000Z">
<meta property="article:modified_time" content="2023-05-03T09:40:37.017Z">
<meta property="article:author" content="Andy Gao">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ggx21.github.io/img/learn_torch.assets/3.8_sigmoid.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Dive-into-DL-PyTorch</title>
    <!-- async scripts -->
    <!-- Google Analytics -->

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-86660611-1"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-86660611-1');
  </script>


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"
    ><i class="fa-solid fa-bars fa-lg"></i
  ></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"
    ><i class="fa-solid fa-bars fa-lg"></i
  ></a>
  <a
    id="top-icon-tablet"
    href="#"
    aria-label="顶部"
    onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"
    style="display: none"
    ><i class="fa-solid fa-chevron-up fa-lg"></i
  ></a>
  <span id="menu">
    <br />
    <span id="actions">
      <ul>
        <li>
          <a class="icon" aria-label="homepage" href="/"
            ><i
              class="fa-solid fa-home"
              aria-hidden="true"
              onmouseover="document.getElementById('homepage').style.display = 'inline';"
              onmouseout="document.getElementById('homepage').style.display = 'none';"
            ></i></a
          ><span id="homepage" class="info" style="display: none"
            >回到主页</span
          >
        </li>
         
        <li>
          <a
            class="icon"
            aria-label="下一篇"
            href="/2023/05/02/homework/DLCE/%E5%9B%9B%E4%BD%8D%E5%8A%A0%E6%B3%95%E5%99%A8REPORT/"
            ><i
              class="fa-solid fa-chevron-right"
              aria-hidden="true"
              onmouseover="document.getElementById('i-next').style.display = 'inline';"
              onmouseout="document.getElementById('i-next').style.display = 'none';"
            ></i></a
          ><span id="i-next" class="info" style="display: none"
            >下一篇</span
          >
        </li>
        
        <li>
          <a
            class="icon"
            aria-label="返回顶部"
            href="#"
            onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"
            ><i
              class="fa-solid fa-chevron-up"
              aria-hidden="true"
              onmouseover="document.getElementById('i-top').style.display = 'inline';"
              onmouseout="document.getElementById('i-top').style.display = 'none';"
            ></i></a
          ><span id="i-top" class="info" style="display: none"
            >返回顶部</span
          >
        </li>
      </ul>
    </span>
    <br />
    <div id="share" style="display: none">
    </div>
    <div id="toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">1.</span> <span class="toc-text">1.准备数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE-save-load"><span class="toc-number">1.0.1.</span> <span class="toc-text">读写数据:(save&amp;load)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text">读写模型:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-slData%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">代码(slData模式):</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">2.创建模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.0.1.</span> <span class="toc-text">多层感知机的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.1.1.</span> <span class="toc-text">例子:</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AAflattenlayer%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.2.</span> <span class="toc-text">一个flattenlayer的例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.3.</span> <span class="toc-text">一个模型的例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">初始化模型参数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.</span> <span class="toc-text">总结:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">3.定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">4.</span> <span class="toc-text">4.定义优化器和学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">5.训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">6.</span> <span class="toc-text">6.使用模型进行预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">7.</span> <span class="toc-text">7.模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.1.</span> <span class="toc-text">模型欠拟合与过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%9C%B0"><span class="toc-number">7.1.1.</span> <span class="toc-text">简单地:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%EF%BC%88training-error%EF%BC%89%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%EF%BC%88generalization-error%EF%BC%89"><span class="toc-number">7.1.2.</span> <span class="toc-text">训练误差（training error）和泛化误差（generalization error）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">7.1.3.</span> <span class="toc-text">训练集\验证集\测试集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.2.</span> <span class="toc-text">应对过拟合:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F%EF%BC%88weight-decay%EF%BC%89"><span class="toc-number">7.2.1.</span> <span class="toc-text">1.权重衰减（weight decay）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-number">7.2.1.1.</span> <span class="toc-text">实现代码：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%A2%E5%BC%83%E6%B3%95-dropout"><span class="toc-number">7.2.2.</span> <span class="toc-text">2.丢弃法(dropout)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.2.2.1.</span> <span class="toc-text">实现:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN"><span class="toc-number">8.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">8.1.</span> <span class="toc-text">0.前置知识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">8.2.</span> <span class="toc-text">1.填充和步幅:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85padding"><span class="toc-number">8.2.0.1.</span> <span class="toc-text">填充padding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85stride"><span class="toc-number">8.2.0.2.</span> <span class="toc-text">步幅stride</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E4%B8%8E%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.</span> <span class="toc-text">2.多输入通道与多输出通道.</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">8.3.1.</span> <span class="toc-text">多输入通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.2.</span> <span class="toc-text">多输出通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">8.3.3.</span> <span class="toc-text">1*1卷积层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B1%A0%E5%8C%96"><span class="toc-number">8.4.</span> <span class="toc-text">2.池化</span></a></li></ol></li></ol></div>
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Dive-into-DL-PyTorch
    </h1>



    <div class="meta">
      <span
        class="author p-author h-card"
        itemprop="author"
        itemscope
        itemtype="http://schema.org/Person"
      >
        <span class="p-name" itemprop="name"
          >Andy Gao</span
        >
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-05-03T08:25:01.000Z" class="dt-published" itemprop="datePublished">2023-05-03</time>
        
      
    </div>

 
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/notes/">notes</a> › <a class="category-link" href="/categories/notes/PyTorch/">PyTorch</a>
    </div>

 
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/PyTorch/" rel="tag">PyTorch</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="1-准备数据">1.准备数据</h2>
<p>在使用pytorch进行深度学习时，首先需要准备好数据。一般来说，数据需要按照一定的格式组织，例如可以使用Dataset和DataLoader将数据读入内存，并按照批次进行划分。</p>
<h4 id="读写数据-save-load">读写数据:(save&amp;load)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p><code>torch.save(x, 'x.pt')</code>:将x存在文件名同为<code>x.pt</code>的文件里。</p>
</li>
<li class="lvl-2">
<p>将数据从存储的文件读回内存<code>x2 = torch.load('x.pt')</code></p>
<p>不仅是tensor.dict啥啥的数据都能这么存储.</p>
<p>无论是文件大小还是读写速度都完爆json</p>
</li>
</ul>
<h4 id="读写模型">读写模型:</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>两个方式:仅保存和加载模型参数(<code>state_dict</code>)(推荐)；保存和加载整个模型。</p>
<p>差别就是,第一种方法在SL时候只SL参数,下次L时先建一个新model,然后把参数L进去就好了.</p>
</li>
</ul>
<h5 id="代码-slData模式">代码(slData模式):</h5>
<p>保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是pt或pthCopy to clipboardErrorCopied</span></span><br></pre></td></tr></table></figure>
<p>加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>一个小问题在于不同设备(CPU与GPU,不同GPU之间.具体遇到问题再说吧)</p>
<h2 id="2-创建模型">2.创建模型</h2>
<p>模型是深度学习的核心，它决定了最终学习的效果。在pytorch中，可以通过继承nn.Module类来创建模型，并在其中定义前向计算函数。</p>
<blockquote>
<p>我的理解是,模型就是一个千层饼.每一层有个什么功能.下面就是一层饼的例子.</p>
<p>实际上并不是层数越多越好.</p>
<ul class="lvl-1">
<li class="lvl-2">
<p>当神经网络的层数较多时，模型的数值稳定性容易变差:$0.99^ {365}=?;1.01^{365=?}$</p>
</li>
</ul>
<p>通过不停地tensor运算联立方程,是不是可以把千层饼等效为单层?</p>
</blockquote>
<h4 id="多层感知机的设计">多层感知机的设计</h4>
<p>如果千层饼被等效为单层,那么对层的设计就失去了意义.上面的问题在于.</p>
<blockquote>
<p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p>
</blockquote>
<p>解决方法是我们加入一些非线性变换的层</p>
<h5 id="例子">例子:</h5>
<ul class="lvl-0">
<li class="lvl-2">
<p>ReLU（rectified linear unit）函数<br>
$$<br>
ReLU(x)=max(x,0).<br>
$$</p>
</li>
<li class="lvl-2">
<p>sigmoid<br>
$$<br>
sigmoid(x)= \frac 1 {1+exp(−x)}<br>
$$<br>
<img src="/img/learn_torch.assets/3.8_sigmoid.png" alt="img" style="zoom:33%;" /></p>
<img src="/img/learn_torch.assets/3.8_sigmoid_grad.png" alt="img" style="zoom:33%;" />
</li>
<li class="lvl-2">
<p>tanh函数(双曲正切)<br>
$$<br>
tanh(x)= \frac{1+exp(−2x)}{1−exp(−2x)}<br>
$$<br>
图像和sigmoid差不多,但是0附近更陡峭一些</p>
</li>
</ul>
<h4 id="一个flattenlayer的例子">一个flattenlayer的例子</h4>
<p>这里有一个把(batchsize,*,*,…)的多维tensor转换为(batchsize,***)的二维tensor的例子.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FlattenLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="一个模型的例子">一个模型的例子</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict([</span><br><span class="line">        (<span class="string">&#x27;flatten&#x27;</span>, FlattenLayer()),<span class="comment">#看,就是这样一层层搭起来...</span></span><br><span class="line">        (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">    ])</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="初始化模型参数">初始化模型参数:</h3>
<p>构建好了模型,我们得有初始参数啊.</p>
<p>**太好了!**PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略,一般不用我们考虑;如果你真的想知道可参考pytorch<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules">源代码)</a></p>
<h3 id="总结">总结:</h3>
<ol>
<li class="lvl-3">
<p>继承Module类</p>
<p><code>Module</code>类是<code>nn</code>模块里提供的一个模型构造类，是所有神经网络模块的基类</p>
<p>一般来说,我们重载<code>Module</code>类的<code>__init__</code>函数和<code>forward</code>函数。它们分别用于创建模型参数和定义前向计算。无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的<code>backward</code>函数。</p>
<p>注意:重载<code>__init__</code>函数时应该首先调用父类<code>module</code>的初始化函数e.g.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br></pre></td></tr></table></figure>
</li>
<li class="lvl-3">
<p>module的子类:还有一些<code>Sequential</code>,<code>ModuleDict</code>,<code>ModuleList</code>之类的东西.我感觉没什么用啊.好像只是让我把一堆Module以一种整齐的方式凑在一起.<code>Sequential</code>好像还挺有用的,至少他要保证层之间输入输出维度匹配,可以直接forward.</p>
</li>
<li class="lvl-3">
<p>共享模型参数: <code>Module</code>类的<code>forward</code>函数里多次调用同一个层,层之间参数共享。此外，如果我们传入<code>Sequential</code>的模块是同一个<code>Module</code>实例的话参数也是共享的</p>
<blockquote>
<p>真的有人在乎参数究竟是什么吗?</p>
</blockquote>
</li>
<li class="lvl-3">
<p>自定义层:见[flattenlayer](#### 一个flattenlayer的例子)的例子,这是一个不带参数的层,你当然也可以带参数.但意义是?</p>
</li>
<li class="lvl-3">
<p>我想:module就是层的累加.一个头进一个头出.<code>人体蜈蚣.</code></p>
</li>
</ol>
<h2 id="3-定义损失函数">3.定义损失函数</h2>
<p>常见的损失函数有交叉熵损失函数、均方误差损失函数等，可以根据不同的任务选择不同的损失函数。</p>
<blockquote>
<p>例子:</p>
<p>PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="4-定义优化器和学习率">4.定义优化器和学习率</h2>
<p>在训练的过程中，需要使用优化器来更新模型参数。常见的优化器有梯度下降法、Adam等。同时，由于学习率对训练效果影响较大，所以需要定义学习率的初始值以及变化规则。</p>
<blockquote>
<p>e.g.</p>
<p>我们使用学习率为0.1的小批量随机梯度下降作为优化算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="5-训练模型">5.训练模型</h2>
<p>将数据、模型、损失函数、优化器和学习率等组合在一起，循环执行前向计算、损失计算、反向传播和参数更新等步骤，即可训练模型。在每个epoch结束后，还需要对模型进行评估。</p>
<h2 id="6-使用模型进行预测">6.使用模型进行预测</h2>
<p>在训练好模型后，可以使用它对新的数据进行预测，并输出预测结果或概率。</p>
<h2 id="7-模型选择">7.模型选择</h2>
<p>我们怎么评价模型的好坏呢?</p>
<h3 id="模型欠拟合与过拟合">模型欠拟合与过拟合</h3>
<h4 id="简单地"><strong>简单地:</strong></h4>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。</strong></p>
</li>
<li class="lvl-2">
<p><strong>过少的训练样本</strong>也会导致过拟合</p>
</li>
</ul>
<h4 id="训练误差（training-error）和泛化误差（generalization-error）">训练误差（training error）和泛化误差（generalization error）</h4>
<p>当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确。这是为什么呢？</p>
<p>以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。</p>
<p>训练的时候是根据往年题的表现来的(通过减小训练误差).所以往年题做的好不一定代表着高考考的好.(一般来说训练误差的期望$\leq$泛化误差)</p>
<p>但我们关注的是<strong>泛化误差</strong></p>
<h4 id="训练集-验证集-测试集">训练集\验证集\测试集</h4>
<p>高考只能考一次,我们没有办法从训练集(看着答案做题)中知道自己的高考表现.所以我们可以拿一些测试集训练集之外的数据作为<strong>验证集</strong>(模考).</p>
<p>操作上,我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p>
<blockquote>
<p><em>K</em>折交叉验证（K-<em>K</em>-fold cross-validation）</p>
<p>分出K个子集来.这样可以测K次</p>
</blockquote>
<p>但实际上,由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊.</p>
<h3 id="应对过拟合">应对过拟合:</h3>
<h4 id="1-权重衰减（weight-decay）">1.权重衰减（weight decay）</h4>
<blockquote>
<p>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</p>
</blockquote>
<p>权重衰减等价于$L_2$范数正则化（regularization）</p>
<p><strong>我的理解</strong>:就是在计算损失函数的时候加上一项$λ\times L_2$其中λ是一个超参。$L_2$：参数权重越大，$L_2$越大。这样大概保证了各参数大小都差不多，不偏科。</p>
<h5 id="实现代码：">实现代码：</h5>
<p>直接在构造优化器实例时通过<code>weight_decay</code>参数来指定权重衰减超参数。默认下，PyTorch会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line"><span class="comment">#...在optimize时</span></span><br><span class="line"><span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br></pre></td></tr></table></figure>
<h4 id="2-丢弃法-dropout">2.丢弃法(dropout)</h4>
<img src="/img/learn_torch.assets/3.13_dropout.svg" alt="img" style="zoom: 80%;" />
<p>在隐藏层随机丢弃一个单元.比如上图,隐藏层原来有$h_1,h_2,…,h_5.h_2,h_5 $被丢弃了.</p>
<p>这样计算时不会过度依赖某一个隐藏单元</p>
<ul class="lvl-0">
<li class="lvl-2">
<blockquote>
<p>(和权重衰减的目的大概是一样的),都是保证不偏科.要不然模考物理总是很难,就你一个考100,人家都是0分.到了高考一赋分大家都考90,你傻眼了.</p>
</blockquote>
</li>
</ul>
<h5 id="实现">实现:</h5>
<p>在PyTorch中，我们只需要在全连接层后添加<code>Dropout</code>层并指定丢弃概率。在训练模型时，<code>Dropout</code>层将以指定的丢弃概率随机丢弃上一层的输出元素</p>
<p>在测试模型时（即<code>model.eval()</code>后），<code>Dropout</code>层并不发挥作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),<span class="comment">#设定一个丢弃概率</span></span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<blockquote>
<p>丢弃法只在训练模型时使用。</p>
</blockquote>
<h2 id="CNN">CNN</h2>
<p>我写到一半想明白了.根本不需要考虑实际的数据是几维的反正能通过view转换.都转成2维\1维的(一个句子实际上也是二维的),然后batchsize就是channel数是不是就完事了</p>
<h3 id="0-前置知识">0.前置知识</h3>
<p>在net里加上一个卷积层就形成了<strong>CNN</strong></p>
<p>一个小翻译:<code>Kernel</code>在pytorch的卷积计算时就叫做<code>filter</code>.</p>
<p>CNN的学习就是学习这个<code>kernel</code>,(相应的,上面在学习一个参数tensor)</p>
<p>那个经典的图像边缘检测实际上在互相关.kernel走一步,算一圈,下个蛋…</p>
<p>卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>.有什么区别?**没有区别!**反正都是学出来的,学出来的核再上下左右转换一下那么两种运算就交换了.所以根本不需要知道什么是卷积就能CNN.</p>
<h3 id="1-填充和步幅">1.填充和步幅:</h3>
<p>要想使用一个卷积层,操作上显见的问题就是我们要知道这个卷积层输入输出的形状.</p>
<p>一般来说,对于第i维来说,如果输入在第i维长度?<code>有没有更好的名字</code>是$n_i$,核长度是$k_i$那么输出在第i维长度就是($n_i-k_i+1$),显然,$k_i$不应该超过$n_i$,否则会报错</p>
<h5 id="填充padding">填充padding</h5>
<p>**太长不看:**取padding=({$\frac {kernelsize_i-1} 2$,…})可以保证输入输出形状相同</p>
<p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。</p>
<p>如果在第i维的两侧<strong>一共填充$p_i$行</strong></p>
<p>那么输出形状第i维将会是($n_i-k_i+p_i+1$)</p>
<p>所以通常可以把$p_i设置为k_i-1$来使输入输出具有相同形状</p>
<p>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<p><mark>注意</mark>:操作上<code>padding=(2, 1)</code>padding的参数代表着在kernel两侧分别填充多少.也就是说$=p_i/2$,所以尽量也把核的每一维长度取奇数,这样保证了$p_i/2$是个整数.否则还得考虑两边分别取floor和ceiling,<strong>麻烦死了</strong></p>
<h5 id="步幅stride">步幅stride</h5>
<p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p>
<p>步幅可以按比例缩小形状</p>
<p>如果按上一步设置理想的padding，同时如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将缩为原来的1/stride倍.其他情况自己算去,但何必为难自己.</p>
<h3 id="2-多输入通道与多输出通道">2.多输入通道与多输出通道.</h3>
<h4 id="多输入通道">多输入通道</h4>
<p>实际上不就是多了一维吗?比如彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。就是新增了一个长度为3(r,g,b)的一维.然后我们用三个卷积核叠一块,发现在色彩维度上insize和kernersize都是3,然后这一维长度就变成1,退化了.</p>
<p>实现含多个输入通道的互相关运算。我们只需要对每个通道做互相关运算，然后通过<code>add_n</code>函数来进行累加。</p>
<h4 id="多输出通道">多输出通道</h4>
<p>哎呀我一阵头晕目眩,不想知道它是怎么算的了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积层----------------------------------------看这里</span></span><br><span class="line">conv_layer = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">output_tensor = conv_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量形状</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)  <span class="comment"># torch.Size([16, 10, 32, 32])</span></span><br></pre></td></tr></table></figure>
<p>直接来看示例代码吧.知道它怎么算干嘛呢?</p>
<p>在上面的代码中，<code>input_tensor</code> 是一个批次大小为 16、通道数为 3、高度为 32、宽度为 32 的输入张量，<code>conv_layer</code> 是一个输出通道数为 10、卷积核大小为 3x3、步幅为 1、填充为 1 的卷积层，<code>output_tensor</code> 是卷积层的输出张量，它的形状为 <code>[16, 10, 32, 32]</code>。</p>
<p>这是gpt说的啊,我觉着挺对的.可以自己跑着试试.</p>
<h4 id="1-1卷积层">1*1卷积层</h4>
<p>一个长宽都是1的核</p>
<p>1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。</p>
<p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么1×1卷积层的作用与全连接层等价</strong>。</p>
<p>通过这个来调整参数的通道数,控制模型复杂度.e.g.语数外+理综to语数外+三门选科</p>
<h3 id="2-池化">2.池化</h3>
<p><strong>缓解卷积层对位置的过度敏感性</strong>。</p>

  </div>
</article>
<script
  async
  src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"
></script>
<br />
<br />
<hr />
<span id="busuanzi_container_page_pv">
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</span>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/search/">搜索</a></li>
        
          <li><a href="/tags/">标签</a></li>
        
          <li><a href="/categories/">分类</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/ggx21">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">1.</span> <span class="toc-text">1.准备数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE-save-load"><span class="toc-number">1.0.1.</span> <span class="toc-text">读写数据:(save&amp;load)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text">读写模型:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-slData%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">代码(slData模式):</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">2.创建模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.0.1.</span> <span class="toc-text">多层感知机的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.1.1.</span> <span class="toc-text">例子:</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AAflattenlayer%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.2.</span> <span class="toc-text">一个flattenlayer的例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">2.0.3.</span> <span class="toc-text">一个模型的例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">初始化模型参数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.</span> <span class="toc-text">总结:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">3.定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">4.</span> <span class="toc-text">4.定义优化器和学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">5.训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">6.</span> <span class="toc-text">6.使用模型进行预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">7.</span> <span class="toc-text">7.模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.1.</span> <span class="toc-text">模型欠拟合与过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%9C%B0"><span class="toc-number">7.1.1.</span> <span class="toc-text">简单地:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%EF%BC%88training-error%EF%BC%89%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%EF%BC%88generalization-error%EF%BC%89"><span class="toc-number">7.1.2.</span> <span class="toc-text">训练误差（training error）和泛化误差（generalization error）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">7.1.3.</span> <span class="toc-text">训练集\验证集\测试集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E5%AF%B9%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">7.2.</span> <span class="toc-text">应对过拟合:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F%EF%BC%88weight-decay%EF%BC%89"><span class="toc-number">7.2.1.</span> <span class="toc-text">1.权重衰减（weight decay）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-number">7.2.1.1.</span> <span class="toc-text">实现代码：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%A2%E5%BC%83%E6%B3%95-dropout"><span class="toc-number">7.2.2.</span> <span class="toc-text">2.丢弃法(dropout)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.2.2.1.</span> <span class="toc-text">实现:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN"><span class="toc-number">8.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">8.1.</span> <span class="toc-text">0.前置知识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">8.2.</span> <span class="toc-text">1.填充和步幅:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85padding"><span class="toc-number">8.2.0.1.</span> <span class="toc-text">填充padding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85stride"><span class="toc-number">8.2.0.2.</span> <span class="toc-text">步幅stride</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E4%B8%8E%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.</span> <span class="toc-text">2.多输入通道与多输出通道.</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">8.3.1.</span> <span class="toc-text">多输入通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">8.3.2.</span> <span class="toc-text">多输出通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">8.3.3.</span> <span class="toc-text">1*1卷积层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B1%A0%E5%8C%96"><span class="toc-number">8.4.</span> <span class="toc-text">2.池化</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&text=Dive-into-DL-PyTorch"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&is_video=false&description=Dive-into-DL-PyTorch"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Dive-into-DL-PyTorch&body=Check out this article: https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&title=Dive-into-DL-PyTorch"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&name=Dive-into-DL-PyTorch&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://ggx21.github.io/2023/05/03/notes/pytorch/learn_torch/&t=Dive-into-DL-PyTorch"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;   2020-2023 Andy Gao
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       -->
        <li>
          <a href="/"
            >首页</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/search/"
            >搜索</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/tags/"
            >标签</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/categories/"
            >分类</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/archives/"
            >归档</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a href="/about/"
            >关于</a
          >
        </li>
        <!--
     --><!--
       -->
        <li>
          <a target="_blank" rel="noopener" href="http://github.com/ggx21"
            >项目</a
          >
        </li>
        <!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = 'Ggx21/comments.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = '✨';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
